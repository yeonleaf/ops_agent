#!/usr/bin/env python3
"""
ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì œê±°ëœ í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸
ko-sroberta-multitask ëª¨ë¸ì˜ ì›ë³¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©
"""

import os
import numpy as np
import re
from dotenv import load_dotenv
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer
import logging
from typing import List, Union

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def cleanse_text(text: str) -> str:
    """
    ì„ë² ë”© ì „ í…ìŠ¤íŠ¸ ì •ì œ - ì¡ìŒ ì œê±°
    ë°˜ë³µë˜ëŠ” ë©”íƒ€ë°ì´í„° íŒ¨í„´ì„ ì œê±°í•˜ì—¬ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    """
    if not text:
        return ""
    
    # 1. Jira í‹°ì¼“ í‚¤ íŒ¨í„´ ì œê±° [BTVO-NNNNN]
    text = re.sub(r'\[BTVO-\s?\d+\]', '', text)
    
    # 2. NCMS íŒ¨í„´ ì œê±° [NCMS]
    text = re.sub(r'\[NCMS\]', '', text)
    
    # 3. ë‚ ì§œ íŒ¨í„´ ì œê±° (MM/DD) ë˜ëŠ” (YYYY-MM-DD)
    text = re.sub(r'\(\d{1,2}/\d{1,2}\)', '', text)
    text = re.sub(r'\(\d{4}-\d{2}-\d{2}\)', '', text)
    
    # 4. ê¸°íƒ€ ë¶ˆí•„ìš”í•œ íŒ¨í„´ë“¤
    text = re.sub(r'\[.*?\]', '', text)  # ëŒ€ê´„í˜¸ ì•ˆì˜ ëª¨ë“  ë‚´ìš© ì œê±°
    text = re.sub(r'\(.*?\)', '', text)  # ì†Œê´„í˜¸ ì•ˆì˜ ëª¨ë“  ë‚´ìš© ì œê±°
    
    # 5. ì—°ì†ëœ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    
    # 6. ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    return text

def apply_l2_normalization(embeddings: Union[List[List[float]], np.ndarray]) -> List[List[float]]:
    """
    ì„ë² ë”© ë²¡í„°ë“¤ì— L2 ì •ê·œí™”ë¥¼ ì ìš©
    
    Args:
        embeddings: ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” numpy ë°°ì—´
        
    Returns:
        L2 ì •ê·œí™”ëœ ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
    """
    if isinstance(embeddings, np.ndarray):
        embeddings = embeddings.tolist()
    
    normalized_embeddings = []
    for embedding in embeddings:
        if isinstance(embedding, list):
            embedding_array = np.array(embedding)
        else:
            embedding_array = embedding
        
        # L2 ì •ê·œí™”: ê° ë²¡í„°ë¥¼ ë‹¨ìœ„ ë²¡í„°ë¡œ ë³€í™˜
        l2_norm = np.linalg.norm(embedding_array)
        if l2_norm > 0:
            normalized_embedding = embedding_array / l2_norm
        else:
            normalized_embedding = embedding_array
        
        normalized_embeddings.append(normalized_embedding.tolist())
    
    return normalized_embeddings

def normalize_query_embedding(query_embedding: Union[List[float], np.ndarray]) -> List[float]:
    """
    ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”©ì— L2 ì •ê·œí™”ë¥¼ ì ìš©
    
    Args:
        query_embedding: ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”© ë²¡í„°
        
    Returns:
        L2 ì •ê·œí™”ëœ ì¿¼ë¦¬ ì„ë² ë”© ë²¡í„°
    """
    if isinstance(query_embedding, list):
        embedding_array = np.array(query_embedding)
    else:
        embedding_array = query_embedding
    
    # L2 ì •ê·œí™”: ë²¡í„°ë¥¼ ë‹¨ìœ„ ë²¡í„°ë¡œ ë³€í™˜
    l2_norm = np.linalg.norm(embedding_array)
    if l2_norm > 0:
        normalized_embedding = embedding_array / l2_norm
    else:
        normalized_embedding = embedding_array
    
    return normalized_embedding.tolist()

def calculate_similarity_from_distance(distance: float, method: str = "cosine") -> float:
    """
    ChromaDB ê±°ë¦¬ì—ì„œ ìœ ì‚¬ë„ ê³„ì‚°
    
    Args:
        distance: ChromaDBì—ì„œ ë°˜í™˜í•˜ëŠ” ê±°ë¦¬
        method: ê±°ë¦¬ ê³„ì‚° ë°©ë²• ("cosine", "euclidean", "auto")
        
    Returns:
        ìœ ì‚¬ë„ ì ìˆ˜ (0~1 ë²”ìœ„)
    """
    if method == "cosine":
        # ì½”ì‚¬ì¸ ê±°ë¦¬ì¸ ê²½ìš°: ìœ ì‚¬ë„ = 1 - ê±°ë¦¬
        # ë‹¨, ê±°ë¦¬ê°€ 2ë¥¼ ì´ˆê³¼í•˜ë©´ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¡œ ê°„ì£¼
        if distance <= 2.0:
            return max(0.0, 1.0 - distance)
        else:
            return 1.0 / (1.0 + distance)
    elif method == "euclidean":
        # ìœ í´ë¦¬ë“œ ê±°ë¦¬ì¸ ê²½ìš°: ìœ ì‚¬ë„ = 1 / (1 + ê±°ë¦¬)
        return 1.0 / (1.0 + distance)
    else:  # auto
        # ìë™ ê°ì§€: ê±°ë¦¬ ë²”ìœ„ì— ë”°ë¼ íŒë‹¨
        if distance <= 2.0:
            return max(0.0, 1.0 - distance)  # ì½”ì‚¬ì¸ ê±°ë¦¬ë¡œ ê°„ì£¼
        else:
            return 1.0 / (1.0 + distance)  # ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¡œ ê°„ì£¼

def calculate_cosine_similarity_direct(embedding1: List[float], embedding2: List[float]) -> float:
    """
    ë‘ ì„ë² ë”© ë²¡í„° ê°„ì˜ ì§ì ‘ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    
    Args:
        embedding1: ì²« ë²ˆì§¸ ì„ë² ë”© ë²¡í„°
        embedding2: ë‘ ë²ˆì§¸ ì„ë² ë”© ë²¡í„°
        
    Returns:
        ì½”ì‚¬ì¸ ìœ ì‚¬ë„ (0~1 ë²”ìœ„)
    """
    from sklearn.metrics.pairwise import cosine_similarity
    return cosine_similarity([embedding1], [embedding2])[0][0]

class CleanKoreanEmbeddingFunction:
    """ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì œê±°ëœ í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© í•¨ìˆ˜"""
    
    def __init__(self, model_name: str = "jhgan/ko-sroberta-multitask"):
        """
        í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì—†ìŒ)
        
        Args:
            model_name: ì‚¬ìš©í•  ëª¨ë¸ëª… (ê¸°ë³¸ê°’: jhgan/ko-sroberta-multitask)
        """
        self.model_name = model_name
        self.name = model_name  # ChromaDB í˜¸í™˜ì„±ì„ ìœ„í•œ name ì†ì„±
        self.model = None
        self.tokenizer = None
        self.dimension = 768  # ko-sroberta-multitaskì˜ ì„ë² ë”© ì°¨ì›
        
        logger.info(f"ğŸ”§ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘ (ì „ì²˜ë¦¬ ì—†ìŒ): {model_name}")
        try:
            # SentenceTransformer ëª¨ë¸ ë¡œë“œ
            self.model = SentenceTransformer(model_name)
            
            # ì˜¬ë°”ë¥¸ í† í¬ë‚˜ì´ì € ë¡œë“œ í™•ì¸
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            
            logger.info(f"âœ… í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì™„ë£Œ: {model_name}")
            logger.info(f"   ì„ë² ë”© ì°¨ì›: {self.dimension}")
            logger.info(f"   í† í¬ë‚˜ì´ì €: {self.tokenizer.__class__.__name__}")
            logger.info(f"   ì „ì²˜ë¦¬: ì—†ìŒ (ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©)")
        except Exception as e:
            logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def __call__(self, input):
        """
        í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜ (ChromaDB 0.4.16+ í˜¸í™˜)
        ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì—†ì´ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        
        Args:
            input: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            L2 ì •ê·œí™”ëœ ì„ë² ë”© ë²¡í„° ë¦¬ìŠ¤íŠ¸
        """
        if not self.model:
            raise RuntimeError("ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        try:
            # ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì—†ì´ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            # ë‹¨, ë¹ˆ ë¬¸ìì—´ì´ë‚˜ None ê°’ë§Œ ì²˜ë¦¬
            processed_texts = []
            for text in input:
                if isinstance(text, str) and text.strip():
                    # í…ìŠ¤íŠ¸ ì •ì œ ì ìš© (ì¡ìŒ ì œê±°)
                    cleaned_text = cleanse_text(text.strip())
                    processed_texts.append(cleaned_text)
                else:
                    # ë¹ˆ ë¬¸ìì—´ì¸ ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
                    processed_texts.append("")
            
            # ì„ë² ë”© ìƒì„± (ì›ë¬¸ ê·¸ëŒ€ë¡œ)
            embeddings = self.model.encode(processed_texts, convert_to_tensor=False)
            
            # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            if isinstance(embeddings, np.ndarray):
                embeddings = embeddings.tolist()
            
            # L2 ì •ê·œí™” ì ìš© (ì„ë² ë”© í’ˆì§ˆ í–¥ìƒ)
            normalized_embeddings = apply_l2_normalization(embeddings)
            
            logger.info(f"âœ… {len(input)}ê°œ í…ìŠ¤íŠ¸ ì„ë² ë”© ì™„ë£Œ (ì°¨ì›: {len(normalized_embeddings[0]) if normalized_embeddings else 0}, L2 ì •ê·œí™” ì ìš©, í…ìŠ¤íŠ¸ ì •ì œ ì ìš©)")
            return normalized_embeddings
            
        except Exception as e:
            logger.error(f"âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ë”ë¯¸ ì„ë² ë”© ë°˜í™˜ (L2 ì •ê·œí™” ì ìš©)
            dummy_embeddings = []
            for _ in input:
                dummy_embedding = np.random.normal(0, 0.1, self.dimension)
                dummy_embeddings.append(dummy_embedding)
            
            # L2 ì •ê·œí™” ì ìš©
            normalized_dummy_embeddings = apply_l2_normalization(dummy_embeddings)
            return normalized_dummy_embeddings

def setup_clean_korean_embedding():
    """ChromaDBì— ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì œê±°ëœ í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •"""
    logger.info("ğŸ”§ ChromaDBì— ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì œê±°ëœ í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© í•¨ìˆ˜ ì„¤ì • ì¤‘...")
    
    # í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
    load_dotenv()
    
    # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    client = chromadb.PersistentClient(
        path="./vector_db",
        settings=Settings(
            anonymized_telemetry=False,
            allow_reset=True
        )
    )
    
    # ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì œê±°ëœ í•œêµ­ì–´ ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™”
    try:
        clean_korean_embedding_function = CleanKoreanEmbeddingFunction()
    except Exception as e:
        logger.error(f"âŒ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        return False
    
    # ê¸°ì¡´ ì»¬ë ‰ì…˜ë“¤ í™•ì¸
    collections = client.list_collections()
    logger.info(f"ğŸ“‹ ë°œê²¬ëœ ì»¬ë ‰ì…˜: {[c.name for c in collections]}")
    
    # ê° ì»¬ë ‰ì…˜ì„ ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì œê±°ëœ í•œêµ­ì–´ ì„ë² ë”© í•¨ìˆ˜ë¡œ ì¬ì„¤ì •
    for collection_info in collections:
        collection_name = collection_info.name
        logger.info(f"\nğŸ”§ ì»¬ë ‰ì…˜ '{collection_name}' ì¬ì„¤ì • ì¤‘...")
        
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
            client.delete_collection(collection_name)
            logger.info(f"   âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ: {collection_name}")
            
            # ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì œê±°ëœ í•œêµ­ì–´ ì„ë² ë”© í•¨ìˆ˜ë¡œ ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
            new_collection = client.create_collection(
                name=collection_name,
                embedding_function=clean_korean_embedding_function,
                metadata={
                    "description": f"ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì œê±°ëœ í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© - {collection_name}",
                    "embedding_model": "jhgan/ko-sroberta-multitask",
                    "embedding_dimension": 768,
                    "language": "korean",
                    "l2_normalization": True,
                    "custom_preprocessing": False,
                    "tokenizer": "AutoTokenizer"
                }
            )
            logger.info(f"   âœ… ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì œê±°ëœ í•œêµ­ì–´ ì„ë² ë”© í•¨ìˆ˜ ì„¤ì • ì™„ë£Œ: {collection_name}")
            
        except Exception as e:
            logger.error(f"   âŒ ì»¬ë ‰ì…˜ ì„¤ì • ì‹¤íŒ¨: {collection_name} - {e}")
    
    logger.info("\nğŸ‰ ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì œê±°ëœ í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© í•¨ìˆ˜ ì„¤ì • ì™„ë£Œ!")
    logger.info("   ì´ì œ ëª¨ë“  ì»¬ë ‰ì…˜ì´ 768ì°¨ì› í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    logger.info("   ëª¨ë¸: jhgan/ko-sroberta-multitask")
    logger.info("   ì „ì²˜ë¦¬: ì—†ìŒ (ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©)")
    logger.info("   L2 ì •ê·œí™”: ì ìš©ë¨")
    
    return True

def test_clean_korean_embedding():
    """ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì œê±°ëœ í•œêµ­ì–´ ì„ë² ë”© í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§ª ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì œê±°ëœ í•œêµ­ì–´ ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    try:
        # í•œêµ­ì–´ ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™”
        clean_korean_embedding_function = CleanKoreanEmbeddingFunction()
        
        # í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸ë“¤ (ì›ë¬¸ ê·¸ëŒ€ë¡œ)
        test_texts = [
            "ì„œë²„ì— ì ‘ì†í•  ìˆ˜ ì—†ëŠ” ë¬¸ì œê°€ ìˆë‚˜ìš”?",
            "ë©”ì¸ ì„œë²„ì— ì ‘ì†ì´ ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. HTTP 500 ì˜¤ë¥˜ê°€ ë°œìƒí•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ê°€ ì§ê´€ì ì´ì§€ ì•Šì•„ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.",
            "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "API ì‘ë‹µ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤."
        ]
        
        # ì„ë² ë”© ìƒì„±
        embeddings = clean_korean_embedding_function(test_texts)
        
        # ê²°ê³¼ ê²€ì¦
        logger.info(f"âœ… ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì™„ë£Œ:")
        logger.info(f"   í…ìŠ¤íŠ¸ ê°œìˆ˜: {len(test_texts)}")
        logger.info(f"   ì„ë² ë”© ê°œìˆ˜: {len(embeddings)}")
        logger.info(f"   ì„ë² ë”© ì°¨ì›: {len(embeddings[0]) if embeddings else 0}")
        
        # L2 ì •ê·œí™” ê²€ì¦
        for i, embedding in enumerate(embeddings):
            l2_norm = np.linalg.norm(embedding)
            logger.info(f"   í…ìŠ¤íŠ¸ {i+1}: L2 norm = {l2_norm:.6f} (ì •ê·œí™”ë¨: {abs(l2_norm - 1.0) < 1e-6})")
        
        # ìœ ì‚¬ë„ ê³„ì‚° í…ŒìŠ¤íŠ¸
        if len(embeddings) >= 2:
            from sklearn.metrics.pairwise import cosine_similarity
            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]
            logger.info(f"   ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸: {similarity:.4f}")
        
        return True
        
    except Exception as e:
        logger.error(f"âŒ ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì œê±°ëœ í•œêµ­ì–´ ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        return False

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ë¨¼ì € ì‹¤í–‰
    if test_clean_korean_embedding():
        # í…ŒìŠ¤íŠ¸ ì„±ê³µ ì‹œ ì„¤ì • ì‹¤í–‰
        setup_clean_korean_embedding()
    else:
        logger.error("âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨ë¡œ ì¸í•´ ì„¤ì •ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
