#!/usr/bin/env python3
"""
í–¥ìƒëœ ë©”ì¼ ë‚´ìš© ì¶”ì¶œê¸°
HTML ì •ì œ + ì •ê·œì‹ + ì¤‘ë³µ ì œê±°ë¡œ í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œ
"""

import re
import html
from typing import Optional, Dict, List
from bs4 import BeautifulSoup
import logging

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)


class EnhancedContentExtractor:
    """í–¥ìƒëœ ë©”ì¼ ë‚´ìš© ì¶”ì¶œê¸°"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        # ë¶ˆí•„ìš”í•œ íŒ¨í„´ë“¤ (ì´ë¯¸ì§€ íƒœê·¸ëŠ” ì œì™¸)
        self.noise_patterns = [
            r'<script[^>]*>.*?</script>',
            r'<style[^>]*>.*?</style>',
            r'<meta[^>]*>',
            r'<link[^>]*>',
            r'<!--.*?-->',
            r'You are receiving this email because.*?',
            r'Privacy\s*Statement.*?',
            r'This email is generated through.*?',
            r'Notification settings:.*?',
            r'Unsubscribe.*?',
            r'Contoso\'s use of Microsoft 365.*?',
            # CSS ìŠ¤íƒ€ì¼ íŒ¨í„´ë“¤
            r'[a-zA-Z0-9_-]+\s*\{[^}]*\}',
            r'@media[^{]*\{[^}]*\}',
            r'/\*.*?\*/',
            r'#[a-zA-Z0-9_-]+\s*\{[^}]*\}',
            r'\.[a-zA-Z0-9_-]+\s*\{[^}]*\}',
            r'[a-zA-Z0-9_-]+\s*:\s*[^;]+;',
            r'!important',
        ]
        
        # ì¤‘ìš”í•œ íŒ¨í„´ë“¤
        self.important_patterns = [
            r'(?i)(urgent|important|deadline|meeting|project|task|issue|bug|error|critical)',
            r'(?i)(request|approve|review|feedback|action|required|needed)',
            r'(?i)(schedule|appointment|conference|call|meeting)',
            r'(?i)(due|deadline|expire|expiry|expires)',
            r'(?i)(order|smoothie|stuff|logistics)',
            r'(?i)(open in|browser|teams)',
            r'\d{1,2}[/\-\.]\d{1,2}[/\-\.]\d{2,4}',
            r'\d{4}-\d{2}-\d{2}',
            r'\d{1,2}:\d{2}\s*(AM|PM|am|pm)?',
            r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',
            r'\+?[\d\s\-\(\)]{10,}',
        ]

    def extract_clean_content(self, content: str, content_type: str = 'html') -> Dict[str, str]:
        """ë©”ì¼ ë‚´ìš©ì—ì„œ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œ"""

        logger.info(f"ğŸ” ì»¨í…ì¸  ì¶”ì¶œ ì‹œì‘ - íƒ€ì…: {content_type}, ê¸¸ì´: {len(content) if content else 0}ì")
        
        if not content or not content.strip():
            logger.warning("âŒ ë¹ˆ ì»¨í…ì¸  - ì¶”ì¶œ ì¤‘ë‹¨")
            return {
                'cleaned_text': '',
                'summary': '',
                'key_points': [],
                'extraction_method': 'empty_content'
            }
        
        try:
            if content_type.lower() == 'html':
                logger.info("ğŸ” HTML ì»¨í…ì¸  ì¶”ì¶œ ì‹œì‘")
                result = self._extract_from_html(content)
                logger.info(f"âœ… HTML ì»¨í…ì¸  ì¶”ì¶œ ì™„ë£Œ - ì •ë¦¬ëœ í…ìŠ¤íŠ¸: {len(result['cleaned_text'])}ì")
                return result
            else:
                logger.info("ğŸ” í…ìŠ¤íŠ¸ ì»¨í…ì¸  ì¶”ì¶œ ì‹œì‘")
                result = self._extract_from_text(content)
                logger.info(f"âœ… í…ìŠ¤íŠ¸ ì»¨í…ì¸  ì¶”ì¶œ ì™„ë£Œ - ì •ë¦¬ëœ í…ìŠ¤íŠ¸: {len(result['cleaned_text'])}ì")
                return result
                
        except Exception as e:
            logger.error(f"âŒ ì»¨í…ì¸  ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return {
                'cleaned_text': self._basic_text_clean(content),
                'summary': self._basic_text_clean(content)[:200] + "..." if len(content) > 200 else self._basic_text_clean(content),
                'key_points': [],
                'extraction_method': f'fallback_due_to_error: {str(e)}'
            }

    def _extract_from_html(self, html_content: str) -> Dict[str, str]:
        """HTML ì½˜í…ì¸ ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        
        logger.debug(f"ğŸ” HTML ì›ë³¸ ë¯¸ë¦¬ë³´ê¸°: {html_content[:200]}...")

        # 1ë‹¨ê³„: ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±°
        logger.debug("ğŸ” 1ë‹¨ê³„: ë¶ˆí•„ìš”í•œ íŒ¨í„´ ì œê±° ì‹œì‘")
        cleaned_html = html_content
        for pattern in self.noise_patterns:
            before_len = len(cleaned_html)
            cleaned_html = re.sub(pattern, '', cleaned_html, flags=re.DOTALL | re.IGNORECASE)
            after_len = len(cleaned_html)
            if before_len != after_len:
                logger.debug(f"   íŒ¨í„´ ì œê±°: {before_len} -> {after_len}ì ({before_len - after_len}ì ì œê±°)")

        # 2ë‹¨ê³„: BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        logger.debug("ğŸ” 2ë‹¨ê³„: BeautifulSoup íŒŒì‹±")
        soup = BeautifulSoup(cleaned_html, 'html.parser')

        # 3ë‹¨ê³„: ì´ë¯¸ì§€ ì²˜ë¦¬ (ì¸ë¼ì¸ ì´ë¯¸ì§€ ì¶”ì¶œ)
        logger.debug("ğŸ” 3ë‹¨ê³„: ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘")
        images_found = len(soup.find_all('img'))
        logger.info(f"ğŸ–¼ï¸ ë°œê²¬ëœ ì´ë¯¸ì§€ ìˆ˜: {images_found}ê°œ")

        image_text = self._extract_image_content(soup)
        if image_text and image_text.strip():
            logger.info(f"âœ… ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(image_text)}ì")
            logger.debug(f"   ì¶”ì¶œëœ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸: {image_text[:200]}...")
        else:
            logger.warning("âŒ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨ ë˜ëŠ” ë¹ˆ ê²°ê³¼")

        # 4ë‹¨ê³„: ì´ë¯¸ì§€ íƒœê·¸ ì œê±° (ì •ë³´ ì¶”ì¶œ í›„)
        logger.debug("ğŸ” 4ë‹¨ê³„: ì´ë¯¸ì§€ íƒœê·¸ ì œê±°")
        for img in soup.find_all('img'):
            img.decompose()

        # 5ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ
        logger.debug("ğŸ” 5ë‹¨ê³„: í…ìŠ¤íŠ¸ ì¶”ì¶œ")
        text = soup.get_text()
        logger.debug(f"   ê¸°ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(text)}ì")

        # 6ë‹¨ê³„: ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì¶”ê°€
        logger.debug("ğŸ” 6ë‹¨ê³„: ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ê²°í•©")
        if image_text and image_text.strip():
            text += "\n\n[ì´ë¯¸ì§€ì—ì„œ ì¶”ì¶œëœ ë‚´ìš©]\n" + image_text
            logger.info(f"âœ… ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ ê²°í•© ì™„ë£Œ - ìµœì¢… ê¸¸ì´: {len(text)}ì")
        else:
            logger.warning("âš ï¸ ì´ë¯¸ì§€ í…ìŠ¤íŠ¸ê°€ ì—†ì–´ ê²°í•© ê±´ë„ˆëœ€")

        # 7ë‹¨ê³„: í…ìŠ¤íŠ¸ ì •ë¦¬
        cleaned_text = self._clean_text(text)

        # 8ë‹¨ê³„: ì¤‘ìš”í•œ ì •ë³´ ì¶”ì¶œ
        important_lines = self._extract_important_lines(cleaned_text)

        # 9ë‹¨ê³„: ìš”ì•½ ë° í•µì‹¬ í¬ì¸íŠ¸ ìƒì„±
        summary = self._generate_summary(important_lines, cleaned_text)
        key_points = self._extract_key_points(important_lines)

        return {
            'cleaned_text': cleaned_text,
            'summary': summary,
            'key_points': key_points,
            'extraction_method': 'enhanced_html_extraction_with_images'
        }

    def _extract_from_text(self, text_content: str) -> Dict[str, str]:
        """ì¼ë°˜ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        cleaned_text = self._clean_text(text_content)
        
        # ì¤‘ìš”í•œ ì •ë³´ ì¶”ì¶œ
        important_lines = self._extract_important_lines(cleaned_text)
        
        # ìš”ì•½ ë° í•µì‹¬ í¬ì¸íŠ¸ ìƒì„±
        summary = self._generate_summary(important_lines, cleaned_text)
        key_points = self._extract_key_points(important_lines)
        
        return {
            'cleaned_text': cleaned_text,
            'summary': summary,
            'key_points': key_points,
            'extraction_method': 'enhanced_text_extraction'
        }

    def _clean_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ë¦¬"""
        
        if not text:
            return ""
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = html.unescape(text)
        
        # CSS ìŠ¤íƒ€ì¼ ì œê±° (ë” ì •êµí•œ íŒ¨í„´)
        css_patterns = [
            # CSS ë¸”ë¡ íŒ¨í„´ (ì¤‘ê´„í˜¸ë¡œ ë‘˜ëŸ¬ì‹¸ì¸)
            r'\{[^}]*\}',  # ì¤‘ê´„í˜¸ ë¸”ë¡ ì „ì²´ ì œê±°
            r'@media[^{]*\{[^}]*\}',  # ë¯¸ë””ì–´ ì¿¼ë¦¬
            r'/\*.*?\*/',  # CSS ì£¼ì„
            r'@import[^;]*;',  # import ë¬¸
            r'@font-face[^}]*\}',  # font-face

            # CSS ì†ì„±ë“¤ (ë” êµ¬ì²´ì ì¸ íŒ¨í„´)
            r'(?:color|background|font|margin|padding|border|width|height|position|display|float|text-align|text-decoration):[^;]+;',
            r'(?:top|left|right|bottom|z-index|opacity|visibility|overflow):[^;]+;',

            # CSS ì„ íƒì (ë” ì•ˆì „í•œ íŒ¨í„´)
            r'^\s*\.[a-zA-Z0-9_-]+\s*$',  # ë‹¨ë… í´ë˜ìŠ¤ëª… ë¼ì¸
            r'^\s*#[a-zA-Z0-9_-]+\s*$',  # ë‹¨ë… ID ë¼ì¸

            # !important
            r'!important',
        ]

        for pattern in css_patterns:
            text = re.sub(pattern, ' ', text, flags=re.DOTALL | re.IGNORECASE | re.MULTILINE)
        
        # ê³¼ë„í•œ ê³µë°± ì •ë¦¬
        text = re.sub(r'[ \t]+', ' ', text)
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        # íŠ¹ìˆ˜ë¬¸ì ì •ë¦¬
        text = re.sub(r'[^\w\s\.\,\!\?\;\:\-\(\)\[\]\{\}@#\$%\^&\*\+\=\|\\\/\'\"`~<>]', ' ', text)
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ë¦¬ (ë” ì •êµí•œ ë¶„ë¦¬)
        # ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ, ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¬¸ì¥ ë¶„ë¦¬
        sentences = re.split(r'[.!?\n]+', text)
        
        cleaned_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and len(sentence) > 5:
                # CSS ì”ì¬ ì œê±° (ë¬¸ì¥ ë‹¨ìœ„)
                sentence = re.sub(r'^[0-9]+\s*', '', sentence)  # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” íŒ¨í„´
                sentence = re.sub(r'^[a-zA-Z0-9_-]+\s*$', '', sentence)  # CSS í´ë˜ìŠ¤ëª…ë§Œ ìˆëŠ” ë¼ì¸
                sentence = re.sub(r'^[#\.][a-zA-Z0-9_-]+\s*$', '', sentence)  # CSS ì„ íƒìë§Œ ìˆëŠ” ë¼ì¸
                sentence = re.sub(r'^[a-zA-Z0-9_-]+\s*:\s*$', '', sentence)  # CSS ì†ì„±ëª…ë§Œ ìˆëŠ” ë¼ì¸
                sentence = re.sub(r'^[a-zA-Z0-9_-]+\s*,\s*#\s*$', '', sentence)  # "Reddit , #" ê°™ì€ íŒ¨í„´
                sentence = re.sub(r'^[a-zA-Z0-9_-]+\s*,\s*$', '', sentence)  # "Reddit ," ê°™ì€ íŒ¨í„´
                
                if sentence and len(sentence) > 5:
                    # ì¶”ê°€ë¡œ íŠ¹ì • í‚¤ì›Œë“œë¡œ ë¶„ë¦¬
                    sub_sentences = re.split(r'(?i)(open in|due in|in the plan|privacy statement)', sentence)
                    for sub_sentence in sub_sentences:
                        sub_sentence = sub_sentence.strip()
                        if sub_sentence and len(sub_sentence) > 3:
                            cleaned_sentences.append(sub_sentence)
        
        # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
        unique_sentences = []
        seen = set()
        
        for sentence in cleaned_sentences:
            if sentence not in seen:
                unique_sentences.append(sentence)
                seen.add(sentence)
        
        return '\n'.join(unique_sentences)

    def _extract_important_lines(self, text: str) -> List[str]:
        """ì¤‘ìš”í•œ ë¼ì¸ ì¶”ì¶œ"""
        
        lines = text.split('\n')
        important_lines = []
        
        for line in lines:
            line = line.strip()
            if line and len(line) > 5:
                # ì¤‘ìš”í•œ íŒ¨í„´ í™•ì¸
                is_important = False
        for pattern in self.important_patterns:
                    if re.search(pattern, line, re.IGNORECASE):
                        is_important = True
                        break
                
                # ê¸¸ì´ ê¸°ë°˜ ì¤‘ìš”ë„
                if not is_important and 10 <= len(line) <= 500:
                    is_important = True
                
                if is_important:
                    important_lines.append(line)
        
        return important_lines

    def _generate_summary(self, important_lines: List[str], full_text: str) -> str:
        """ìš”ì•½ ìƒì„±"""
        
        if important_lines:
            # ì¤‘ìš”í•œ ë¼ì¸ë“¤ë¡œ ìš”ì•½ ìƒì„± (ì¤‘ë³µ ì œê±°)
            summary_parts = []
            for line in important_lines[:3]:
                if line not in summary_parts:
                    summary_parts.append(line)
            summary = ' | '.join(summary_parts)
        else:
            # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì²« 200ì
            summary = full_text
        
        # ê¸¸ì´ ì œí•œ
        if len(summary) > 300:
            summary = summary[:297] + "..."
        
        return summary.strip()

    def _extract_key_points(self, important_lines: List[str]) -> List[str]:
        """í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ"""
        
        key_points = []
        
        for line in important_lines[:5]:
            line = line.strip()
            if len(line) > 15 and len(line) < 300:
                if line not in key_points:
                    key_points.append(line)
        
        return key_points[:5]

    def _extract_image_content(self, soup: BeautifulSoup) -> str:
        """HTMLì—ì„œ ì¸ë¼ì¸ ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ"""
        try:
            image_texts = []
            images = soup.find_all('img')

            logger.debug(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ íƒœê·¸ {len(images)}ê°œ ë°œê²¬")

            for i, img in enumerate(images, 1):
                logger.debug(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ {i} ì²˜ë¦¬ ì‹œì‘")

                # Alt í…ìŠ¤íŠ¸ ì¶”ì¶œ
                alt_text = img.get('alt', '')
                if alt_text:
                    logger.debug(f"   Alt í…ìŠ¤íŠ¸: '{alt_text}'")

                # Title ì†ì„± ì¶”ì¶œ
                title_text = img.get('title', '')
                if title_text:
                    logger.debug(f"   Title í…ìŠ¤íŠ¸: '{title_text}'")

                # src ë¶„ì„ (base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ì¸ì§€ í™•ì¸)
                src = img.get('src', '')
                logger.debug(f"   Src: '{src[:100]}{'...' if len(src) > 100 else ''}'")

                # ì´ë¯¸ì§€ ì„¤ëª… ìƒì„±
                img_description = []

                # Alt í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
                if alt_text and alt_text.strip():
                    img_description.append(alt_text.strip())
                elif title_text and title_text.strip():
                    img_description.append(title_text.strip())

                # ì´ë¯¸ì§€ íƒ€ì…ë³„ ì²˜ë¦¬
                if src.startswith('data:image'):
                    # Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€
                    if not any('ì¸ë¼ì¸' in desc for desc in img_description):
                        img_description.append("ì¸ë¼ì¸ ì´ë¯¸ì§€")
                    logger.info(f"ğŸ–¼ï¸ Base64 ì´ë¯¸ì§€ ë°œê²¬ - í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„")

                    # base64 ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                    extracted_text = self._extract_text_from_base64_image(src)
                    if extracted_text and extracted_text.strip() and 'í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨' not in extracted_text:
                        img_description.append(f"ì¶”ì¶œëœ í…ìŠ¤íŠ¸: {extracted_text}")
                        logger.info(f"âœ… Base64 ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(extracted_text)}ì")
                    else:
                        logger.warning("âŒ Base64 ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")

                elif 'cid:' in src:
                    # ì²¨ë¶€ëœ ì´ë¯¸ì§€
                    if not any('ì²¨ë¶€' in desc for desc in img_description):
                        img_description.append("ì²¨ë¶€ëœ ì´ë¯¸ì§€")
                    logger.debug("ğŸ–¼ï¸ ì²¨ë¶€ ì´ë¯¸ì§€ (CID) ë°œê²¬")

                elif src:
                    # ì™¸ë¶€ ì´ë¯¸ì§€ URL ì²˜ë¦¬
                    if 'user=' in src and 'end=' in src:
                        if not any('ì‚¬ìš©ì' in desc for desc in img_description):
                            img_description.append("ì‚¬ìš©ì ê´€ë ¨ ì´ë¯¸ì§€")
                    elif 'http' in src:
                        # URLì—ì„œ ë„ë©”ì¸ë§Œ ì¶”ì¶œ
                        try:
                            from urllib.parse import urlparse
                            parsed = urlparse(src)
                            domain = parsed.netloc
                            if domain and not any(domain in desc for desc in img_description):
                                img_description.append(f"ì™¸ë¶€ ì´ë¯¸ì§€ ({domain})")
                        except:
                            if not any('ì™¸ë¶€' in desc for desc in img_description):
                                img_description.append("ì™¸ë¶€ ì´ë¯¸ì§€")
                    else:
                        if not any('ì™¸ë¶€' in desc for desc in img_description):
                            img_description.append("ì™¸ë¶€ ì´ë¯¸ì§€")
                    logger.debug("ğŸ–¼ï¸ ì™¸ë¶€ ì´ë¯¸ì§€ URL ë°œê²¬")

                if img_description:
                    # ì˜ë¯¸ìˆëŠ” ì„¤ëª…ì´ ìˆìœ¼ë©´ ê°„ë‹¨í•˜ê²Œ í‘œì‹œ
                    if len(img_description) == 1 and not any(x in img_description[0].lower() for x in ['ì™¸ë¶€ ì´ë¯¸ì§€', 'ì¸ë¼ì¸ ì´ë¯¸ì§€', 'ì²¨ë¶€ëœ ì´ë¯¸ì§€']):
                        image_texts.append(f"ì´ë¯¸ì§€ {i}: {img_description[0]}")
                    else:
                        # ì—¬ëŸ¬ ì •ë³´ê°€ ìˆìœ¼ë©´ ê²°í•©
                        combined_desc = '; '.join(img_description)
                        image_texts.append(f"ì´ë¯¸ì§€ {i}: {combined_desc}")
                    logger.debug(f"âœ… ì´ë¯¸ì§€ {i} ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
                else:
                    image_texts.append(f"ì´ë¯¸ì§€ {i}: ì´ë¯¸ì§€")
                    logger.debug(f"âš ï¸ ì´ë¯¸ì§€ {i} ì •ë³´ ì—†ìŒ")

            result = '\n'.join(image_texts) if image_texts else ""
            logger.info(f"ğŸ–¼ï¸ ì „ì²´ ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ ì™„ë£Œ: {len(result)}ì")
            return result

        except Exception as e:
            logger.error(f"âŒ ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
            return f"ì´ë¯¸ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}"

    def _extract_text_from_base64_image(self, base64_src: str) -> str:
        """Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            import base64
            import io
            from PIL import Image
            import os

            # data:image/png;base64,iVBORw0KGgoAAAA... í˜•ì‹ì—ì„œ base64 ë¶€ë¶„ ì¶”ì¶œ
            if ';base64,' in base64_src:
                base64_data = base64_src.split(';base64,')[1]
            else:
                return ""

            # base64 ë””ì½”ë”©
            image_data = base64.b64decode(base64_data)

            # PIL Imageë¡œ ë³€í™˜
            image = Image.open(io.BytesIO(image_data))

            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            import tempfile
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:
                image.save(temp_file.name, 'PNG')
                temp_path = temp_file.name

            try:
                # Azure Vision APIë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                azure_text = self._extract_with_azure_vision(temp_path)
                if azure_text and len(azure_text.strip()) > 10:
                    return azure_text

                # Tesseractë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                tesseract_text = self._extract_with_tesseract(temp_path)
                if tesseract_text and len(tesseract_text.strip()) > 5:
                    return tesseract_text

                return "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨"

            finally:
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                if os.path.exists(temp_path):
                    os.unlink(temp_path)

        except Exception as e:
            return f"Base64 ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}"

    def _extract_with_azure_vision(self, image_path: str) -> str:
        """Azure Vision APIë¡œ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            import requests
            import time
            import os

            endpoint = os.getenv("AZURE_VISION_ENDPOINT")
            key = os.getenv("AZURE_VISION_KEY")

            if not endpoint or not key:
                return ""

            # ì´ë¯¸ì§€ íŒŒì¼ ì½ê¸°
            with open(image_path, 'rb') as image_data:
                image_bytes = image_data.read()

            # OCR API í˜¸ì¶œ
            ocr_url = f"{endpoint}/vision/v3.2/read/analyze"
            headers = {
                'Ocp-Apim-Subscription-Key': key,
                'Content-Type': 'application/octet-stream'
            }

            response = requests.post(ocr_url, headers=headers, data=image_bytes)
            response.raise_for_status()

            # ê²°ê³¼ URL ê°€ì ¸ì˜¤ê¸°
            operation_location = response.headers["Operation-Location"]

            # ê²°ê³¼ ëŒ€ê¸° ë° ê°€ì ¸ì˜¤ê¸°
            for i in range(10):  # 2ì´ˆì”© 10ë²ˆ = 20ì´ˆ
                time.sleep(2)
                result_response = requests.get(operation_location, headers={'Ocp-Apim-Subscription-Key': key})
                result = result_response.json()

                if result["status"] == "succeeded":
                    # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    extracted_text = ""
                    for page in result.get("analyzeResult", {}).get("readResults", []):
                        for line in page.get("lines", []):
                            extracted_text += line.get("text", "") + "\n"

                    return extracted_text.strip()
                elif result["status"] == "failed":
                    break

            return ""

        except Exception:
            return ""

    def _extract_with_tesseract(self, image_path: str) -> str:
        """Tesseractë¡œ ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            import pytesseract
            from PIL import Image

            # ì´ë¯¸ì§€ ì—´ê¸°
            image = Image.open(image_path)

            # OCR ìˆ˜í–‰
            text = pytesseract.image_to_string(image, lang='kor+eng')

            return text.strip()

        except ImportError:
            return ""
        except Exception:
            return ""

    def _basic_text_clean(self, text: str) -> str:
        """ê¸°ë³¸ í…ìŠ¤íŠ¸ ì •ë¦¬ (í´ë°±ìš©)"""
        
        if not text:
            return ""
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = html.unescape(text)
        
        # ê³¼ë„í•œ ê³µë°± ì •ë¦¬
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        
        return text.strip()


def test_enhanced_extractor():
    """í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    
    extractor = EnhancedContentExtractor()
    
    # HTML í…ŒìŠ¤íŠ¸
    html_test = """
    <html>
    <head><style>body{color:red}</style></head>
    <body>
        <h2>Urgent Meeting Request</h2>
        <p>Hi John,</p>
        <p>We need to schedule an <strong>urgent meeting</strong> about the project deadline.</p>
        <p>Please contact me at <a href="mailto:john.doe@company.com">john.doe@company.com</a> or call +1-555-0123.</p>
        <p>Meeting time: <strong>2024-12-15 at 2:00 PM</strong></p>
        <div style="color:gray">This is an automated message...</div>
        <table><tr><td>Unsubscribe</td></tr></table>
    </body>
    </html>
    """
    
    result = extractor.extract_clean_content(html_test, 'html')
    print("í–¥ìƒëœ HTML í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"ì •ë¦¬ëœ í…ìŠ¤íŠ¸:\n{result['cleaned_text']}")
    print(f"\nìš”ì•½: {result['summary']}")
    print(f"í•µì‹¬ í¬ì¸íŠ¸: {result['key_points']}")
    print(f"ì¶”ì¶œ ë°©ë²•: {result['extraction_method']}")


if __name__ == "__main__":
    test_enhanced_extractor()