#!/usr/bin/env python3
"""
ì§€ëŠ¥í˜• ì²­í¬ ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ RAG Golden Set ìƒì„± ìŠ¤í¬ë¦½íŠ¸
ê¸°ì¡´ generate_golden_set_with_rag.pyì— ì§€ëŠ¥í˜• ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œì„ í†µí•©
"""

import os
import sys
import pandas as pd
import json
from datetime import datetime
from typing import Dict, List, Any
from dotenv import load_dotenv

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ê¸°ì¡´ ëª¨ë“ˆë“¤ import
from generate_golden_set import (
    initialize_azure_openai,
    generate_question_for_ticket,
    log_test_case,
    JIRA_CSV_FILE_PATH,
    QUESTIONS_PER_TICKET,
    MAX_TICKETS_TO_PROCESS
)

# RAG ê´€ë ¨ ëª¨ë“ˆë“¤ import
from multi_vector_cross_encoder_rag import MultiVectorCrossEncoderRAG
from intelligent_chunk_weighting import IntelligentChunkWeighting

class EnhancedRAGWithWeighting:
    """ì§€ëŠ¥í˜• ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ RAG ì‹œìŠ¤í…œ"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.base_rag = MultiVectorCrossEncoderRAG()
        self.weighting_system = IntelligentChunkWeighting()

    def search_with_intelligent_weighting(self, query: str, n_candidates: int = 50,
                                        top_k: int = 10) -> List[Dict[str, Any]]:
        """
        ì§€ëŠ¥í˜• ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            n_candidates: 1ë‹¨ê³„ í›„ë³´ ìˆ˜
            top_k: ìµœì¢… ê²°ê³¼ ìˆ˜

        Returns:
            ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            # 1. ê¸°ë³¸ RAG ê²€ìƒ‰ ìˆ˜í–‰
            base_results = self.base_rag.search(query, n_candidates, top_k)

            # 2. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            enhanced_results = []
            for result in base_results:
                # chunk_type ì¶”ì¶œ (ì‹¤ì œ ë©”íƒ€ë°ì´í„°ì—ì„œ)
                metadata = result.get('metadata', {})

                # ì‹¤ì œ chunk_type ì •ë³´ ë³µì› ì‹œë„
                chunk_type = self._extract_real_chunk_type(result)

                enhanced_result = {
                    'id': result.get('id', ''),
                    'content': result.get('content', ''),
                    'chunk_type': chunk_type,
                    'cosine_score': result.get('similarity_score', 0.0),
                    'embedding': [],  # ì„ë² ë”©ì€ ì¬ê³„ì‚°í•˜ì§€ ì•ŠìŒ
                    'metadata': {
                        **metadata,
                        'original_score': result.get('similarity_score', 0.0),
                        'search_method': 'enhanced_rag_with_weighting'
                    }
                }
                enhanced_results.append(enhanced_result)

            # 3. Mock ì¿¼ë¦¬ ì„ë² ë”© (ì‹¤ì œë¡œëŠ” ê²€ìƒ‰ ì‹œ ì‚¬ìš©ëœ ì„ë² ë”© í™œìš©)
            import numpy as np
            mock_query_embedding = np.random.normal(0, 1, 768).tolist()

            # 4. ê°€ì¤‘ì¹˜ ì ìš©
            weighted_results = self.weighting_system.apply_weighted_scoring(
                enhanced_results, mock_query_embedding
            )

            # 5. ê²°ê³¼ë¥¼ ì›ë˜ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            final_results = []
            for weighted_result in weighted_results:
                final_result = {
                    "id": weighted_result.id,
                    "content": weighted_result.content,
                    "similarity_score": weighted_result.weighted_score,  # ê°€ì¤‘ì¹˜ ì ìš©ëœ ì ìˆ˜
                    "source": "enhanced_rag_with_weighting",
                    "metadata": {
                        **weighted_result.metadata,
                        'chunk_type': weighted_result.chunk_type,
                        'weight': weighted_result.weight,
                        'original_cosine_score': weighted_result.cosine_score,
                        'weighted_score': weighted_result.weighted_score,
                        'weight_applied': True
                    }
                }
                final_results.append(final_result)

            return final_results

        except Exception as e:
            print(f"âŒ ê°€ì¤‘ì¹˜ ì ìš© ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ RAG ê²°ê³¼ ë°˜í™˜
            return self.base_rag.search(query, n_candidates, top_k)

    def _extract_real_chunk_type(self, result: Dict[str, Any]) -> str:
        """
        ì‹¤ì œ chunk_type ì¶”ì¶œ (content ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±)

        Args:
            result: ê²€ìƒ‰ ê²°ê³¼

        Returns:
            ì¶”ì¶œëœ chunk_type
        """
        content = result.get('content', '').lower()
        metadata = result.get('metadata', {})

        # ë©”íƒ€ë°ì´í„°ì—ì„œ chunk_type í™•ì¸
        if 'chunk_type' in metadata and metadata['chunk_type'] != 'multi_vector':
            return metadata['chunk_type']

        # content ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± ë¶„ë¥˜
        if any(keyword in content for keyword in ['ì œëª©:', 'íƒ€ì´í‹€:', 'title:']):
            return 'title'
        elif any(keyword in content for keyword in ['ìš”ì•½:', 'summary:', 'ê°œìš”:']):
            return 'summary'
        elif any(keyword in content for keyword in ['ì„¤ëª…:', 'description:', 'ìƒì„¸:']):
            return 'description'
        elif any(keyword in content for keyword in ['ëŒ“ê¸€:', 'comment:', 'ì˜ê²¬:']):
            return 'comment'
        elif any(keyword in content for keyword in ['í—¤ë”:', 'header:']):
            return 'header'
        elif len(content) < 100:  # ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ì œëª©ìœ¼ë¡œ ì¶”ì •
            return 'title'
        elif len(content) > 500:  # ê¸´ í…ìŠ¤íŠ¸ëŠ” ë³¸ë¬¸ìœ¼ë¡œ ì¶”ì •
            return 'body'
        else:
            return 'description'  # ì¤‘ê°„ ê¸¸ì´ëŠ” ì„¤ëª…ìœ¼ë¡œ ì¶”ì •

def search_rag_with_weighting(query: str) -> List[Dict[str, Any]]:
    """
    ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ RAG ê²€ìƒ‰ í•¨ìˆ˜ (generate_golden_set_with_rag.py í˜¸í™˜)

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬

    Returns:
        ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    try:
        # ê°€ì¤‘ì¹˜ ì ìš© RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        enhanced_rag = EnhancedRAGWithWeighting()

        # ê°€ì¤‘ì¹˜ ì ìš© ê²€ìƒ‰ ì‹¤í–‰
        results = enhanced_rag.search_with_intelligent_weighting(
            query=query,
            n_candidates=50,  # 1ë‹¨ê³„ í›„ë³´ ìˆ˜
            top_k=10         # ìµœì¢… ê²°ê³¼ ìˆ˜
        )

        # ê²°ê³¼ë¥¼ ê¸°ì¡´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        formatted_results = []
        for i, item in enumerate(results):
            # ê°€ì¤‘ì¹˜ ì •ë³´ í¬í•¨í•˜ì—¬ ê²°ê³¼ í¬ë§·íŒ…
            result = {
                "id": item.get("id", f"ITEM-{i}"),
                "content": item.get("content", ""),
                "score": item.get("similarity_score", 0.0),
                "raw_score": item.get("metadata", {}).get("original_cosine_score", item.get("similarity_score", 0.0)),
                "similarity_to_query": item.get("similarity_score", 0.0),
                "source": "enhanced_rag_with_weighting"
            }

            # ê°€ì¤‘ì¹˜ ì ìš© ê²°ê³¼ì¸ ê²½ìš° ìƒì„¸ ì •ë³´ ì¶”ê°€
            metadata = item.get('metadata', {})
            if metadata.get('weight_applied'):
                chunk_type = metadata.get('chunk_type', 'unknown')
                weight = metadata.get('weight', 1.0)
                weighted_score = metadata.get('weighted_score', 0.0)
                original_score = metadata.get('original_cosine_score', 0.0)

                # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                content = item.get('content', 'No Content')
                if len(content) > 200:
                    content_preview = content[:200] + "..."
                else:
                    content_preview = content

                # ê°€ì¤‘ì¹˜ ì •ë³´ë¥¼ í¬í•¨í•œ ìƒì„¸ ë‚´ìš©
                result["content"] = f"[ê°€ì¤‘ì¹˜ ì ìš©] {chunk_type.upper()} (weight: {weight:.1f})\n" \
                                  f"ì›ë³¸ ì ìˆ˜: {original_score:.4f} â†’ ê°€ì¤‘ì¹˜ ì ìˆ˜: {weighted_score:.4f}\n" \
                                  f"ë‚´ìš©: {content_preview}"

            formatted_results.append(result)

        print(f"ğŸ” ê°€ì¤‘ì¹˜ ì ìš© RAG ê²€ìƒ‰ ì‹¤í–‰: '{query}' -> {len(formatted_results)}ê°œ ê²°ê³¼")

        # ê°€ì¤‘ì¹˜ íš¨ê³¼ ìš”ì•½ ì¶œë ¥
        if formatted_results:
            weighted_items = [r for r in results if r.get('metadata', {}).get('weight_applied')]
            if weighted_items:
                print(f"   âš–ï¸ ê°€ì¤‘ì¹˜ ì ìš©ëœ ê²°ê³¼: {len(weighted_items)}ê°œ")
                for item in weighted_items[:3]:  # ìƒìœ„ 3ê°œë§Œ í‘œì‹œ
                    metadata = item.get('metadata', {})
                    chunk_type = metadata.get('chunk_type', 'unknown')
                    weight = metadata.get('weight', 1.0)
                    improvement = metadata.get('weighted_score', 0) - metadata.get('original_cosine_score', 0)
                    print(f"      - {chunk_type}: ê°€ì¤‘ì¹˜ {weight:.1f}, ì ìˆ˜ í–¥ìƒ {improvement:+.4f}")

        return formatted_results

    except Exception as e:
        print(f"âŒ ê°€ì¤‘ì¹˜ ì ìš© RAG ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        # ì‹¤íŒ¨ ì‹œ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        return []

def create_enhanced_test_questions() -> List[str]:
    """ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸ì— íŠ¹í™”ëœ ì§ˆë¬¸ë“¤ ìƒì„±"""
    return [
        # title ìš°ì„  ì§ˆë¬¸ (title chunkê°€ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒ)
        "ì„œë²„ ì ‘ì† ë¬¸ì œ í•´ê²° ë°©ë²•",
        "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜",
        "ë¡œê·¸ì¸ ì‹œìŠ¤í…œ ì¥ì• ",

        # description ìš°ì„  ì§ˆë¬¸ (description chunkê°€ ë†’ì€ ì ìˆ˜ë¥¼ ë°›ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒ)
        "ë©”ì¸ ì„œë²„ì— ì ‘ì†í•  ìˆ˜ ì—†ì„ ë•Œ í™•ì¸í•´ì•¼ í•  ì‚¬í•­ë“¤ì„ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤",
        "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ëŠì–´ì§€ëŠ” ë¬¸ì œì˜ ì›ì¸ê³¼ í•´ê²°ì±…ì„ ì„¤ëª…í•´ì£¼ì„¸ìš”",
        "ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ê°œì„  ë°©ì•ˆì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì´ í•„ìš”í•©ë‹ˆë‹¤",

        # comment ìš°ì„  ì§ˆë¬¸ (comment chunkëŠ” ë‚®ì€ ì ìˆ˜ë¥¼ ë°›ì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒ)
        "ë‹¤ë¥¸ ì‚¬ìš©ìë“¤ì˜ ê²½í—˜ë‹´ì´ë‚˜ ì˜ê²¬ì„ ì•Œê³  ì‹¶ì–´ìš”",
        "ì´ ë¬¸ì œì— ëŒ€í•œ ì»¤ë®¤ë‹ˆí‹° í”¼ë“œë°±ì€ ì–´ë–¤ê°€ìš”",
        "ë¹„ìŠ·í•œ ê²½í—˜ì„ í•œ ì‚¬ëŒë“¤ì˜ í•´ê²° ë°©ë²•ì„ ì°¸ê³ í•˜ê³  ì‹¶ìŠµë‹ˆë‹¤"
    ]

def run_enhanced_weighting_test():
    """ê°€ì¤‘ì¹˜ íš¨ê³¼ë¥¼ í™•ì¸í•˜ëŠ” ì „ìš© í…ŒìŠ¤íŠ¸"""
    print("="*80)
    print("ğŸ¯ ì§€ëŠ¥í˜• ê°€ì¤‘ì¹˜ íš¨ê³¼ ê²€ì¦ í…ŒìŠ¤íŠ¸")
    print("="*80)

    test_questions = create_enhanced_test_questions()

    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}: {question}")
        print("-" * 60)

        # ê°€ì¤‘ì¹˜ ì ìš© ê²€ìƒ‰
        results = search_rag_with_weighting(question)

        if results:
            print(f"âœ… {len(results)}ê°œ ê²°ê³¼ (ê°€ì¤‘ì¹˜ ì ìš©)")

            # ìƒìœ„ 3ê°œ ê²°ê³¼ì˜ ê°€ì¤‘ì¹˜ íš¨ê³¼ ë¶„ì„
            for j, result in enumerate(results[:3], 1):
                score = result.get('score', 0)
                print(f"  {j}. ID: {result.get('id', 'Unknown')} (ì ìˆ˜: {score:.4f})")

                # ê°€ì¤‘ì¹˜ ì •ë³´ ì¶”ì¶œ
                content = result.get('content', '')
                if '[ê°€ì¤‘ì¹˜ ì ìš©]' in content:
                    lines = content.split('\n')
                    if len(lines) > 1:
                        print(f"     {lines[1]}")  # ì ìˆ˜ ë³€í™” ì •ë³´ ì¶œë ¥
        else:
            print("âŒ ê²°ê³¼ ì—†ìŒ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ì§€ëŠ¥í˜• ê°€ì¤‘ì¹˜ ì ìš© RAG Golden Set ìƒì„± ìŠ¤í¬ë¦½íŠ¸")

    # ì‚¬ìš©ìì—ê²Œ í…ŒìŠ¤íŠ¸ ìœ í˜• ì„ íƒ ìš”ì²­
    print("\nğŸ“‹ í…ŒìŠ¤íŠ¸ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ê¸°ì¡´ Golden Set ìƒì„± (ê°€ì¤‘ì¹˜ ì ìš©)")
    print("2. ê°€ì¤‘ì¹˜ íš¨ê³¼ ê²€ì¦ ì „ìš© í…ŒìŠ¤íŠ¸")
    print("3. ë‘˜ ë‹¤ ì‹¤í–‰")

    choice = input("\nì„ íƒ (1/2/3): ").strip()

    if choice == "1":
        run_original_golden_set_with_weighting()
    elif choice == "2":
        run_enhanced_weighting_test()
    elif choice == "3":
        run_original_golden_set_with_weighting()
        print("\n" + "="*80)
        run_enhanced_weighting_test()
    else:
        print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. ê°€ì¤‘ì¹˜ íš¨ê³¼ ê²€ì¦ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        run_enhanced_weighting_test()

def run_original_golden_set_with_weighting():
    """ê¸°ì¡´ Golden Set ìƒì„± ë¡œì§ (ê°€ì¤‘ì¹˜ ì ìš© ë²„ì „)"""
    try:
        # Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        llm_client = initialize_azure_openai()

        # CSV íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(JIRA_CSV_FILE_PATH):
            print(f"âš ï¸ Jira CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {JIRA_CSV_FILE_PATH}")
            print("ê°€ì¤‘ì¹˜ íš¨ê³¼ ê²€ì¦ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            run_enhanced_weighting_test()
            return

        # CSV íŒŒì¼ ì½ê¸°
        print(f"ğŸ“– CSV íŒŒì¼ ì½ê¸°: {JIRA_CSV_FILE_PATH}")
        df = pd.read_csv(JIRA_CSV_FILE_PATH)
        print(f"âœ… {len(df)}ê°œì˜ í‹°ì¼“ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

        # ì²˜ë¦¬í•  í‹°ì¼“ ìˆ˜ ì œí•œ
        if len(df) > MAX_TICKETS_TO_PROCESS:
            df = df.head(MAX_TICKETS_TO_PROCESS)
            print(f"âš ï¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ {MAX_TICKETS_TO_PROCESS}ê°œ í‹°ì¼“ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        # ë¡œê·¸ íŒŒì¼ëª…ì— ê°€ì¤‘ì¹˜ í‘œì‹œ ì¶”ê°€
        output_file = f"golden_set_results_with_intelligent_weighting_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

        # ë¡œê·¸ íŒŒì¼ ì—´ê¸°
        with open(output_file, 'w', encoding='utf-8') as log_file:
            # í—¤ë” ì •ë³´ ê¸°ë¡
            log_file.write("="*80 + "\n")
            log_file.write("RAG ì‹œìŠ¤í…œ Golden Set ìƒì„± ê²°ê³¼ (ì§€ëŠ¥í˜• ê°€ì¤‘ì¹˜ ì ìš©)\n")
            log_file.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            log_file.write(f"ì²˜ë¦¬ëœ í‹°ì¼“ ìˆ˜: {len(df)}\n")
            log_file.write(f"í‹°ì¼“ë‹¹ ì§ˆë¬¸ ìˆ˜: {QUESTIONS_PER_TICKET}\n")
            log_file.write("\nğŸ¯ ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ ì •ë³´:\n")
            log_file.write("- title: 1.5ë°°, summary: 1.3ë°°, description: 1.2ë°°\n")
            log_file.write("- body: 1.0ë°° (ê¸°ë³¸), comment: 0.8ë°°, attachment: 0.6ë°°\n")
            log_file.write("- ê°€ì¤‘ì¹˜ ì ìš©ëœ ì ìˆ˜ = ì›ë³¸ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ Ã— chunk_type ê°€ì¤‘ì¹˜\n")
            log_file.write("="*80 + "\n")

            # ê° í‹°ì¼“ ì²˜ë¦¬
            for index, row in df.iterrows():
                test_case_num = index + 1
                ticket = row.to_dict()

                print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ #{test_case_num} ì²˜ë¦¬ ì¤‘...")
                print(f"   í‹°ì¼“ ID: {ticket.get('Key', 'Unknown')}")

                try:
                    # ì§ˆë¬¸ ìƒì„±
                    question = generate_question_for_ticket(ticket, llm_client)
                    print(f"   ìƒì„±ëœ ì§ˆë¬¸: {question}")

                    # ê°€ì¤‘ì¹˜ ì ìš© RAG ê²€ìƒ‰ ì‹¤í–‰
                    rag_results = search_rag_with_weighting(question)

                    # ê²°ê³¼ ë¡œê·¸ì— ê¸°ë¡
                    log_test_case(log_file, test_case_num, ticket, question, rag_results)

                    print(f"   âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ #{test_case_num} ì™„ë£Œ")

                except Exception as e:
                    print(f"   âŒ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ #{test_case_num} ì‹¤íŒ¨: {str(e)}")
                    log_file.write(f"\n--- [Test Case #{test_case_num}] ---\n")
                    log_file.write(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n")
                    log_file.write("="*80 + "\n")
                    continue

        print(f"\nğŸ‰ ì§€ëŠ¥í˜• ê°€ì¤‘ì¹˜ ì ìš© Golden Set ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼: {output_file}")
        print(f"ğŸ“Š ì²˜ë¦¬ëœ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(df)}ê°œ")

    except Exception as e:
        print(f"âŒ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        return 1

    return 0

if __name__ == "__main__":
    # ì „ì—­ search_rag í•¨ìˆ˜ë¥¼ ê°€ì¤‘ì¹˜ ë²„ì „ìœ¼ë¡œ êµì²´
    import sys
    current_module = sys.modules[__name__]
    current_module.search_rag = search_rag_with_weighting

    exit_code = main()
    exit(exit_code)