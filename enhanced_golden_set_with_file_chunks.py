#!/usr/bin/env python3
"""
file_chunks ë°ì´í„°ë¥¼ í™œìš©í•œ ê°€ì¤‘ì¹˜ ì ìš© Golden Set í…ŒìŠ¤íŠ¸
jira_multi_vector_chunks ëŒ€ì‹  ê¸°ì¡´ file_chunks í™œìš©
"""

import os
import sys
import pandas as pd
import json
from datetime import datetime
from typing import Dict, List, Any
from dotenv import load_dotenv
import chromadb
from chromadb.config import Settings

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# ê¸°ì¡´ ëª¨ë“ˆë“¤ import
from generate_golden_set import (
    initialize_azure_openai,
    generate_question_for_ticket,
    JIRA_CSV_FILE_PATH,
    QUESTIONS_PER_TICKET,
    MAX_TICKETS_TO_PROCESS
)

from intelligent_chunk_weighting import IntelligentChunkWeighting
import numpy as np

class FileChunksRAGWithWeighting:
    """file_chunks ë°ì´í„°ë¥¼ í™œìš©í•œ ê°€ì¤‘ì¹˜ ì ìš© RAG ì‹œìŠ¤í…œ"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.client = chromadb.PersistentClient(
            path='./vector_db',
            settings=Settings(anonymized_telemetry=False, allow_reset=True)
        )
        self.collection = self.client.get_collection("file_chunks")
        self.weighting_system = IntelligentChunkWeighting()
        print(f"ğŸ“Š file_chunks ì»¬ë ‰ì…˜ ì—°ê²°: {self.collection.count()}ê°œ ë¬¸ì„œ")

    def search_with_weighting(self, query: str, n_results: int = 10) -> List[Dict[str, Any]]:
        """
        file_chunksì—ì„œ ê°€ì¤‘ì¹˜ ì ìš© ê²€ìƒ‰

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            n_results: ê²°ê³¼ ìˆ˜

        Returns:
            ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ê²€ìƒ‰ ê²°ê³¼
        """
        try:
            # 1. ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰ (ê¸°ì¡´ ì„ë² ë”© í•¨ìˆ˜ ì‚¬ìš©)
            basic_results = self.collection.query(
                query_texts=[query],
                n_results=n_results
            )

            if not basic_results['ids'][0]:
                return []

            # 2. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            search_results = []
            for i in range(len(basic_results['ids'][0])):
                metadata = basic_results['metadatas'][0][i] if basic_results['metadatas'][0] else {}
                content = basic_results['documents'][0][i] if basic_results['documents'][0] else ""
                distance = basic_results['distances'][0][i] if basic_results['distances'][0] else 1.0

                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ë³€í™˜
                cosine_score = max(0.0, 1.0 - distance)

                # chunk_type ì¶”ì •
                chunk_type = self._estimate_chunk_type(metadata, content)

                search_result = {
                    'id': basic_results['ids'][0][i],
                    'content': content,
                    'chunk_type': chunk_type,
                    'cosine_score': cosine_score,
                    'embedding': [],
                    'metadata': {
                        **metadata,
                        'estimated_chunk_type': chunk_type,
                        'original_distance': distance
                    }
                }
                search_results.append(search_result)

            # 3. ê°€ì¤‘ì¹˜ ì ìš© (Mock ì¿¼ë¦¬ ì„ë² ë”© ì‚¬ìš© - 384ì°¨ì›)
            mock_query_embedding = np.random.normal(0, 1, 384).tolist()
            weighted_results = self.weighting_system.apply_weighted_scoring(
                search_results, mock_query_embedding
            )

            # 4. ê²°ê³¼ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            final_results = []
            for weighted_result in weighted_results:
                result = {
                    "id": weighted_result.id,
                    "content": weighted_result.content,
                    "score": weighted_result.weighted_score,
                    "raw_score": weighted_result.cosine_score,
                    "similarity_to_query": weighted_result.weighted_score,
                    "source": "file_chunks_with_weighting"
                }

                # ê°€ì¤‘ì¹˜ ì •ë³´ í¬í•¨í•œ ìƒì„¸ ë‚´ìš©
                chunk_type = weighted_result.chunk_type
                weight = weighted_result.weight
                content_preview = weighted_result.content[:200].replace('\n', ' ') + "..."

                result["content"] = f"[ê°€ì¤‘ì¹˜ ì ìš©] {chunk_type.upper()} (weight: {weight:.1f})\n" \
                                  f"ì›ë³¸ ì ìˆ˜: {weighted_result.cosine_score:.4f} â†’ ê°€ì¤‘ì¹˜ ì ìˆ˜: {weighted_result.weighted_score:.4f}\n" \
                                  f"ë‚´ìš©: {content_preview}"

                final_results.append(result)

            return final_results

        except Exception as e:
            print(f"âŒ file_chunks ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _estimate_chunk_type(self, metadata: Dict[str, Any], content: str) -> str:
        """ë©”íƒ€ë°ì´í„°ì™€ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ chunk_type ì¶”ì •"""
        content_lower = content.lower()

        # 1. êµ¬ì¡°ì  íŒ¨í„´ ë¶„ì„
        if any(pattern in content_lower for pattern in ['ì œëª©:', 'title:', 'ì´ìŠˆ í‚¤:', 'issue key:']):
            return 'title'
        elif any(pattern in content_lower for pattern in ['ìš”ì•½:', 'summary:', 'ê°œìš”:']):
            return 'summary'
        elif any(pattern in content_lower for pattern in ['ì„¤ëª…:', 'description:', 'ìƒì„¸:', 'ë‚´ìš©:']):
            return 'description'
        elif any(pattern in content_lower for pattern in ['ëŒ“ê¸€:', 'comment:', 'ì˜ê²¬:', 'í”¼ë“œë°±:']):
            return 'comment'
        elif any(pattern in content_lower for pattern in ['í—¤ë”:', 'header:']):
            return 'header'

        # 2. ë‚´ìš© ê¸¸ì´ ê¸°ë°˜
        elif len(content.strip()) < 30:
            return 'title'
        elif len(content.strip()) < 100:
            return 'summary'
        elif len(content.strip()) > 1000:
            return 'body'
        else:
            return 'description'

def search_rag_with_file_chunks(query: str) -> List[Dict[str, Any]]:
    """
    file_chunksë¥¼ í™œìš©í•œ ê°€ì¤‘ì¹˜ ì ìš© RAG ê²€ìƒ‰

    Args:
        query: ê²€ìƒ‰ ì¿¼ë¦¬

    Returns:
        ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
    """
    try:
        # file_chunks ê¸°ë°˜ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        rag_system = FileChunksRAGWithWeighting()

        # ê²€ìƒ‰ ì‹¤í–‰
        results = rag_system.search_with_weighting(query, n_results=10)

        print(f"ğŸ” file_chunks ê°€ì¤‘ì¹˜ ì ìš© ê²€ìƒ‰: '{query}' -> {len(results)}ê°œ ê²°ê³¼")

        # ê°€ì¤‘ì¹˜ íš¨ê³¼ ìš”ì•½
        if results:
            weighted_items = [r for r in results if '[ê°€ì¤‘ì¹˜ ì ìš©]' in r.get('content', '')]
            if weighted_items:
                print(f"   âš–ï¸ ê°€ì¤‘ì¹˜ ì ìš©ëœ ê²°ê³¼: {len(weighted_items)}ê°œ")

                # chunk_typeë³„ í†µê³„
                chunk_types = {}
                for item in weighted_items[:5]:
                    content = item.get('content', '')
                    if '[ê°€ì¤‘ì¹˜ ì ìš©]' in content:
                        # chunk_type ì¶”ì¶œ
                        lines = content.split('\n')
                        if len(lines) > 0:
                            first_line = lines[0]
                            if '(' in first_line and ')' in first_line:
                                chunk_type = first_line.split('[ê°€ì¤‘ì¹˜ ì ìš©]')[1].split('(')[0].strip()
                                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1

                for chunk_type, count in chunk_types.items():
                    print(f"      - {chunk_type}: {count}ê°œ")

        return results

    except Exception as e:
        print(f"âŒ file_chunks ê°€ì¤‘ì¹˜ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
        return []

def log_test_case_with_weighting(log_file, test_case_num: int, ticket: Dict[str, Any],
                                question: str, rag_results: List[Dict[str, Any]]):
    """
    ê°€ì¤‘ì¹˜ ì •ë³´ë¥¼ í¬í•¨í•œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë¡œê¹…

    Args:
        log_file: ë¡œê·¸ íŒŒì¼ ê°ì²´
        test_case_num: í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ë²ˆí˜¸
        ticket: ì›ë³¸ í‹°ì¼“ ì •ë³´
        question: ìƒì„±ëœ ì§ˆë¬¸
        rag_results: ê°€ì¤‘ì¹˜ ì ìš©ëœ RAG ê²€ìƒ‰ ê²°ê³¼
    """
    ticket_id = ticket.get('Key', 'Unknown')
    ticket_summary = ticket.get('Summary', 'No Summary')
    ticket_description = ticket.get('Description', 'No Description')

    log_file.write(f"\n--- [Test Case #{test_case_num}] ---\n")
    log_file.write(f"ğŸ¯ ì •ë‹µ í‹°ì¼“: {ticket_id} ({ticket_summary})\n")
    log_file.write(f"ğŸ“ í‹°ì¼“ ì„¤ëª…: {ticket_description}\n")
    log_file.write(f"ğŸ§  ìƒì„±ëœ ì§ˆë¬¸: {question}\n")
    log_file.write(f"ğŸ” file_chunks ê°€ì¤‘ì¹˜ ì ìš© ê²€ìƒ‰ ê²°ê³¼ (Top {len(rag_results)}):\n")

    for i, result in enumerate(rag_results, 1):
        result_id = result.get('id', 'Unknown')
        result_content = result.get('content', 'No Content')
        result_score = result.get('score', 0.0)
        raw_score = result.get('raw_score', result_score)

        log_file.write(f"\n   {i}. ID: {result_id} (ê°€ì¤‘ì¹˜ ì ìˆ˜: {result_score:.4f}, ì›ë³¸: {raw_score:.4f})\n")
        log_file.write(f"      ë‚´ìš©:\n")

        # ë‚´ìš©ì„ ì—¬ëŸ¬ ì¤„ë¡œ ë‚˜ëˆ„ì–´ í‘œì‹œ
        content_lines = result_content.split('\n')
        for line in content_lines:
            log_file.write(f"      {line}\n")

        log_file.write(f"      {'-' * 60}\n")

    log_file.write("\n" + "="*80 + "\n")

def create_weighted_test_questions() -> List[str]:
    """ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸ì— íŠ¹í™”ëœ ì§ˆë¬¸ë“¤"""
    return [
        # ë‹¤ì–‘í•œ chunk_typeì„ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ë“¤
        "ì‹œìŠ¤í…œ ì œëª©ì´ë‚˜ í—¤ë”ì— ê´€ë ¨ëœ ì •ë³´",  # title ìš°ì„ 
        "í”„ë¡œì íŠ¸ ìš”ì•½ì´ë‚˜ ê°œìš” ì •ë³´",        # summary ìš°ì„ 
        "ìƒì„¸í•œ ì„¤ëª…ì´ë‚˜ êµ¬í˜„ ë°©ë²•",          # description ìš°ì„ 
        "ì‚¬ìš©ì í”¼ë“œë°±ì´ë‚˜ ëŒ“ê¸€ ë‚´ìš©",        # comment ìš°ì„ 
        "ì „ì²´ì ì¸ ì‹œìŠ¤í…œ êµ¬ì¡°ë‚˜ ë³¸ë¬¸"         # body ìš°ì„ 
    ]

def run_weighted_test_only():
    """ê°€ì¤‘ì¹˜ íš¨ê³¼ë§Œ í™•ì¸í•˜ëŠ” ì „ìš© í…ŒìŠ¤íŠ¸"""
    print("="*80)
    print("ğŸ¯ file_chunks ê°€ì¤‘ì¹˜ íš¨ê³¼ ê²€ì¦ í…ŒìŠ¤íŠ¸")
    print("="*80)

    test_questions = create_weighted_test_questions()

    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}: {question}")
        print("-" * 60)

        # ê°€ì¤‘ì¹˜ ì ìš© ê²€ìƒ‰
        results = search_rag_with_file_chunks(question)

        if results:
            print(f"âœ… {len(results)}ê°œ ê²°ê³¼")

            # ìƒìœ„ 3ê°œ ê²°ê³¼ì˜ ê°€ì¤‘ì¹˜ íš¨ê³¼ í‘œì‹œ
            for j, result in enumerate(results[:3], 1):
                score = result.get('score', 0)
                raw_score = result.get('raw_score', 0)
                improvement = score - raw_score
                print(f"  {j}. ID: {result.get('id', 'Unknown')[:12]}...")
                print(f"     ê°€ì¤‘ì¹˜ íš¨ê³¼: {raw_score:.4f} â†’ {score:.4f} ({improvement:+.4f})")

                # chunk_type ì •ë³´ ì¶”ì¶œ
                content = result.get('content', '')
                if '[ê°€ì¤‘ì¹˜ ì ìš©]' in content:
                    lines = content.split('\n')
                    if len(lines) > 0:
                        print(f"     {lines[0]}")  # chunk_typeê³¼ weight ì •ë³´
        else:
            print("âŒ ê²°ê³¼ ì—†ìŒ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ file_chunks ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš© Golden Set í…ŒìŠ¤íŠ¸")

    # í…ŒìŠ¤íŠ¸ ìœ í˜• ì„ íƒ
    print("\nğŸ“‹ í…ŒìŠ¤íŠ¸ ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. ê¸°ì¡´ Golden Set ìƒì„± (file_chunks + ê°€ì¤‘ì¹˜)")
    print("2. ê°€ì¤‘ì¹˜ íš¨ê³¼ ê²€ì¦ ì „ìš© í…ŒìŠ¤íŠ¸")

    choice = input("\nì„ íƒ (1/2): ").strip()

    if choice == "1":
        run_golden_set_with_file_chunks()
    else:
        run_weighted_test_only()

def run_golden_set_with_file_chunks():
    """ê¸°ì¡´ Golden Set ìƒì„± (file_chunks ë²„ì „)"""
    try:
        # CSV íŒŒì¼ í™•ì¸
        if not os.path.exists(JIRA_CSV_FILE_PATH):
            print(f"âš ï¸ Jira CSV íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {JIRA_CSV_FILE_PATH}")
            print("ê°€ì¤‘ì¹˜ íš¨ê³¼ ê²€ì¦ í…ŒìŠ¤íŠ¸ë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
            run_weighted_test_only()
            return

        # Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        llm_client = initialize_azure_openai()

        # CSV íŒŒì¼ ì½ê¸°
        print(f"ğŸ“– CSV íŒŒì¼ ì½ê¸°: {JIRA_CSV_FILE_PATH}")
        df = pd.read_csv(JIRA_CSV_FILE_PATH)
        print(f"âœ… {len(df)}ê°œì˜ í‹°ì¼“ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

        # ì²˜ë¦¬í•  í‹°ì¼“ ìˆ˜ ì œí•œ
        if len(df) > MAX_TICKETS_TO_PROCESS:
            df = df.head(MAX_TICKETS_TO_PROCESS)
            print(f"âš ï¸ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ {MAX_TICKETS_TO_PROCESS}ê°œ í‹°ì¼“ë§Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.")

        # ë¡œê·¸ íŒŒì¼ëª…
        output_file = f"file_chunks_weighted_golden_set_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

        # ë¡œê·¸ íŒŒì¼ ìƒì„±
        with open(output_file, 'w', encoding='utf-8') as log_file:
            # í—¤ë” ì •ë³´
            log_file.write("="*80 + "\n")
            log_file.write("file_chunks ê¸°ë°˜ ê°€ì¤‘ì¹˜ ì ìš© Golden Set ê²°ê³¼\n")
            log_file.write(f"ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            log_file.write(f"ì²˜ë¦¬ëœ í‹°ì¼“ ìˆ˜: {len(df)}\n")
            log_file.write(f"ë°ì´í„° ì†ŒìŠ¤: file_chunks (1517ê°œ ë¬¸ì„œ)\n")
            log_file.write("\nğŸ¯ ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ:\n")
            log_file.write("- title: 1.5ë°°, summary: 1.3ë°°, description: 1.2ë°°\n")
            log_file.write("- body: 1.0ë°°, comment: 0.8ë°°, attachment: 0.6ë°°\n")
            log_file.write("="*80 + "\n")

            # ê° í‹°ì¼“ ì²˜ë¦¬
            for index, row in df.iterrows():
                test_case_num = index + 1
                ticket = row.to_dict()

                print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ #{test_case_num} ì²˜ë¦¬ ì¤‘...")
                print(f"   í‹°ì¼“ ID: {ticket.get('Key', 'Unknown')}")

                try:
                    # ì§ˆë¬¸ ìƒì„±
                    question = generate_question_for_ticket(ticket, llm_client)
                    print(f"   ìƒì„±ëœ ì§ˆë¬¸: {question}")

                    # file_chunks ê°€ì¤‘ì¹˜ ê²€ìƒ‰ ì‹¤í–‰
                    rag_results = search_rag_with_file_chunks(question)

                    # ê²°ê³¼ ë¡œê·¸ì— ê¸°ë¡
                    log_test_case_with_weighting(log_file, test_case_num, ticket, question, rag_results)

                    print(f"   âœ… í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ #{test_case_num} ì™„ë£Œ")

                except Exception as e:
                    print(f"   âŒ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ #{test_case_num} ì‹¤íŒ¨: {str(e)}")
                    log_file.write(f"\n--- [Test Case #{test_case_num}] ---\n")
                    log_file.write(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}\n")
                    log_file.write("="*80 + "\n")
                    continue

        print(f"\nğŸ‰ file_chunks ê°€ì¤‘ì¹˜ ì ìš© Golden Set ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ“„ ê²°ê³¼ íŒŒì¼: {output_file}")
        print(f"ğŸ“Š ì²˜ë¦¬ëœ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {len(df)}ê°œ")

    except Exception as e:
        print(f"âŒ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}")
        return 1

    return 0

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code)