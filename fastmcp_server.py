#!/usr/bin/env python3
"""
FastMCP ì„œë²„ - ì´ë©”ì¼ ì—°ë™ ì œê±°ë¨ (ë³´ì•ˆ ì •ì±…)
Jira ì—°ë™ ë° ê¸°íƒ€ ê¸°ëŠ¥ë§Œ ì œê³µ
"""

import os
import logging
from datetime import datetime
from dotenv import load_dotenv
from fastmcp import FastMCP

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# FastMCP ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
mcp = FastMCP("OpsAgentServer")

# ============================================================
# ì´ë©”ì¼ ê´€ë ¨ ê¸°ëŠ¥ì€ ë³´ì•ˆ ì •ì±…ìœ¼ë¡œ ì œê±°ë˜ì—ˆìŠµë‹ˆë‹¤
# ============================================================

@mcp.tool()
def get_server_status() -> dict:
    """
    ì„œë²„ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
    
    Returns:
        dict: ì„œë²„ ìƒíƒœ ì •ë³´
    """
    try:
        return {
            'status': 'healthy',
            'message': 'FastMCP ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤',
            'email_integration': 'disabled (ë³´ì•ˆ ì •ì±…)',
            'jira_integration': 'available',
            'timestamp': datetime.now().isoformat()
        }
    except Exception as e:
        logging.error(f"ì„œë²„ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {str(e)}")
        return {
            'status': 'error',
            'error': str(e),
            'timestamp': datetime.now().isoformat()
        }

@mcp.tool()
def simple_llm_call(prompt: str) -> str:
    """
    Azure OpenAI LLMì„ í˜¸ì¶œí•©ë‹ˆë‹¤.
    
    Args:
        prompt: LLMì— ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸
        
    Returns:
        str: LLMì˜ ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    try:
        from openai import AzureOpenAI
        
        azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')
        azure_api_key = os.getenv('AZURE_OPENAI_API_KEY')
        azure_deployment = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME', 'gpt-4.1')
        azure_api_version = os.getenv('AZURE_OPENAI_API_VERSION', '2024-10-21')
        
        if not all([azure_endpoint, azure_api_key]):
            return "ì˜¤ë¥˜: Azure OpenAI í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        
        client = AzureOpenAI(
            azure_endpoint=azure_endpoint,
            api_key=azure_api_key,
            api_version=azure_api_version
        )
        
        response = client.chat.completions.create(
            model=azure_deployment,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=1000,
            temperature=0.7
        )
        
        return response.choices[0].message.content.strip()
        
    except Exception as e:
        logging.error(f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return f"ì˜¤ë¥˜: {str(e)}"

def run_fastmcp_server():
    """FastMCP ì„œë²„ ì‹¤í–‰"""
    logging.info("ğŸš€ FastMCP ì„œë²„ ì‹œì‘")
    logging.info("ğŸ“§ ì´ë©”ì¼ ì—°ë™ ê¸°ëŠ¥: ì œê±°ë¨ (ë³´ì•ˆ ì •ì±…)")
    logging.info("ğŸ”§ ë“±ë¡ëœ ë„êµ¬ë“¤:")
    logging.info("  - get_server_status")
    logging.info("  - simple_llm_call")
    
    # FastMCP ì„œë²„ ì‹¤í–‰
    mcp.run(
        transport="streamable-http",
        host="127.0.0.1",
        port=8001,
        log_level="info"
    )

if __name__ == "__main__":
    run_fastmcp_server()
