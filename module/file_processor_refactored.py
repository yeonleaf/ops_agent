"""
ë¦¬íŒ©í† ë§ëœ íŒŒì¼ ì²˜ë¦¬ ì‹œìŠ¤í…œ
ì „ëµ íŒ¨í„´, ì˜ì¡´ì„± ì£¼ì…, í‘œì¤€í™”ëœ ì—ëŸ¬ ì²˜ë¦¬, ê°œì„ ëœ ì„ì‹œ íŒŒì¼ ê´€ë¦¬ ì ìš©
"""

import os
import json
import tempfile
import logging
from pathlib import Path
from typing import List, Dict, Any, Optional
from datetime import datetime
from dotenv import load_dotenv

from module.exceptions import (
    ProcessingError, 
    FileTypeNotSupportedError, 
    ContentExtractionError
)
from module.processors import TextBasedProcessor, LayoutBasedProcessor
from module.converters import ConverterFactory

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)


class FileTypeDetector:
    """íŒŒì¼ íƒ€ì… íŒë³„ê¸°"""
    
    @staticmethod
    def detect_file_type(file_path: str) -> str:
        """íŒŒì¼ í™•ì¥ìë¡œë¶€í„° ë¬¸ì„œ íƒ€ì… íŒë³„"""
        ext = Path(file_path).suffix.lower()
        if ext in ['.docx', '.pptx', '.pdf', '.xlsx', '.txt', '.md', '.csv', '.xml']:
            return ext[1:]  # .docx -> docx
        else:
            raise FileTypeNotSupportedError(file_path, ext)
    
    @staticmethod
    def detect_content_type(file_path: str, doc_type: str) -> str:
        """ì½˜í…ì¸ ê°€ text-basedì¸ì§€ layout-basedì¸ì§€ íŒë³„"""
        try:
            if doc_type == 'pdf':
                return FileTypeDetector._analyze_pdf_content(file_path)
            elif doc_type == 'docx':
                return FileTypeDetector._analyze_docx_content(file_path)
            elif doc_type == 'pptx':
                return FileTypeDetector._analyze_pptx_content(file_path)
            elif doc_type == 'xlsx':
                return FileTypeDetector._analyze_xlsx_content(file_path)
            elif doc_type == 'xml':
                return 'text_based'  # XMLì€ í•­ìƒ text-basedë¡œ ì²˜ë¦¬
            else:
                return 'text_based'
        except Exception as e:
            logger.warning(f"ì½˜í…ì¸  íƒ€ì… íŒë³„ ì˜¤ë¥˜: {e}")
            return 'layout_based'  # ì˜¤ë¥˜ ì‹œ ì•ˆì „í•˜ê²Œ layout-basedë¡œ ë¶„ë¥˜
    
    @staticmethod
    def _analyze_pdf_content(file_path: str) -> str:
        """PDF ì½˜í…ì¸  ë¶„ì„"""
        try:
            import fitz
            doc = fitz.open(file_path)
            text_content = ""
            image_count = 0
            
            for page in doc:
                text_content += page.get_text()
                image_list = page.get_images()
                image_count += len(image_list)
            
            doc.close()
            
            # í…ìŠ¤íŠ¸ê°€ ì¶©ë¶„í•˜ê³  ì´ë¯¸ì§€ê°€ ì ìœ¼ë©´ text-based
            if len(text_content.strip()) > 100 and image_count < 3:
                return 'text_based'
            else:
                return 'layout_based'
                
        except Exception:
            return 'layout_based'
    
    @staticmethod
    def _analyze_docx_content(file_path: str) -> str:
        """DOCX ì½˜í…ì¸  ë¶„ì„"""
        try:
            from docx import Document
            doc = Document(file_path)
            text_content = ""
            image_count = 0
            
            for para in doc.paragraphs:
                text_content += para.text + "\n"
            
            # ì´ë¯¸ì§€ ê°œìˆ˜ í™•ì¸ (ê°„ë‹¨í•œ ë°©ë²•)
            for rel in doc.part.rels.values():
                if "image" in rel.target_ref:
                    image_count += 1
            
            if len(text_content.strip()) > 100 and image_count < 3:
                return 'text_based'
            else:
                return 'layout_based'
                
        except Exception:
            return 'layout_based'
    
    @staticmethod
    def _analyze_pptx_content(file_path: str) -> str:
        """
        PPTX ì½˜í…ì¸  ë¶„ì„ (íŒŒì¼ ë‹¨ìœ„ - ìŠ¬ë¼ì´ë“œë³„ ë™ì  ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´ë¨)
        
        ì°¸ê³ : ì´ í•¨ìˆ˜ëŠ” ë” ì´ìƒ ì‚¬ìš©ë˜ì§€ ì•Šìœ¼ë©°, 
        TextBasedProcessor._analyze_slide_content()ì—ì„œ ìŠ¬ë¼ì´ë“œë³„ë¡œ ë™ì  ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        # PPTXëŠ” ìŠ¬ë¼ì´ë“œë³„ë¡œ ë™ì  ë¶„ì„í•˜ë¯€ë¡œ ê¸°ë³¸ê°’ ë°˜í™˜
        # ì‹¤ì œ ë¶„ì„ì€ TextBasedProcessor._analyze_slide_content()ì—ì„œ ìˆ˜í–‰
        return 'mixed_content'  # í˜¼í•© ì½˜í…ì¸ ë¡œ ë¶„ë¥˜
    
    @staticmethod
    def _analyze_xlsx_content(file_path: str) -> str:
        """XLSX ì½˜í…ì¸  ë¶„ì„"""
        try:
            from openpyxl import load_workbook
            wb = load_workbook(file_path, data_only=True)
            text_content = ""
            
            for sheet_name in wb.sheetnames:
                ws = wb[sheet_name]
                for row in ws.iter_rows(values_only=True):
                    for cell in row:
                        if cell:
                            text_content += str(cell) + " "
            
            wb.close()
            
            # ì—‘ì…€ì€ ëŒ€ë¶€ë¶„ text-based
            if len(text_content.strip()) > 50:
                return 'text_based'
            else:
                return 'layout_based'
                
        except Exception:
            return 'layout_based'


class FileProcessor:
    """ë¦¬íŒ©í† ë§ëœ ë©”ì¸ íŒŒì¼ ì²˜ë¦¬ê¸°"""
    
    def __init__(self, azure_processor=None, converter=None):
        """
        íŒŒì¼ ì²˜ë¦¬ê¸° ì´ˆê¸°í™”
        
        Args:
            azure_processor: Azure OpenAI í”„ë¡œì„¸ì„œ (ì„ íƒì‚¬í•­)
            converter: íŒŒì¼ ë³€í™˜ê¸° (ì„ íƒì‚¬í•­)
        """
        self.azure_processor = azure_processor
        self.converter = converter
        
        # í”„ë¡œì„¸ì„œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        self.text_processor = TextBasedProcessor(azure_processor)
        self.layout_processor = LayoutBasedProcessor(azure_processor, converter)
        
        # ì²˜ë¦¬ í†µê³„
        self.processing_stats = {
            "total_files": 0,
            "successful_files": 0,
            "failed_files": 0,
            "total_chunks": 0
        }
    
    def process_file(self, file_path: str) -> Dict[str, Any]:
        """
        íŒŒì¼ì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜
        
        Args:
            file_path: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        try:
            logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì‹œì‘: {file_path}")
            
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(file_path):
                raise ProcessingError(f"íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}", file_path)
            
            # 1. íŒŒì¼ íƒ€ì… íŒë³„
            doc_type = FileTypeDetector.detect_file_type(file_path)
            content_type = FileTypeDetector.detect_content_type(file_path, doc_type)
            
            logger.info(f"íŒŒì¼ íƒ€ì…: {doc_type}, ì½˜í…ì¸  íƒ€ì…: {content_type}")
            
            # 2. íŒŒì¼ íƒ€ì…ë³„ ì§ì ‘ ì²˜ë¦¬ (ì´ì¤‘ í•˜ì´ë¸Œë¦¬ë“œ vs ë‹¨ìˆœ ë³€í™˜)
            if doc_type in ['pptx', 'pdf', 'docx']:
                # ì´ì¤‘ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹: ìš”ì†Œ ë‹¨ìœ„ + Vision ë¶„ì„
                processing_method = "dual_path_hybrid"
                chunks = self.text_processor.process(file_path)
            elif doc_type in ['xlsx', 'txt', 'md', 'csv']:
                # ë‹¨ìˆœ ë³€í™˜ ë°©ì‹: ìš”ì†Œ ë‹¨ìœ„ ë¶„ì„ë§Œ
                processing_method = "simple_conversion"
                chunks = self.text_processor.process(file_path)
            else:
                # ê¸°ì¡´ ë°©ì‹ (fallback)
                if content_type == 'text_based':
                    processor = self.text_processor
                    processing_method = "text_based"
                else:
                    processor = self.layout_processor
                    processing_method = "layout_based"
                chunks = processor.process(file_path)
            
            # 4. ê²°ê³¼ êµ¬ì„±
            result = {
                "file_info": {
                    "file_path": file_path,
                    "file_name": Path(file_path).name,
                    "file_type": doc_type,
                    "content_type": content_type,
                    "processing_method": processing_method,
                    "total_chunks": len(chunks),
                    "processing_timestamp": datetime.now().isoformat()
                },
                "chunks": chunks,
                "processing_stats": {
                    "chunks_by_type": self._count_chunks_by_type(chunks),
                    "total_elements": len(chunks)
                }
            }
            
            # 5. ì²˜ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸
            self._update_processing_stats(True, len(chunks))
            
            logger.info(f"íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ: {file_path} -> {len(chunks)}ê°œ ì²­í¬")
            return result
            
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            self._update_processing_stats(False, 0)
            
            # ì˜ˆì™¸ë¥¼ í‘œì¤€í™”ëœ ì—ëŸ¬ ë©”ì‹œì§€ë¡œ ë³€í™˜
            if isinstance(e, ProcessingError):
                return e.to_dict()
            else:
                error = ProcessingError(str(e), file_path)
                return error.to_dict()
    
    def process_files_batch(self, file_paths: List[str]) -> List[Dict[str, Any]]:
        """
        ì—¬ëŸ¬ íŒŒì¼ì„ ì¼ê´„ ì²˜ë¦¬
        
        Args:
            file_paths: ì²˜ë¦¬í•  íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        results = []
        
        for file_path in file_paths:
            try:
                result = self.process_file(file_path)
                results.append(result)
            except Exception as e:
                logger.error(f"ì¼ê´„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ({file_path}): {e}")
                error_result = ProcessingError(str(e), file_path).to_dict()
                results.append(error_result)
        
        return results
    
    def _count_chunks_by_type(self, chunks: List[Dict[str, Any]]) -> Dict[str, int]:
        """ì²­í¬ íƒ€ì…ë³„ ê°œìˆ˜ë¥¼ ê³„ì‚°"""
        chunk_types = {}
        for chunk in chunks:
            element_type = chunk.get("metadata", {}).get("element_type", "unknown")
            chunk_types[element_type] = chunk_types.get(element_type, 0) + 1
        return chunk_types
    
    def _update_processing_stats(self, success: bool, chunk_count: int):
        """ì²˜ë¦¬ í†µê³„ ì—…ë°ì´íŠ¸"""
        self.processing_stats["total_files"] += 1
        if success:
            self.processing_stats["successful_files"] += 1
            self.processing_stats["total_chunks"] += chunk_count
        else:
            self.processing_stats["failed_files"] += 1
    
    def get_processing_stats(self) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ë°˜í™˜"""
        return self.processing_stats.copy()
    
    def reset_stats(self):
        """ì²˜ë¦¬ í†µê³„ ì´ˆê¸°í™”"""
        self.processing_stats = {
            "total_files": 0,
            "successful_files": 0,
            "failed_files": 0,
            "total_chunks": 0
        }
    
    def save_result_to_file(self, result: Dict[str, Any], output_path: str, format: str = "json"):
        """
        ì²˜ë¦¬ ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            result: ì €ì¥í•  ê²°ê³¼
            output_path: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
            format: ì¶œë ¥ í˜•ì‹ ("json" ë˜ëŠ” "md")
        """
        try:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            if format.lower() == "json":
                with open(output_path, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                logger.info(f"ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥: {output_path}")
                
            elif format.lower() == "md":
                markdown_content = self._result_to_markdown(result)
                with open(output_path, 'w', encoding='utf-8') as f:
                    f.write(markdown_content)
                logger.info(f"ê²°ê³¼ë¥¼ Markdown íŒŒì¼ë¡œ ì €ì¥: {output_path}")
                
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¶œë ¥ í˜•ì‹: {format}")
                
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì˜¤ë¥˜: {e}")
            raise ProcessingError(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
    
    def _result_to_markdown(self, result: Dict[str, Any]) -> str:
        """ê²°ê³¼ë¥¼ Markdown í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if "error" in result:
            return f"# ì˜¤ë¥˜ ë°œìƒ\n\n```\n{result.get('message', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}\n```"
        
        md_content = []
        
        # íŒŒì¼ ì •ë³´
        file_info = result.get("file_info", {})
        md_content.append(f"# {file_info.get('file_name', 'Unknown File')} ì²˜ë¦¬ ê²°ê³¼")
        md_content.append("")
        md_content.append("## ğŸ“‹ íŒŒì¼ ì •ë³´")
        md_content.append(f"- **íŒŒì¼ëª…**: {file_info.get('file_name', 'N/A')}")
        md_content.append(f"- **íŒŒì¼ íƒ€ì…**: {file_info.get('file_type', 'N/A')}")
        md_content.append(f"- **ì½˜í…ì¸  íƒ€ì…**: {file_info.get('content_type', 'N/A')}")
        md_content.append(f"- **ì²˜ë¦¬ ë°©ë²•**: {file_info.get('processing_method', 'N/A')}")
        md_content.append(f"- **ì´ ì²­í¬ ìˆ˜**: {file_info.get('total_chunks', 0)}")
        md_content.append(f"- **ì²˜ë¦¬ ì‹œê°„**: {file_info.get('processing_timestamp', 'N/A')}")
        md_content.append("")
        
        # ì²˜ë¦¬ í†µê³„
        processing_stats = result.get("processing_stats", {})
        md_content.append("## ğŸ“Š ì²˜ë¦¬ í†µê³„")
        md_content.append(f"- **ì´ ìš”ì†Œ ìˆ˜**: {processing_stats.get('total_elements', 0)}")
        
        chunks_by_type = processing_stats.get("chunks_by_type", {})
        for chunk_type, count in chunks_by_type.items():
            md_content.append(f"- **{chunk_type}**: {count}")
        md_content.append("")
        
        # ì²­í¬ë³„ ë‚´ìš©
        chunks = result.get("chunks", [])
        for i, chunk in enumerate(chunks, 1):
            metadata = chunk.get("metadata", {})
            text_content = chunk.get("text_chunk_to_embed", "")
            
            md_content.append(f"## ğŸ“„ ì²­í¬ {i}")
            md_content.append("")
            md_content.append(f"**íƒ€ì…**: {metadata.get('element_type', 'unknown')}")
            md_content.append(f"**ì„¹ì…˜**: {metadata.get('section_title', 'N/A')}")
            md_content.append(f"**í˜ì´ì§€**: {metadata.get('page_number', 'N/A')}")
            md_content.append("")
            md_content.append("### ë‚´ìš©")
            md_content.append("```")
            md_content.append(text_content)
            md_content.append("```")
            md_content.append("")
            md_content.append("---")
            md_content.append("")
        
        return "\n".join(md_content) 