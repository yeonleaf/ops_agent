#!/usr/bin/env python3
"""
RAG ë°ì´í„° ê´€ë¦¬ì ëª¨ë“ˆ
Streamlit ì•±ì—ì„œ ë¬¸ì„œ ì—…ë¡œë“œ, ì²˜ë¦¬, ë²¡í„° DB ì €ì¥ ê¸°ëŠ¥ì„ ì œê³µ
"""

# import streamlit as st  # Streamlit ì»¨í…ìŠ¤íŠ¸ê°€ í•„ìš”í•  ë•Œë§Œ import
import os
import tempfile
import hashlib
from typing import List, Dict, Any, Optional
from datetime import datetime
import uuid

# FileProcessor import
from module.file_processor import FileProcessor, DocumentType, FileTypeDetector

# Vector DB import
from vector_db_models import VectorDBManager, FileChunk, StructuredChunk

# LangChain imports for embedding
from langchain_openai import AzureOpenAIEmbeddings
from dotenv import load_dotenv

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

def create_embedding_client():
    """Embedding í´ë¼ì´ì–¸íŠ¸ ìƒì„± (Azure OpenAI ìš°ì„  ì‚¬ìš©)"""
    try:
        # 1. Azure OpenAI í‚¤ í™•ì¸
        azure_api_key = os.getenv("AZURE_OPENAI_API_KEY")
        azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
        azure_deployment = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME", "text-embedding-ada-002")
        azure_api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-10-21")
        
        if azure_api_key and azure_endpoint:
            print("ğŸ”§ Azure OpenAI Embedding í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹œë„...")
            try:
                embedding_client = AzureOpenAIEmbeddings(
                    azure_deployment=azure_deployment,
                    azure_endpoint=azure_endpoint,
                    api_key=azure_api_key,
                    api_version=azure_api_version,
                    openai_api_type="azure"
                )
                # ì‹¤ì œ ì„ë² ë”© ìš”ì²­ìœ¼ë¡œ ë°°í¬ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                test_embedding = embedding_client.embed_query("test")
                print("âœ… Azure OpenAI Embedding í´ë¼ì´ì–¸íŠ¸ ìƒì„± ë° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                return embedding_client
            except Exception as e:
                print(f"âŒ Azure OpenAI Embedding ë°°í¬ ì˜¤ë¥˜: {e}")
                if "DeploymentNotFound" in str(e) or "404" in str(e):
                    print("âš ï¸ ì„ë² ë”© ë°°í¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”ë¯¸ ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    return DummyEmbeddingClient()
                else:
                    print("ğŸ”„ ë‹¤ë¥¸ ì˜¤ë¥˜ë¡œ ì¸í•´ ë”ë¯¸ ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                    return DummyEmbeddingClient()
        
        # 2. í‘œì¤€ OpenAI API í‚¤ í™•ì¸ (fallback)
        openai_key = os.getenv("OPENAI_KEY") or os.getenv("OPENAI_API_KEY")
        if openai_key:
            print("ğŸ”§ í‘œì¤€ OpenAI Embedding í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹œë„...")
            embedding_client = AzureOpenAIEmbeddings(
                openai_api_key=openai_key,
                model="text-embedding-ada-002"
            )
            print("âœ… í‘œì¤€ OpenAI Embedding í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì„±ê³µ")
            return embedding_client
        
        # 3. ëª¨ë“  í‚¤ê°€ ì—†ìœ¼ë©´ ë”ë¯¸ ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        print("âš ï¸ OpenAI í‚¤ê°€ ì—†ì–´ì„œ ë”ë¯¸ ì„ë² ë”©ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return DummyEmbeddingClient()
        
    except Exception as e:
        print(f"âŒ Embedding í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        print("ğŸ”„ ë”ë¯¸ ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ë¡œ í´ë°±í•©ë‹ˆë‹¤.")
        return DummyEmbeddingClient()

class DummyEmbeddingClient:
    """ë”ë¯¸ ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ (í…ŒìŠ¤íŠ¸ìš©)"""
    
    def embed_query(self, text: str):
        """ë”ë¯¸ ì„ë² ë”© ìƒì„± (384ì°¨ì› - ChromaDB ê¸°ë³¸ ì„ë² ë”©ê³¼ ë™ì¼)"""
        import random
        # ChromaDB ê¸°ë³¸ ì„ë² ë”©ê³¼ ë™ì¼í•œ 384ì°¨ì›
        return [random.random() for _ in range(384)]
    
    def embed_documents(self, texts: list):
        """ë”ë¯¸ ë¬¸ì„œ ì„ë² ë”© ìƒì„±"""
        return [self.embed_query(text) for text in texts]

def embed_and_store_chunks(file_processing_result: Dict[str, Any], file_name: str) -> int:
    """
    íŒŒì¼ ì²˜ë¦¬ ê²°ê³¼ë¥¼ ì„ë² ë”©í•˜ê³  ë²¡í„° DBì— ì €ì¥
    
    Args:
        file_processing_result: FileProcessor.process_file()ì—ì„œ ë°˜í™˜ëœ ê²°ê³¼
        file_name: ì›ë³¸ íŒŒì¼ëª…
    
    Returns:
        ì €ì¥ëœ ì²­í¬ ê°œìˆ˜
    """
    try:
        # ChromaDB ê¸°ë³¸ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ë³„ë„ ì„ë² ë”© í´ë¼ì´ì–¸íŠ¸ ë¶ˆí•„ìš”
        vector_db = VectorDBManager()
        
        # íŒŒì¼ í•´ì‹œ ìƒì„± (ì¤‘ë³µ ë°©ì§€ìš©)
        file_hash = hashlib.md5(file_name.encode()).hexdigest()
        
        stored_count = 0
        
        # FileProcessor.process_fileì˜ ê²°ê³¼ êµ¬ì¡° í™•ì¸
        if not isinstance(file_processing_result, dict):
            print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ íƒ€ì…: {type(file_processing_result)}")
            return 0
        
        print(f"ğŸ“Š íŒŒì¼ ì²˜ë¦¬ ê²°ê³¼ í‚¤ë“¤: {list(file_processing_result.keys())}")
        
        # processed_pages ë°°ì—´ì—ì„œ ì²­í¬ ì¶”ì¶œ
        processed_pages = file_processing_result.get('processed_pages', [])
        print(f"ğŸ“„ ì²˜ë¦¬ëœ í˜ì´ì§€ ìˆ˜: {len(processed_pages)}")
        
        for page_idx, page_data in enumerate(processed_pages):
            try:
                print(f"ğŸ” í˜ì´ì§€ {page_idx+1} ì²˜ë¦¬ ì¤‘...")
                print(f"ğŸ“Š í˜ì´ì§€ ë°ì´í„° êµ¬ì¡°: {type(page_data)}")
                
                if isinstance(page_data, dict):
                    print(f"ğŸ“Š í˜ì´ì§€ ë°ì´í„° í‚¤ë“¤: {list(page_data.keys())}")
                    
                    # elements ë°°ì—´ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    elements = page_data.get('elements', [])
                    print(f"ğŸ” elements ê°œìˆ˜: {len(elements)}")
                    
                    for element_idx, element in enumerate(elements):
                        try:
                            print(f"ğŸ” element {element_idx}: {type(element)}")
                            
                            if isinstance(element, dict):
                                # í…ìŠ¤íŠ¸ ìš”ì†Œ ì²˜ë¦¬
                                if element.get('element_type') == 'text':
                                    content = element.get('content', '')
                                    if content and len(content.strip()) > 10:
                                        print(f"âœ… í…ìŠ¤íŠ¸ ìš”ì†Œì—ì„œ ë‚´ìš© ì¶”ì¶œ: {len(content)}ì")
                                        
                                        # ì²­í¬ ID ìƒì„±
                                        chunk_id = str(uuid.uuid4())
                                        
                                        # FileChunk ê°ì²´ ìƒì„±
                                        file_chunk = FileChunk(
                                            chunk_id=chunk_id,
                                            file_name=file_name,
                                            file_hash=file_hash,
                                            text_chunk=content,
                                            architecture="dual_path_hybrid",
                                            processing_method="file_processor",
                                            vision_analysis=False,
                                            section_title=page_data.get('section_title', ''),
                                            page_number=page_data.get('page_number', page_idx + 1),
                                            element_count=1,
                                            file_type=file_processing_result.get('file_type', 'unknown'),
                                            elements=[element],
                                            created_at=datetime.now().isoformat(),
                                            file_size=len(content),
                                            processing_duration=0.0
                                        )
                                        
                                        # ë²¡í„° DBì— ì €ì¥
                                        vector_db.add_file_chunk(file_chunk)
                                        stored_count += 1
                                        print(f"âœ… í…ìŠ¤íŠ¸ ì²­í¬ {stored_count} ì €ì¥ ì™„ë£Œ")
                                
                                # í…Œì´ë¸” ìš”ì†Œ ì²˜ë¦¬
                                elif element.get('element_type') == 'table':
                                    table_data = element.get('content', [])
                                    if isinstance(table_data, list) and table_data:
                                        # í…Œì´ë¸”ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
                                        table_text = ""
                                        for row in table_data:
                                            if isinstance(row, list):
                                                row_text = " | ".join(str(cell) for cell in row if cell)
                                                if row_text:
                                                    table_text += row_text + "\n"
                                        
                                        if table_text and len(table_text.strip()) > 10:
                                            print(f"âœ… í…Œì´ë¸” ìš”ì†Œì—ì„œ ë‚´ìš© ì¶”ì¶œ: {len(table_text)}ì")
                                            
                                            # ì²­í¬ ID ìƒì„±
                                            chunk_id = str(uuid.uuid4())
                                            
                                            # FileChunk ê°ì²´ ìƒì„±
                                            file_chunk = FileChunk(
                                                chunk_id=chunk_id,
                                                file_name=file_name,
                                                file_hash=file_hash,
                                                text_chunk=table_text,
                                                architecture="dual_path_hybrid",
                                                processing_method="file_processor",
                                                vision_analysis=False,
                                                section_title=page_data.get('section_title', ''),
                                                page_number=page_data.get('page_number', page_idx + 1),
                                                element_count=1,
                                                file_type=file_processing_result.get('file_type', 'unknown'),
                                                elements=[element],
                                                created_at=datetime.now().isoformat(),
                                                file_size=len(table_text),
                                                processing_duration=0.0
                                            )
                                            
                                            # ë²¡í„° DBì— ì €ì¥
                                            vector_db.add_file_chunk(file_chunk)
                                            stored_count += 1
                                            print(f"âœ… í…Œì´ë¸” ì²­í¬ {stored_count} ì €ì¥ ì™„ë£Œ")
                            
                            else:
                                # elementê°€ dictê°€ ì•„ë‹Œ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
                                element_text = str(element)
                                if len(element_text.strip()) > 10:
                                    print(f"âœ… element {element_idx}ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜: {len(element_text)}ì")
                                    
                                    # ì²­í¬ ID ìƒì„±
                                    chunk_id = str(uuid.uuid4())
                                    
                                    # FileChunk ê°ì²´ ìƒì„±
                                    file_chunk = FileChunk(
                                        chunk_id=chunk_id,
                                        file_name=file_name,
                                        file_hash=file_hash,
                                        text_chunk=element_text,
                                        architecture="dual_path_hybrid",
                                        processing_method="file_processor",
                                        vision_analysis=False,
                                        section_title=page_data.get('section_title', ''),
                                        page_number=page_data.get('page_number', page_idx + 1),
                                        element_count=1,
                                        file_type=file_processing_result.get('file_type', 'unknown'),
                                        elements=[element],
                                        created_at=datetime.now().isoformat(),
                                        file_size=len(element_text),
                                        processing_duration=0.0
                                    )
                                    
                                    # ë²¡í„° DBì— ì €ì¥
                                    vector_db.add_file_chunk(file_chunk)
                                    stored_count += 1
                                    print(f"âœ… ë¬¸ìì—´ ì²­í¬ {stored_count} ì €ì¥ ì™„ë£Œ")
                        
                        except Exception as e:
                            print(f"âŒ element {element_idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                            continue
                
                else:
                    # page_dataê°€ dictê°€ ì•„ë‹Œ ê²½ìš° ë¬¸ìì—´ë¡œ ì²˜ë¦¬
                    page_text = str(page_data)
                    if len(page_text.strip()) > 10:
                        print(f"âœ… í˜ì´ì§€ {page_idx+1}ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜: {len(page_text)}ì")
                        
                        # ì²­í¬ ID ìƒì„±
                        chunk_id = str(uuid.uuid4())
                        
                        # FileChunk ê°ì²´ ìƒì„±
                        file_chunk = FileChunk(
                            chunk_id=chunk_id,
                            file_name=file_name,
                            file_hash=file_hash,
                            text_chunk=page_text,
                            architecture="dual_path_hybrid",
                            processing_method="file_processor",
                            vision_analysis=False,
                            section_title="",
                            page_number=page_idx + 1,
                            element_count=1,
                            file_type=file_processing_result.get('file_type', 'unknown'),
                            elements=[],
                            created_at=datetime.now().isoformat(),
                            file_size=len(page_text),
                            processing_duration=0.0
                        )
                        
                        # ë²¡í„° DBì— ì €ì¥
                        vector_db.add_file_chunk(file_chunk)
                        stored_count += 1
                        print(f"âœ… í˜ì´ì§€ ì²­í¬ {stored_count} ì €ì¥ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ í˜ì´ì§€ {page_idx+1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
    
        return stored_count
        
    except Exception as e:
        print(f"âŒ ì²­í¬ ì„ë² ë”© ë° ì €ì¥ ì‹¤íŒ¨: {e}")
        return 0

def get_db_statistics() -> Dict[str, int]:
    """ë²¡í„° DB í†µê³„ ì •ë³´ ì¡°íšŒ"""
    try:
        vector_db = VectorDBManager()
        
        # íŒŒì¼ ì²­í¬ ê°œìˆ˜ ì¡°íšŒ
        file_chunks_count = vector_db.get_file_chunks_count()
        
        # ë©”ì¼ ê°œìˆ˜ ì¡°íšŒ
        mails_count = vector_db.get_mails_count()
        
        return {
            "file_chunks": file_chunks_count,
            "mails": mails_count,
            "total_documents": file_chunks_count + mails_count
        }
    except Exception as e:
        print(f"âŒ DB í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"file_chunks": 0, "mails": 0, "total_documents": 0}

def clear_all_data():
    """ë²¡í„° DBì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
    try:
        vector_db = VectorDBManager()
        vector_db.clear_all_data()
        print("âœ… ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
        return True
    except Exception as e:
        print(f"âŒ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {e}")
        return False

def reset_chromadb():
    """ChromaDB ê°•ì œ ì¬ì„¤ì • (ì¶©ëŒ í•´ê²°ìš©)"""
    try:
        print("ğŸ”„ ChromaDB ì¬ì„¤ì •ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # VectorDBManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì¶©ëŒ ë°©ì§€)
        vector_db = VectorDBManager()
        
        # ê°•ì œ ì¬ì„¤ì • ì‹¤í–‰
        success = vector_db.force_reset_chromadb()
        
        if success:
            print("âœ… ChromaDB ì¬ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            return True
        else:
            print("âŒ ChromaDB ì¬ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return False
            
    except Exception as e:
        print(f"âŒ ChromaDB ì¬ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return False

def create_rag_manager_tab():
    """RAG ë°ì´í„° ê´€ë¦¬ì íƒ­ ìƒì„±"""
    import streamlit as st
    
    st.header("ğŸ“š RAG ë°ì´í„° ê´€ë¦¬ì")
    st.markdown("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì—¬ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # 2ë‹¨ ë ˆì´ì•„ì›ƒ ìƒì„±
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“ íŒŒì¼ ì²˜ë¦¬")
        
        # íŒŒì¼ ì—…ë¡œë”
        uploaded_files = st.file_uploader(
            "ë¬¸ì„œ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”",
            type=['docx', 'pptx', 'pdf', 'xlsx', 'xls', 'txt', 'md', 'xml'],
            accept_multiple_files=True,
            help="ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (DOCX, PPTX, PDF, XLSX, XLS, TXT, MD, XML ì§€ì›)"
        )
        
        # íŒŒì¼ ì²˜ë¦¬ ë²„íŠ¼
        if st.button("ğŸš€ ì—…ë¡œë“œëœ íŒŒì¼ ì²˜ë¦¬ ë° ì„ë² ë”©", disabled=not uploaded_files):
            if uploaded_files:
                process_uploaded_files(uploaded_files)
            else:
                st.warning("âš ï¸ ì—…ë¡œë“œí•  íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
    
    with col2:
        st.subheader("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ í˜„í™©")
        
        # DB í†µê³„ í‘œì‹œ
        stats = get_db_statistics()
        
        st.metric(
            label="ğŸ“„ ì´ ë¬¸ì„œ ìˆ˜",
            value=stats["total_documents"],
            help="íŒŒì¼ ì²­í¬ + ë©”ì¼ ë°ì´í„°"
        )
        
        st.metric(
            label="ğŸ§© íŒŒì¼ ì²­í¬ ìˆ˜",
            value=stats["file_chunks"],
            help="ì—…ë¡œë“œëœ íŒŒì¼ì—ì„œ ì¶”ì¶œëœ ì²­í¬"
        )
        
        st.metric(
            label="ğŸ“§ ë©”ì¼ ë°ì´í„° ìˆ˜",
            value=stats["mails"],
            help="ì´ë©”ì¼ì—ì„œ ì¶”ì¶œëœ ë°ì´í„°"
        )
        
        # ë°ì´í„° ê´€ë¦¬ ì„¹ì…˜
        with st.expander("ğŸ”§ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬", expanded=False):
            st.markdown("**ChromaDB ì¶©ëŒ í•´ê²°**")
            st.info("ChromaDB ì¸ìŠ¤í„´ìŠ¤ ì¶©ëŒì´ ë°œìƒí•œ ê²½ìš° ì•„ë˜ ë²„íŠ¼ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
            
            if st.button("ğŸ”„ ChromaDB ì¬ì„¤ì •", type="secondary"):
                with st.spinner("ChromaDBë¥¼ ì¬ì„¤ì •í•˜ëŠ” ì¤‘..."):
                    if reset_chromadb():
                        st.success("âœ… ChromaDB ì¬ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.rerun()  # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨
                    else:
                        st.error("âŒ ChromaDB ì¬ì„¤ì •ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            
            st.markdown("---")
            st.markdown("**ì „ì²´ ë°ì´í„° ì‚­ì œ**")
            st.warning("ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!")
            
            if st.button("ğŸ—‘ï¸ ëª¨ë“  ë°ì´í„° ì‚­ì œ", type="secondary"):
                if clear_all_data():
                    st.rerun()  # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨

def process_uploaded_files(uploaded_files):
    """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ ì²˜ë¦¬"""
    import streamlit as st
    
    # Azure OpenAI ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œ ìƒì„±
    from module.image_to_text import AzureOpenAIImageProcessor
    azure_processor = AzureOpenAIImageProcessor(
        api_key=os.getenv("AZURE_OPENAI_API_KEY"),
        endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
        deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
    )
    
    # FileProcessor ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    file_processor = FileProcessor(azure_processor)
    
    total_chunks = 0
    processed_files = 0
    
    for uploaded_file in uploaded_files:
        try:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name
            
            # íŒŒì¼ ì²˜ë¦¬
            with st.spinner(f"ğŸ“„ {uploaded_file.name} ì²˜ë¦¬ ì¤‘..."):
                # íŒŒì¼ íƒ€ì… ê°ì§€
                doc_type = FileTypeDetector.detect_file_type(tmp_file_path)
                
                # íŒŒì¼ ì²˜ë¦¬
                result = file_processor.process_file(tmp_file_path)
                
                if result and result.get('processed_pages'):
                    # ì²­í¬ ì„ë² ë”© ë° ì €ì¥
                    chunks_stored = embed_and_store_chunks(
                        result, 
                        uploaded_file.name
                    )
                    
                    if chunks_stored > 0:
                        st.success(f"âœ… {uploaded_file.name} ì²˜ë¦¬ ì™„ë£Œ! {chunks_stored}ê°œì˜ ì²­í¬ê°€ DBì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        total_chunks += chunks_stored
                        processed_files += 1
                    else:
                        st.warning(f"âš ï¸ {uploaded_file.name} ì²˜ë¦¬ ì™„ë£Œë˜ì—ˆì§€ë§Œ ì €ì¥ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.error(f"âŒ {uploaded_file.name} ì²˜ë¦¬ ì‹¤íŒ¨")
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_file_path)
            
        except Exception as e:
            st.error(f"âŒ {uploaded_file.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    if processed_files > 0:
        st.success(f"ğŸ‰ ì´ {processed_files}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ! {total_chunks}ê°œì˜ ì²­í¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.rerun()  # DB í†µê³„ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨

def process_uploaded_files_with_structured_chunking(uploaded_files, use_structured_chunking: bool = True):
    """
    ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ êµ¬ì¡°ì  ì²­í‚¹ìœ¼ë¡œ ì²˜ë¦¬
    
    Args:
        uploaded_files: Streamlit ì—…ë¡œë“œëœ íŒŒì¼ ë¦¬ìŠ¤íŠ¸
        use_structured_chunking: êµ¬ì¡°ì  ì²­í‚¹ ì‚¬ìš© ì—¬ë¶€
    """
    if not uploaded_files:
        st.warning("âš ï¸ ì—…ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # Vector DB ì´ˆê¸°í™”
    vector_db = VectorDBManager()
    
    # FileProcessor ì´ˆê¸°í™”
    azure_processor = None  # êµ¬ì¡°ì  ì²­í‚¹ì—ì„œëŠ” Vision ì²˜ë¦¬ ë¶ˆí•„ìš”
    file_processor = FileProcessor(azure_processor)
    
    total_chunks = 0
    processed_files = 0
    
    for uploaded_file in uploaded_files:
        try:
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(delete=False, suffix=f".{uploaded_file.name.split('.')[-1]}") as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name
            
            # íŒŒì¼ ì²˜ë¦¬
            with st.spinner(f"ğŸ“„ {uploaded_file.name} êµ¬ì¡°ì  ì²­í‚¹ ì²˜ë¦¬ ì¤‘..."):
                if use_structured_chunking:
                    # êµ¬ì¡°ì  ì²­í‚¹ ì²˜ë¦¬
                    structured_chunks = file_processor.process_with_structured_chunking(tmp_file_path)
                    
                    if structured_chunks:
                        # êµ¬ì¡°ì  ì²­í¬ë“¤ì„ Vector DBì— ì €ì¥
                        chunks_stored = 0
                        for structured_chunk in structured_chunks:
                            # StructuredChunk ê°ì²´ ìƒì„±
                            chunk_id = str(uuid.uuid4())
                            file_hash = hashlib.md5(uploaded_file.getvalue()).hexdigest()
                            
                            structured_chunk_obj = StructuredChunk(
                                chunk_id=chunk_id,
                                content=structured_chunk.content,
                                chunk_type=structured_chunk.chunk_type,
                                ticket_id=structured_chunk.ticket_id,
                                field_name=structured_chunk.field_name,
                                field_value=structured_chunk.field_value,
                                priority=structured_chunk.priority,
                                file_name=uploaded_file.name,
                                file_type=Path(tmp_file_path).suffix.lower(),
                                metadata=structured_chunk.metadata,
                                created_at=datetime.now().isoformat(),
                                commenter=structured_chunk.commenter
                            )
                            
                            # Vector DBì— ì €ì¥
                            if vector_db.add_structured_chunk(structured_chunk_obj):
                                chunks_stored += 1
                        
                        if chunks_stored > 0:
                            st.success(f"âœ… {uploaded_file.name} êµ¬ì¡°ì  ì²­í‚¹ ì™„ë£Œ! {chunks_stored}ê°œì˜ êµ¬ì¡°ì  ì²­í¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            total_chunks += chunks_stored
                            processed_files += 1
                        else:
                            st.warning(f"âš ï¸ {uploaded_file.name} êµ¬ì¡°ì  ì²­í‚¹ ì™„ë£Œë˜ì—ˆì§€ë§Œ ì €ì¥ëœ ì²­í¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
                    else:
                        st.warning(f"âš ï¸ {uploaded_file.name} êµ¬ì¡°ì  ì²­í‚¹ì„ ì ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¼ë°˜ ì²˜ë¦¬ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                        # ì¼ë°˜ ì²˜ë¦¬ë¡œ í´ë°±
                        result = file_processor.process_file(tmp_file_path)
                        if result and result.get('processed_pages'):
                            chunks_stored = embed_and_store_chunks(result, uploaded_file.name)
                            if chunks_stored > 0:
                                st.success(f"âœ… {uploaded_file.name} ì¼ë°˜ ì²˜ë¦¬ ì™„ë£Œ! {chunks_stored}ê°œì˜ ì²­í¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                                total_chunks += chunks_stored
                                processed_files += 1
                else:
                    # ì¼ë°˜ ì²˜ë¦¬
                    result = file_processor.process_file(tmp_file_path)
                    if result and result.get('processed_pages'):
                        chunks_stored = embed_and_store_chunks(result, uploaded_file.name)
                        if chunks_stored > 0:
                            st.success(f"âœ… {uploaded_file.name} ì²˜ë¦¬ ì™„ë£Œ! {chunks_stored}ê°œì˜ ì²­í¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                            total_chunks += chunks_stored
                            processed_files += 1
            
            # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            os.unlink(tmp_file_path)
            
        except Exception as e:
            st.error(f"âŒ {uploaded_file.name} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    if processed_files > 0:
        st.success(f"ğŸ‰ ì´ {processed_files}ê°œ íŒŒì¼ ì²˜ë¦¬ ì™„ë£Œ! {total_chunks}ê°œì˜ ì²­í¬ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        st.rerun()  # DB í†µê³„ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨

def get_structured_chunk_stats():
    """êµ¬ì¡°ì  ì²­í¬ í†µê³„ ì¡°íšŒ"""
    try:
        vector_db = VectorDBManager()
        stats = vector_db.get_structured_chunk_stats()
        return stats
    except Exception as e:
        st.error(f"âŒ êµ¬ì¡°ì  ì²­í¬ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return {"total_chunks": 0, "chunk_types": {}, "unique_tickets": 0, "tickets": {}}
