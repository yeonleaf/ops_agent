#!/usr/bin/env python3
"""
XML Jira ë°ì´í„°ë¥¼ ì›ë¬¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ Vector DB ì¬êµ¬ì¶•
- ì»¤ìŠ¤í…€ ì „ì²˜ë¦¬ ì œê±°ë¨
- ko-sroberta-multitask ëª¨ë¸ ì‚¬ìš©
- L2 ì •ê·œí™” ì ìš©
"""

import xml.etree.ElementTree as ET
import re
import html
from typing import List, Dict, Any
import chromadb
from chromadb.config import Settings
from clean_korean_embedding import CleanKoreanEmbeddingFunction
from jira_chunk_processor import JiraChunkProcessor
from jira_chunk_models import JiraChunk, JiraChunkType
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def cleanse_text(text: str) -> str:
    """
    ì„ë² ë”© ì „ í…ìŠ¤íŠ¸ ì •ì œ - ì¡ìŒ ì œê±°
    ë°˜ë³µë˜ëŠ” ë©”íƒ€ë°ì´í„° íŒ¨í„´ì„ ì œê±°í•˜ì—¬ ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
    """
    if not text:
        return ""
    
    # 1. Jira í‹°ì¼“ í‚¤ íŒ¨í„´ ì œê±° [BTVO-NNNNN]
    text = re.sub(r'\[BTVO-\s?\d+\]', '', text)
    
    # 2. NCMS íŒ¨í„´ ì œê±° [NCMS]
    text = re.sub(r'\[NCMS\]', '', text)
    
    # 3. ë‚ ì§œ íŒ¨í„´ ì œê±° (MM/DD) ë˜ëŠ” (YYYY-MM-DD)
    text = re.sub(r'\(\d{1,2}/\d{1,2}\)', '', text)
    text = re.sub(r'\(\d{4}-\d{2}-\d{2}\)', '', text)
    
    # 4. ê¸°íƒ€ ë¶ˆí•„ìš”í•œ íŒ¨í„´ë“¤
    text = re.sub(r'\[.*?\]', '', text)  # ëŒ€ê´„í˜¸ ì•ˆì˜ ëª¨ë“  ë‚´ìš© ì œê±°
    text = re.sub(r'\(.*?\)', '', text)  # ì†Œê´„í˜¸ ì•ˆì˜ ëª¨ë“  ë‚´ìš© ì œê±°
    
    # 5. ì—°ì†ëœ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    
    # 6. ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()
    
    return text

def create_multi_vector_chunks_from_ticket(ticket: Dict[str, Any]) -> List[JiraChunk]:
    """
    ë‹¤ì¤‘ ë²¡í„° í‘œí˜„ì„ ìœ„í•œ ì²­í¬ ìƒì„±
    - ì œëª© ì²­í¬ (1ê°œ): í‹°ì¼“ì˜ ì œëª©ë§Œìœ¼ë¡œ êµ¬ì„±
    - ì„¤ëª… ì²­í¬ (1ê°œ): í‹°ì¼“ì˜ ë³¸ë¬¸(Description)ë§Œìœ¼ë¡œ êµ¬ì„±  
    - ëŒ“ê¸€ ì²­í¬ (Nê°œ): ê° ëŒ“ê¸€ì„ ê°œë³„ ì²­í¬ë¡œ ìƒì„± (ë¬¸ë§¥ ìœ ì§€)
    """
    chunks = []
    ticket_key = ticket.get('Key', 'UNKNOWN')
    ticket_summary = ticket.get('Summary', '')
    description = ticket.get('Description', '')
    comments = ticket.get('Comments', [])
    
    # í…ìŠ¤íŠ¸ ì •ì œ ì ìš©
    clean_summary = cleanse_text(ticket_summary)
    clean_description = cleanse_text(description)
    
    # 1. ì œëª© ì²­í¬ ìƒì„± (1ê°œ)
    if clean_summary:
        title_chunk = JiraChunk(
            ticket_id=f"{ticket_key}_title",
            parent_ticket_id=ticket_key,
            ticket_summary=clean_summary,
            chunk_type=JiraChunkType.SUMMARY,
            content=clean_summary,
            field_name="title",
            field_value=clean_summary
        )
        chunks.append(title_chunk)
    
    # 2. ì„¤ëª… ì²­í¬ ìƒì„± (1ê°œ) - ê¸´ ì„¤ëª…ë„ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ìœ ì§€
    if clean_description:
        description_chunk = JiraChunk(
            ticket_id=f"{ticket_key}_description",
            parent_ticket_id=ticket_key,
            ticket_summary=clean_summary,
            chunk_type=JiraChunkType.DESCRIPTION,
            content=clean_description,
            field_name="description",
            field_value=clean_description
        )
        chunks.append(description_chunk)
    
    # 3. ëŒ“ê¸€ ì²­í¬ ìƒì„± (Nê°œ) - ê° ëŒ“ê¸€ì„ ê°œë³„ ì²­í¬ë¡œ ìƒì„±
    for i, comment in enumerate(comments):
        comment_text = comment.get('text', '')
        comment_author = comment.get('author', 'Unknown')
        comment_date = comment.get('date', '')
        
        if comment_text:
            # ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì •ì œ
            clean_comment = cleanse_text(comment_text)
            
            # ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•œ ëŒ“ê¸€ ì²­í¬ í¬ë§·
            comment_content = f"í‹°ì¼“ ì œëª©: {clean_summary}\n\nëŒ“ê¸€: {clean_comment}"
            
            comment_chunk = JiraChunk(
                ticket_id=f"{ticket_key}_comment_{i}",
                parent_ticket_id=ticket_key,
                ticket_summary=clean_summary,
                chunk_type=JiraChunkType.COMMENT,
                content=comment_content,
                field_name="comment",
                field_value=clean_comment,
                comment_author=comment_author,
                comment_date=comment_date,
                comment_id=f"comment_{i}"
            )
            chunks.append(comment_chunk)
    
    return chunks

def create_chunks_from_ticket(ticket: Dict[str, Any]) -> List[JiraChunk]:
    """ê¸°ì¡´ ë°©ì‹ ìœ ì§€ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    return create_multi_vector_chunks_from_ticket(ticket)

def clean_html_content(html_content: str) -> str:
    """HTML íƒœê·¸ì™€ ì—”í‹°í‹°ë¥¼ ì •ë¦¬í•˜ì—¬ ìˆœìˆ˜ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
    if not html_content:
        return ""
    
    # HTML ì—”í‹°í‹° ë””ì½”ë”©
    text = html.unescape(html_content)
    
    # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'<[^>]+>', '', text)
    
    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

def parse_jira_xml(xml_file_path: str) -> List[Dict[str, Any]]:
    """XML íŒŒì¼ì—ì„œ Jira í‹°ì¼“ ë°ì´í„° íŒŒì‹±"""
    logger.info(f"ğŸ“„ XML íŒŒì¼ íŒŒì‹± ì‹œì‘: {xml_file_path}")
    
    try:
        tree = ET.parse(xml_file_path)
        root = tree.getroot()
        
        tickets = []
        
        # RSS êµ¬ì¡°ì—ì„œ item ìš”ì†Œë“¤ ì°¾ê¸°
        for item in root.findall('.//item'):
            try:
                # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                key_elem = item.find('key')
                title_elem = item.find('title')
                summary_elem = item.find('summary')
                description_elem = item.find('description')
                type_elem = item.find('type')
                priority_elem = item.find('priority')
                status_elem = item.find('status')
                assignee_elem = item.find('assignee')
                reporter_elem = item.find('reporter')
                created_elem = item.find('created')
                updated_elem = item.find('updated')
                
                # í‚¤ ì¶”ì¶œ
                key = key_elem.text if key_elem is not None else "UNKNOWN"
                
                # ì œëª© ì¶”ì¶œ (titleì—ì„œ [KEY] ë¶€ë¶„ ì œê±°)
                title = title_elem.text if title_elem is not None else ""
                if title.startswith(f"[{key}]"):
                    title = title[len(f"[{key}]"):].strip()
                
                # ìš”ì•½ ì¶”ì¶œ
                summary = summary_elem.text if summary_elem is not None else ""
                if summary.startswith(f"[{key}]"):
                    summary = summary[len(f"[{key}]"):].strip()
                
                # ì„¤ëª… ì¶”ì¶œ ë° HTML ì •ë¦¬
                description = ""
                if description_elem is not None and description_elem.text:
                    description = clean_html_content(description_elem.text)
                
                # ê¸°íƒ€ ë©”íƒ€ë°ì´í„°
                issue_type = type_elem.text if type_elem is not None else "Unknown"
                priority = priority_elem.text if priority_elem is not None else "Unknown"
                status = status_elem.text if status_elem is not None else "Unknown"
                assignee = assignee_elem.text if assignee_elem is not None else "Unknown"
                reporter = reporter_elem.text if reporter_elem is not None else "Unknown"
                created = created_elem.text if created_elem is not None else ""
                updated = updated_elem.text if updated_elem is not None else ""
                
                # ëŒ“ê¸€ ë°ì´í„° ì¶”ì¶œ
                comments = []
                comments_elem = item.find('comments')
                if comments_elem is not None:
                    for comment_elem in comments_elem.findall('comment'):
                        comment_text = ""
                        comment_author = "Unknown"
                        comment_date = ""
                        
                        # ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        if comment_elem.text:
                            comment_text = clean_html_content(comment_elem.text)
                        
                        # ëŒ“ê¸€ ì‘ì„±ì ì¶”ì¶œ
                        author_elem = comment_elem.find('author')
                        if author_elem is not None:
                            comment_author = author_elem.text or "Unknown"
                        
                        # ëŒ“ê¸€ ë‚ ì§œ ì¶”ì¶œ
                        date_elem = comment_elem.find('created')
                        if date_elem is not None:
                            comment_date = date_elem.text or ""
                        
                        if comment_text.strip():
                            comments.append({
                                'text': comment_text,
                                'author': comment_author,
                                'date': comment_date
                            })
                
                # í‹°ì¼“ ë°ì´í„° êµ¬ì„±
                ticket = {
                    'Key': key,
                    'Summary': summary or title,  # summaryê°€ ì—†ìœ¼ë©´ title ì‚¬ìš©
                    'Description': description,
                    'Status': status,
                    'Priority': priority,
                    'Issue Type': issue_type,
                    'Assignee': assignee,
                    'Reporter': reporter,
                    'Created': created,
                    'Updated': updated,
                    'Comments': comments
                }
                
                tickets.append(ticket)
                
            except Exception as e:
                logger.warning(f"âš ï¸ í‹°ì¼“ íŒŒì‹± ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"âœ… {len(tickets)}ê°œ í‹°ì¼“ íŒŒì‹± ì™„ë£Œ")
        return tickets
        
    except Exception as e:
        logger.error(f"âŒ XML íŒŒì‹± ì‹¤íŒ¨: {e}")
        return []

def rebuild_vector_db():
    """Vector DB ì¬êµ¬ì¶•"""
    logger.info("ğŸ—ï¸ Vector DB ì¬êµ¬ì¶• ì‹œì‘...")
    
    # 1. ê¸°ì¡´ Vector DB ì‚­ì œ í™•ì¸
    import os
    if os.path.exists('./vector_db'):
        logger.info("ğŸ—‘ï¸ ê¸°ì¡´ Vector DB ì‚­ì œë¨")
    else:
        logger.info("ğŸ“ Vector DB ë””ë ‰í† ë¦¬ ì—†ìŒ (ìƒˆë¡œ ìƒì„±)")
    
    # 2. í•œêµ­ì–´ ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™”
    logger.info("ğŸ”§ í•œêµ­ì–´ ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™”...")
    embedding_function = CleanKoreanEmbeddingFunction()
    
    # 3. ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    logger.info("ğŸ’¾ ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”...")
    client = chromadb.PersistentClient(
        path='./vector_db',
        settings=Settings(anonymized_telemetry=False, allow_reset=True)
    )
    
    # 4. Jira ë‹¤ì¤‘ ë²¡í„° ì²­í¬ ì»¬ë ‰ì…˜ ìƒì„±
    logger.info("ğŸ“ Jira ë‹¤ì¤‘ ë²¡í„° ì²­í¬ ì»¬ë ‰ì…˜ ìƒì„±...")
    try:
        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ (ìˆë‹¤ë©´)
        try:
            client.delete_collection('jira_multi_vector_chunks')
            logger.info("ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œë¨")
        except:
            pass
        
        collection = client.create_collection(
            name='jira_multi_vector_chunks',
            embedding_function=embedding_function,
            metadata={
                'description': 'Jira ë‹¤ì¤‘ ë²¡í„° ì²­í¬ ì»¬ë ‰ì…˜ - ì œëª©/ì„¤ëª…/ëŒ“ê¸€ë³„ ê°œë³„ ë²¡í„°',
                'embedding_model': 'jhgan/ko-sroberta-multitask',
                'embedding_dimension': 768,
                'language': 'korean',
                'l2_normalization': True,
                'custom_preprocessing': False,
                'tokenizer': 'BertTokenizerFast',
                'multi_vector_representation': True,
                'chunk_types': 'title,description,comment'
            }
        )
        logger.info("âœ… ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
        
    except Exception as e:
        logger.error(f"âŒ ì»¬ë ‰ì…˜ ìƒì„± ì‹¤íŒ¨: {e}")
        return False
    
    # 5. Jira ì²­í¬ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” (ì „ì²˜ë¦¬ ì œê±°ë¨)
    logger.info("ğŸ”§ Jira ì²­í¬ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”...")
    processor = JiraChunkProcessor(enable_text_cleaning=False)
    
    # 6. XML íŒŒì¼ë“¤ì—ì„œ Jira ë°ì´í„° íŒŒì‹±
    xml_files = ['ncms_1.xml', 'ncms_2.xml']
    all_tickets = []
    
    for xml_file in xml_files:
        if os.path.exists(xml_file):
            tickets = parse_jira_xml(xml_file)
            all_tickets.extend(tickets)
            logger.info(f"ğŸ“Š {xml_file}: {len(tickets)}ê°œ í‹°ì¼“")
        else:
            logger.warning(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {xml_file}")
    
    logger.info(f"ğŸ“Š ì´ {len(all_tickets)}ê°œ í‹°ì¼“ íŒŒì‹± ì™„ë£Œ")
    
    if not all_tickets:
        logger.error("âŒ íŒŒì‹±ëœ í‹°ì¼“ì´ ì—†ìŠµë‹ˆë‹¤")
        return False
    
    # 7. ì²­í¬ ìƒì„± ë° Vector DBì— ì €ì¥
    logger.info("ğŸ”„ ì²­í¬ ìƒì„± ë° Vector DB ì €ì¥ ì‹œì‘...")
    total_chunks = 0
    processed_tickets = 0
    
    for i, ticket in enumerate(all_tickets):
        try:
            # ì§ì ‘ ì²­í¬ ìƒì„± (ì›ë¬¸ ê·¸ëŒ€ë¡œ)
            chunks = create_chunks_from_ticket(ticket)
            
            if chunks:
                # Vector DBì— ì €ì¥
                for chunk in chunks:
                    try:
                        # ë¬¸ì„œ í™•ì¥ëœ ë‚´ìš© ì‚¬ìš©
                        expanded_content = chunk.create_expanded_content()
                        
                        # ChromaDBì— ì¶”ê°€ (None ê°’ ì œê±°)
                        metadata = {
                            'parent_ticket_id': chunk.parent_ticket_id or '',
                            'ticket_key': chunk.ticket_id or '',
                            'ticket_summary': chunk.ticket_summary or '',
                            'chunk_type': chunk.chunk_type.value,
                            'original_content': chunk.content or '',
                            'expanded_content': expanded_content or '',
                            'field_name': chunk.field_name or '',
                            'field_value': chunk.field_value or '',
                            'document_expansion': True,
                            'custom_preprocessing': False,
                            'l2_normalization': True,
                            'multi_vector_representation': True
                        }
                        
                        # Noneì´ ì•„ë‹Œ ëŒ“ê¸€ ê´€ë ¨ í•„ë“œë§Œ ì¶”ê°€
                        if chunk.comment_author is not None:
                            metadata['comment_author'] = chunk.comment_author
                        if chunk.comment_date is not None:
                            metadata['comment_date'] = chunk.comment_date
                        if chunk.comment_id is not None:
                            metadata['comment_id'] = chunk.comment_id
                        
                        collection.add(
                            documents=[expanded_content],
                            metadatas=[metadata],
                            ids=[f"{chunk.parent_ticket_id}_{chunk.chunk_type.value}_{chunk.chunk_id}"]
                        )
                        total_chunks += 1
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ ì²­í¬ ì €ì¥ ì‹¤íŒ¨ {chunk.ticket_id}: {e}")
                        continue
                
                processed_tickets += 1
                
                # ì§„í–‰ ìƒí™© ë¡œê·¸
                if (i + 1) % 50 == 0:
                    logger.info(f"ğŸ“ˆ ì§„í–‰ë¥ : {i + 1}/{len(all_tickets)} í‹°ì¼“, {total_chunks}ê°œ ì²­í¬ ì €ì¥ë¨")
            
        except Exception as e:
            logger.warning(f"âš ï¸ í‹°ì¼“ ì²˜ë¦¬ ì‹¤íŒ¨ {ticket.get('Key', 'UNKNOWN')}: {e}")
            continue
    
    # 8. ìµœì¢… ê²°ê³¼
    logger.info("ğŸ‰ ë‹¤ì¤‘ ë²¡í„° Vector DB ì¬êµ¬ì¶• ì™„ë£Œ!")
    logger.info(f"   âœ… ì²˜ë¦¬ëœ í‹°ì¼“: {processed_tickets}/{len(all_tickets)}")
    logger.info(f"   âœ… ì €ì¥ëœ ì²­í¬: {total_chunks}ê°œ")
    logger.info(f"   âœ… ë‹¤ì¤‘ ë²¡í„° í‘œí˜„ ì ìš©ë¨")
    logger.info(f"   âœ… ì œëª©/ì„¤ëª…/ëŒ“ê¸€ë³„ ê°œë³„ ë²¡í„°")
    logger.info(f"   âœ… ë¬¸ë§¥ ìœ ì§€ ëŒ“ê¸€ ì²­í¬")
    logger.info(f"   âœ… L2 ì •ê·œí™” ì ìš©ë¨")
    logger.info(f"   âœ… ì˜¬ë°”ë¥¸ í† í¬ë‚˜ì´ì € ì‚¬ìš©")
    
    # 9. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    logger.info("ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸...")
    try:
        test_results = collection.query(
            query_texts=["ì„œë²„ ë¬¸ì œ", "ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜"],
            n_results=3
        )
        
        logger.info(f"ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
        for i, (doc, meta, dist) in enumerate(zip(
            test_results['documents'][0], 
            test_results['metadatas'][0], 
            test_results['distances'][0]
        )):
            similarity = 1 - dist  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
            logger.info(f"  {i+1}. ìœ ì‚¬ë„: {similarity:.3f}")
            logger.info(f"     ë¶€ëª¨ í‹°ì¼“: {meta.get('parent_ticket_id', 'Unknown')}")
            logger.info(f"     ì²­í¬ íƒ€ì…: {meta.get('chunk_type', 'Unknown')}")
            logger.info(f"     í•„ë“œëª…: {meta.get('field_name', 'Unknown')}")
            logger.info(f"     ë‚´ìš©: {doc[:100]}...")
    
    except Exception as e:
        logger.warning(f"âš ï¸ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    return True

if __name__ == "__main__":
    success = rebuild_vector_db()
    if success:
        print("\nğŸ‰ Vector DB ì¬êµ¬ì¶•ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ Vector DB ì¬êµ¬ì¶•ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
