#!/usr/bin/env python3
"""
Retrieve then Re-rank ê²€ìƒ‰ ì‹œìŠ¤í…œ
BM25 í‚¤ì›Œë“œ ê²€ìƒ‰ê³¼ ë²¡í„° ê²€ìƒ‰ì„ ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰í•œ í›„, CohereRerankë¡œ ìµœì¢… ì„ ë³„
"""

import logging
from typing import List, Dict, Any, Optional
from langchain_community.retrievers import BM25Retriever
from langchain_core.documents import Document
from vector_db_models import VectorDBManager
from text_preprocessor import preprocess_for_embedding
from keyword_extractor import KeywordExtractor
import cohere
import os
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)

class RetrieveRerankRetriever:
    """
    Retrieve then Re-rank ê²€ìƒ‰ ì‹œìŠ¤í…œ
    1ë‹¨ê³„: Vector + BM25 ë…ë¦½ ê²€ìƒ‰
    2ë‹¨ê³„: í›„ë³´êµ° í†µí•© ë° ì¤‘ë³µ ì œê±°
    3ë‹¨ê³„: CohereRerankë¡œ ìµœì¢… ì„ ë³„
    """
    
    def __init__(self, vector_db_manager: VectorDBManager, enable_bm25: bool = False):
        self.vector_db_manager = vector_db_manager
        self.bm25_retriever = None
        self.documents = []
        self.keyword_extractor = KeywordExtractor()
        self.cohere_client = cohere.Client(os.getenv("COHERE_API_KEY"))
        self.enable_bm25 = enable_bm25
        
        # BM25ê°€ í™œì„±í™”ëœ ê²½ìš°ì—ë§Œ ë¬¸ì„œ ìˆ˜ì§‘ ë° ì¸ë±ìŠ¤ ìƒì„±
        if self.enable_bm25:
            self._collect_documents()
            self._setup_bm25_retriever()
        else:
            logger.info("âš ï¸ BM25 ê²€ìƒ‰ ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ)")
        
        logger.info("âœ… RetrieveRerankRetriever ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _collect_documents(self):
        """ì œí•œëœ ë¬¸ì„œë¥¼ ìˆ˜ì§‘í•˜ì—¬ BM25 ì¸ë±ìŠ¤ ìƒì„±ì„ ìœ„í•œ Document ê°ì²´ ìƒì„± (ë©”ëª¨ë¦¬ ì ˆì•½)"""
        try:
            logger.info("ğŸ“š ì œí•œëœ ë¬¸ì„œ ìˆ˜ì§‘ ì‹œì‘ (ë©”ëª¨ë¦¬ ì ˆì•½)...")
            
            # 1. íŒŒì¼ ì²­í¬ ë¬¸ì„œ ìˆ˜ì§‘ (ìµœëŒ€ 200ê°œë¡œ ì œí•œ)
            file_chunks = self._get_file_chunks(limit=200)
            self.documents.extend(file_chunks)
            logger.info(f"âœ… íŒŒì¼ ì²­í¬ ë¬¸ì„œ {len(file_chunks)}ê°œ ìˆ˜ì§‘ (ì œí•œë¨)")
            
            # 2. ë©”ì¼ ë¬¸ì„œ ìˆ˜ì§‘ (ìµœëŒ€ 50ê°œë¡œ ì œí•œ)
            mail_docs = self._get_mail_documents(limit=50)
            self.documents.extend(mail_docs)
            logger.info(f"âœ… ë©”ì¼ ë¬¸ì„œ {len(mail_docs)}ê°œ ìˆ˜ì§‘ (ì œí•œë¨)")
            
            # 3. êµ¬ì¡°ì  ì²­í¬ ë¬¸ì„œ ìˆ˜ì§‘ (ìµœëŒ€ 50ê°œë¡œ ì œí•œ)
            structured_docs = self._get_structured_documents(limit=50)
            self.documents.extend(structured_docs)
            logger.info(f"âœ… êµ¬ì¡°ì  ì²­í¬ ë¬¸ì„œ {len(structured_docs)}ê°œ ìˆ˜ì§‘ (ì œí•œë¨)")
            
            logger.info(f"âœ… ì´ {len(self.documents)}ê°œ ë¬¸ì„œ ìˆ˜ì§‘ ì™„ë£Œ (ë©”ëª¨ë¦¬ ì ˆì•½ ëª¨ë“œ)")
            
        except Exception as e:
            logger.error(f"âŒ ë¬¸ì„œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            self.documents = []
    
    def _get_file_chunks(self, limit: int = 200) -> List[Document]:
        """íŒŒì¼ ì²­í¬ ë¬¸ì„œë“¤ì„ Document ê°ì²´ë¡œ ë³€í™˜ (ì œí•œëœ ìˆ˜)"""
        documents = []
        try:
            file_chunks = self.vector_db_manager.get_all_file_chunks()
            # ì œí•œëœ ìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
            limited_chunks = file_chunks[:limit]
            for chunk in limited_chunks:
                content = chunk.get('content', '')
                if content and len(content.strip()) > 0:
                    metadata = chunk.get('metadata', {})
                    metadata.update({
                        'source_type': 'file_chunk',
                        'chunk_id': chunk.get('chunk_id', ''),
                        'file_name': chunk.get('file_name', '')
                    })
                    documents.append(Document(page_content=content, metadata=metadata))
        except Exception as e:
            logger.error(f"íŒŒì¼ ì²­í¬ ë¬¸ì„œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return documents
    
    def _get_mail_documents(self, limit: int = 50) -> List[Document]:
        """ë©”ì¼ ë¬¸ì„œë“¤ì„ Document ê°ì²´ë¡œ ë³€í™˜ (ì œí•œëœ ìˆ˜)"""
        documents = []
        try:
            mails = self.vector_db_manager.get_all_mails()
            # ì œí•œëœ ìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
            limited_mails = mails[:limit]
            for mail in limited_mails:
                content = mail.get('content', '')
                if content and len(content.strip()) > 0:
                    metadata = mail.get('metadata', {})
                    metadata.update({
                        'source_type': 'mail',
                        'message_id': mail.get('message_id', ''),
                        'subject': mail.get('subject', ''),
                        'sender': mail.get('sender', '')
                    })
                    documents.append(Document(page_content=content, metadata=metadata))
        except Exception as e:
            logger.error(f"ë©”ì¼ ë¬¸ì„œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return documents
    
    def _get_structured_documents(self, limit: int = 50) -> List[Document]:
        """êµ¬ì¡°ì  ì²­í¬ ë¬¸ì„œë“¤ì„ Document ê°ì²´ë¡œ ë³€í™˜ (ì œí•œëœ ìˆ˜)"""
        documents = []
        try:
            structured_chunks = self.vector_db_manager.get_all_structured_chunks()
            # ì œí•œëœ ìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
            limited_chunks = structured_chunks[:limit]
            for chunk in limited_chunks:
                content = chunk.get('content', '')
                if content and len(content.strip()) > 0:
                    metadata = chunk.get('metadata', {})
                    metadata.update({
                        'source_type': 'structured_chunk',
                        'chunk_id': chunk.get('chunk_id', ''),
                        'ticket_id': chunk.get('ticket_id', ''),
                        'chunk_type': chunk.get('chunk_type', '')
                    })
                    documents.append(Document(page_content=content, metadata=metadata))
        except Exception as e:
            logger.error(f"êµ¬ì¡°ì  ì²­í¬ ë¬¸ì„œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return documents
    
    def _setup_bm25_retriever(self):
        """BM25Retriever ì„¤ì •"""
        try:
            if not self.documents:
                logger.warning("âš ï¸ ìˆ˜ì§‘ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ë¹ˆ BM25Retrieverë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
                self.documents = [Document(page_content="", metadata={})]
            
            logger.info("ğŸ” BM25Retriever ìƒì„± ì¤‘...")
            self.bm25_retriever = BM25Retriever.from_documents(self.documents, k=10)
            logger.info("âœ… BM25Retriever ìƒì„± ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ BM25Retriever ì„¤ì • ì‹¤íŒ¨: {e}")
            self.bm25_retriever = None
    
    def _create_vector_retriever(self):
        """VectorDBManagerë¥¼ ìœ„í•œ Vector Retriever ìƒì„±"""
        from multi_query_retriever import ChromaDBRetriever
        return ChromaDBRetriever(
            vector_db_manager=self.vector_db_manager,
            collection_name="file_chunks"
        )
    
    def _perform_vector_search(self, query: str, k: int = 10) -> List[Document]:
        """1ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ ìˆ˜í–‰ (ì§ì ‘ VectorDBManager ì‚¬ìš©)"""
        try:
            logger.info(f"ğŸ” ë²¡í„° ê²€ìƒ‰ ì‹œì‘: '{query}'")
            
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬
            preprocessed_query = preprocess_for_embedding(query)
            
            # VectorDBManagerë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰
            search_results = self.vector_db_manager.search_similar_file_chunks(preprocessed_query, n_results=k)
            
            # ê²°ê³¼ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
            documents = []
            for i, result in enumerate(search_results):
                content = result.get('content', '')
                metadata = result.get('metadata', {})
                metadata.update({
                    'search_type': 'vector',
                    'search_rank': i + 1,
                    'similarity_score': result.get('similarity_score', 0.0)
                })
                documents.append(Document(page_content=content, metadata=metadata))
            
            logger.info(f"âœ… ë²¡í„° ê²€ìƒ‰ ì™„ë£Œ: {len(documents)}ê°œ ê²°ê³¼")
            return documents
            
        except Exception as e:
            logger.error(f"âŒ ë²¡í„° ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _perform_bm25_search(self, query: str, k: int = 10) -> List[Document]:
        """1ë‹¨ê³„: BM25 ê²€ìƒ‰ ìˆ˜í–‰"""
        try:
            if not self.enable_bm25 or not self.bm25_retriever:
                logger.warning("âš ï¸ BM25 ê²€ìƒ‰ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return []
            
            logger.info(f"ğŸ” BM25 ê²€ìƒ‰ ì‹œì‘: '{query}'")
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self.keyword_extractor.extract_keywords(query)
            keyword_query = " ".join(keywords)
            logger.info(f"ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
            
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬
            preprocessed_query = preprocess_for_embedding(keyword_query)
            
            # BM25 ê²€ìƒ‰ ìˆ˜í–‰
            documents = self.bm25_retriever.get_relevant_documents(preprocessed_query)
            
            # ê²°ê³¼ì— ê²€ìƒ‰ íƒ€ì… í‘œì‹œ
            for doc in documents:
                doc.metadata['search_type'] = 'bm25'
                doc.metadata['search_rank'] = documents.index(doc) + 1
                doc.metadata['extracted_keywords'] = keywords
            
            logger.info(f"âœ… BM25 ê²€ìƒ‰ ì™„ë£Œ: {len(documents)}ê°œ ê²°ê³¼")
            return documents[:k]
            
        except Exception as e:
            logger.error(f"âŒ BM25 ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _merge_and_deduplicate_candidates(self, vector_docs: List[Document], bm25_docs: List[Document]) -> List[Document]:
        """2ë‹¨ê³„: í›„ë³´êµ° í†µí•© ë° ì¤‘ë³µ ì œê±°"""
        try:
            logger.info("ğŸ”„ í›„ë³´êµ° í†µí•© ë° ì¤‘ë³µ ì œê±° ì‹œì‘...")
            
            # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ë”•ì…”ë„ˆë¦¬
            unique_docs = {}
            
            # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ë¨¼ì € ì¶”ê°€
            for doc in vector_docs:
                doc_id = doc.metadata.get('chunk_id', doc.metadata.get('message_id', str(hash(doc.page_content))))
                if doc_id not in unique_docs:
                    unique_docs[doc_id] = doc
                else:
                    # ì¤‘ë³µ ë¬¸ì„œì¸ ê²½ìš° ë²¡í„° ê²€ìƒ‰ ê²°ê³¼ ìš°ì„  (ë” ë†’ì€ í’ˆì§ˆ)
                    existing_doc = unique_docs[doc_id]
                    existing_doc.metadata['search_type'] = 'hybrid'
                    existing_doc.metadata['vector_rank'] = doc.metadata.get('search_rank', 0)
                    existing_doc.metadata['bm25_rank'] = existing_doc.metadata.get('search_rank', 0)
            
            # BM25 ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€ (ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²ƒë§Œ)
            for doc in bm25_docs:
                doc_id = doc.metadata.get('chunk_id', doc.metadata.get('message_id', str(hash(doc.page_content))))
                if doc_id not in unique_docs:
                    unique_docs[doc_id] = doc
                else:
                    # ì¤‘ë³µ ë¬¸ì„œì¸ ê²½ìš° BM25 ìˆœìœ„ ì •ë³´ ì¶”ê°€
                    existing_doc = unique_docs[doc_id]
                    if existing_doc.metadata.get('search_type') == 'hybrid':
                        existing_doc.metadata['bm25_rank'] = doc.metadata.get('search_rank', 0)
            
            # ìµœì¢… í›„ë³´êµ° ë¦¬ìŠ¤íŠ¸ ìƒì„±
            final_candidates = list(unique_docs.values())
            
            logger.info(f"âœ… í›„ë³´êµ° í†µí•© ì™„ë£Œ: ë²¡í„° {len(vector_docs)}ê°œ, BM25 {len(bm25_docs)}ê°œ â†’ í†µí•© {len(final_candidates)}ê°œ")
            return final_candidates
            
        except Exception as e:
            logger.error(f"âŒ í›„ë³´êµ° í†µí•© ì‹¤íŒ¨: {e}")
            return vector_docs + bm25_docs  # ì‹¤íŒ¨ ì‹œ ë‹¨ìˆœ í•©ì¹˜ê¸°
    
    def _rerank_candidates(self, query: str, candidates: List[Document], k: int = 3) -> List[Document]:
        """3ë‹¨ê³„: CohereRerankë¡œ ìµœì¢… ì„ ë³„"""
        try:
            if not candidates:
                logger.warning("âš ï¸ ì¬ìˆœìœ„í™”í•  í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            logger.info(f"ğŸ¯ CohereRerank ì¬ìˆœìœ„í™” ì‹œì‘: {len(candidates)}ê°œ í›„ë³´ â†’ {k}ê°œ ìµœì¢… ê²°ê³¼")
            
            # Document ê°ì²´ë¥¼ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            documents_text = [doc.page_content for doc in candidates]
            
            # Cohere APIë¥¼ ì‚¬ìš©í•œ ì¬ìˆœìœ„í™”
            response = self.cohere_client.rerank(
                model="rerank-multilingual-v3.0",
                query=query,
                documents=documents_text,
                top_n=k
            )
            
            # ì¬ìˆœìœ„í™”ëœ ê²°ê³¼ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜
            final_documents = []
            for i, result in enumerate(response.results):
                # ì›ë³¸ Document ì°¾ê¸°
                original_doc = None
                for doc in candidates:
                    if doc.page_content == documents_text[result.index]:
                        original_doc = doc
                        break
                
                if original_doc:
                    # ì¬ìˆœìœ„í™” ì •ë³´ ì¶”ê°€
                    original_doc.metadata.update({
                        'rerank_score': result.relevance_score,
                        'final_rank': i + 1,
                        'rerank_method': 'cohere'
                    })
                    final_documents.append(original_doc)
            
            logger.info(f"âœ… CohereRerank ì¬ìˆœìœ„í™” ì™„ë£Œ: {len(final_documents)}ê°œ ìµœì¢… ê²°ê³¼")
            
            # ìƒìœ„ 3ê°œ ê²°ê³¼ì˜ ì ìˆ˜ ë¡œê¹…
            for i, doc in enumerate(final_documents[:3]):
                rerank_score = doc.metadata.get('rerank_score', 0)
                search_type = doc.metadata.get('search_type', 'unknown')
                logger.info(f"  - ìµœì¢… ê²°ê³¼ {i+1}: {search_type} (ì¬ìˆœìœ„í™” ì ìˆ˜: {rerank_score:.3f})")
            
            return final_documents
            
        except Exception as e:
            logger.error(f"âŒ CohereRerank ì¬ìˆœìœ„í™” ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ì›ë³¸ ìˆœì„œëŒ€ë¡œ ìƒìœ„ kê°œ ë°˜í™˜
            return candidates[:k]
    
    def search(self, query: str, k: int = 3) -> List[Dict[str, Any]]:
        """
        Retrieve then Re-rank ê²€ìƒ‰ ìˆ˜í–‰
        1ë‹¨ê³„: Vector + BM25 ë…ë¦½ ê²€ìƒ‰
        2ë‹¨ê³„: í›„ë³´êµ° í†µí•© ë° ì¤‘ë³µ ì œê±°
        3ë‹¨ê³„: CohereRerankë¡œ ìµœì¢… ì„ ë³„
        """
        try:
            logger.info(f"ğŸš€ Retrieve then Re-rank ê²€ìƒ‰ ì‹œì‘: '{query}'")
            
            # 1ë‹¨ê³„: ë‘ ê²€ìƒ‰ê¸°ë¡œ ê°ê° ê²€ìƒ‰
            vector_docs = self._perform_vector_search(query, k=10)
            bm25_docs = self._perform_bm25_search(query, k=10)
            
            # 2ë‹¨ê³„: í›„ë³´êµ° í†µí•© ë° ì¤‘ë³µ ì œê±°
            candidates = self._merge_and_deduplicate_candidates(vector_docs, bm25_docs)
            
            # 3ë‹¨ê³„: CohereRerankë¡œ ìµœì¢… ì„ ë³„
            final_docs = self._rerank_candidates(query, candidates, k=k)
            
            # ê²°ê³¼ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            results = []
            for i, doc in enumerate(final_docs):
                result = {
                    "id": f"rerank_{i}",
                    "content": doc.page_content,
                    "metadata": doc.metadata,
                    "similarity_score": doc.metadata.get("rerank_score", 0.0),
                    "source": "retrieve_rerank",
                    "search_type": doc.metadata.get("search_type", "unknown"),
                    "final_rank": doc.metadata.get("final_rank", i + 1)
                }
                results.append(result)
            
            logger.info(f"âœ… Retrieve then Re-rank ê²€ìƒ‰ ì™„ë£Œ: {len(results)}ê°œ ìµœì¢… ê²°ê³¼")
            return results
            
        except Exception as e:
            logger.error(f"âŒ Retrieve then Re-rank ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_search_info(self) -> Dict[str, Any]:
        """ê²€ìƒ‰ ì‹œìŠ¤í…œ ì •ë³´ ë°˜í™˜"""
        return {
            "total_documents": len(self.documents),
            "bm25_retriever_ready": self.bm25_retriever is not None,
            "cohere_rerank_ready": self.cohere_client is not None,
            "search_method": "retrieve_then_rerank",
            "pipeline_steps": [
                "1. Vector Search (top 10)",
                "2. BM25 Search (top 10)", 
                "3. Merge & Deduplicate",
                "4. CohereRerank (final top 3)"
            ]
        }

def create_retrieve_rerank_retriever(vector_db_manager: VectorDBManager, enable_bm25: bool = False) -> RetrieveRerankRetriever:
    """RetrieveRerankRetriever ì¸ìŠ¤í„´ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    return RetrieveRerankRetriever(vector_db_manager, enable_bm25=enable_bm25)
