#!/usr/bin/env python3
"""
FileChunk â†’ UnifiedChunk ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

ê¸°ì¡´ file_chunks ì»¬ë ‰ì…˜ì˜ FileChunk ë°ì´í„°ë¥¼
UnifiedChunk í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.

Usage:
    # Dry-run (ì‹¤í–‰í•˜ì§€ ì•Šê³  ê³„íšë§Œ í™•ì¸)
    python scripts/migrate_filechunk_to_unified.py --dry-run

    # ë°±ì—… ìƒì„± í›„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
    python scripts/migrate_filechunk_to_unified.py --backup --execute

    # ë¡¤ë°± (ë°±ì—…ì—ì„œ ë³µì›)
    python scripts/migrate_filechunk_to_unified.py --rollback
"""

import argparse
import os
import sys
import shutil
from datetime import datetime
from typing import List, Dict, Any
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from chromadb_singleton import get_chromadb_client
from models.unified_chunk import UnifiedChunk
from vector_db_models import VectorDBManager


class FileChunkMigrator:
    """FileChunk â†’ UnifiedChunk ë§ˆì´ê·¸ë ˆì´ì…˜ ê´€ë¦¬ì"""

    def __init__(self, db_path: str = "./vector_db"):
        """
        ì´ˆê¸°í™”

        Args:
            db_path: ChromaDB ê²½ë¡œ
        """
        self.db_path = db_path
        self.backup_path = f"{db_path}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.client = get_chromadb_client()
        self.vector_db = VectorDBManager(db_path=db_path)

    def analyze_current_data(self) -> Dict[str, Any]:
        """
        í˜„ì¬ ë°ì´í„° ë¶„ì„

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        print("\n" + "=" * 60)
        print("ğŸ“Š í˜„ì¬ ë°ì´í„° ë¶„ì„")
        print("=" * 60)

        try:
            collection = self.client.get_collection("file_chunks")
            total_count = collection.count()

            print(f"âœ… file_chunks ì»¬ë ‰ì…˜ ë°œê²¬")
            print(f"   ì´ ì²­í¬ ê°œìˆ˜: {total_count}ê°œ")

            if total_count == 0:
                print("âš ï¸  ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë§ˆì´ê·¸ë ˆì´ì…˜ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.")
                return {"total_count": 0, "needs_migration": False}

            # ìƒ˜í”Œ ë°ì´í„° ì¡°íšŒ (ì²˜ìŒ 5ê°œ)
            sample_data = collection.get(limit=5, include=["metadatas", "documents"])

            print(f"\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„° êµ¬ì¡°:")
            if sample_data['metadatas']:
                sample_metadata = sample_data['metadatas'][0]
                print(f"   ë©”íƒ€ë°ì´í„° í‚¤: {list(sample_metadata.keys())}")

                # ìŠ¤í‚¤ë§ˆ ë²„ì „ í™•ì¸
                if "data_source" in sample_metadata:
                    print(f"   âœ… ì´ë¯¸ UnifiedChunk ìŠ¤í‚¤ë§ˆì…ë‹ˆë‹¤")
                    return {
                        "total_count": total_count,
                        "needs_migration": False,
                        "schema_version": "unified_v1"
                    }
                else:
                    print(f"   ğŸ”„ FileChunk ìŠ¤í‚¤ë§ˆ â†’ UnifiedChunk ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ í•„ìš”")
                    return {
                        "total_count": total_count,
                        "needs_migration": True,
                        "schema_version": "file_chunk_legacy"
                    }

        except Exception as e:
            print(f"âŒ ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"total_count": 0, "needs_migration": False, "error": str(e)}

    def backup_database(self) -> bool:
        """
        ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        print("\n" + "=" * 60)
        print("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ ë°±ì—…")
        print("=" * 60)

        try:
            if os.path.exists(self.backup_path):
                print(f"âš ï¸  ë°±ì—… ê²½ë¡œê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {self.backup_path}")
                print(f"   ê¸°ì¡´ ë°±ì—…ì„ ì‚­ì œí•©ë‹ˆë‹¤...")
                shutil.rmtree(self.backup_path)

            print(f"ğŸ“ ë°±ì—… ìƒì„± ì¤‘: {self.db_path} â†’ {self.backup_path}")
            shutil.copytree(self.db_path, self.backup_path)
            print(f"âœ… ë°±ì—… ì™„ë£Œ!")
            return True

        except Exception as e:
            print(f"âŒ ë°±ì—… ì‹¤íŒ¨: {e}")
            return False

    def migrate_data(self, dry_run: bool = False) -> Dict[str, Any]:
        """
        ë°ì´í„° ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰

        Args:
            dry_run: Trueë©´ ì‹¤ì œ ì‹¤í–‰í•˜ì§€ ì•Šê³  ê³„íšë§Œ ì¶œë ¥

        Returns:
            ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼
        """
        print("\n" + "=" * 60)
        if dry_run:
            print("ğŸ” ë§ˆì´ê·¸ë ˆì´ì…˜ ê³„íš (Dry-run)")
        else:
            print("ğŸš€ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰")
        print("=" * 60)

        try:
            collection = self.client.get_collection("file_chunks")
            total_count = collection.count()

            print(f"ğŸ“Š ì´ {total_count}ê°œ ì²­í¬ ë§ˆì´ê·¸ë ˆì´ì…˜ ì˜ˆì •")

            # ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            all_data = collection.get(include=["metadatas", "documents", "embeddings"])

            migrated_count = 0
            skipped_count = 0

            for idx, (chunk_id, metadata, document) in enumerate(
                zip(all_data['ids'], all_data['metadatas'], all_data['documents'])
            ):
                # ì´ë¯¸ UnifiedChunk í˜•ì‹ì¸ì§€ í™•ì¸
                if "data_source" in metadata:
                    skipped_count += 1
                    if idx < 3:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                        print(f"â­ï¸  [{idx+1}/{total_count}] {chunk_id}: ì´ë¯¸ UnifiedChunk í˜•ì‹ (ìŠ¤í‚µ)")
                    continue

                if not dry_run:
                    # FileChunk ë©”íƒ€ë°ì´í„°ë¥¼ UnifiedChunk í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    new_metadata = self._convert_metadata_to_unified(metadata, document)

                    # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ í›„ ìƒˆ í˜•ì‹ìœ¼ë¡œ ì €ì¥
                    collection.delete(ids=[chunk_id])
                    collection.add(
                        ids=[chunk_id],
                        documents=[document],
                        metadatas=[new_metadata]
                    )

                migrated_count += 1

                # ì§„í–‰ìƒí™© ì¶œë ¥ (ì²˜ìŒ 10ê°œ + 10ê°œë§ˆë‹¤)
                if idx < 10 or (idx + 1) % 10 == 0:
                    print(f"âœ… [{idx+1}/{total_count}] {chunk_id}: ë§ˆì´ê·¸ë ˆì´ì…˜ {'ì˜ˆì •' if dry_run else 'ì™„ë£Œ'}")

            print(f"\nğŸ“Š ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼:")
            print(f"   âœ… ë§ˆì´ê·¸ë ˆì´ì…˜: {migrated_count}ê°œ")
            print(f"   â­ï¸  ìŠ¤í‚µ: {skipped_count}ê°œ")
            print(f"   ğŸ“¦ ì´ê³„: {total_count}ê°œ")

            return {
                "success": True,
                "migrated_count": migrated_count,
                "skipped_count": skipped_count,
                "total_count": total_count
            }

        except Exception as e:
            print(f"âŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return {
                "success": False,
                "error": str(e)
            }

    def _convert_metadata_to_unified(self, old_metadata: Dict[str, Any], document: str) -> Dict[str, Any]:
        """
        FileChunk ë©”íƒ€ë°ì´í„°ë¥¼ UnifiedChunk ë©”íƒ€ë°ì´í„°ë¡œ ë³€í™˜

        Args:
            old_metadata: ê¸°ì¡´ FileChunk ë©”íƒ€ë°ì´í„°
            document: ë¬¸ì„œ í…ìŠ¤íŠ¸

        Returns:
            UnifiedChunk ë©”íƒ€ë°ì´í„°
        """
        now = datetime.now().isoformat()

        # file_metadata êµ¬ì„±
        file_metadata = {
            "file_name": old_metadata.get("file_name", ""),
            "file_hash": old_metadata.get("file_hash", ""),
            "file_type": old_metadata.get("file_type", ""),
            "file_size": len(document.encode('utf-8')),
            "architecture": old_metadata.get("architecture", ""),
            "processing_method": old_metadata.get("processing_method", ""),
            "vision_analysis": old_metadata.get("vision_analysis", False),
            "processing_duration": old_metadata.get("processing_duration", 0.0),
            "section_title": old_metadata.get("section_title", ""),
            "page_number": old_metadata.get("page_number", 1),
            "element_count": old_metadata.get("element_count", 0),
            "elements": []  # elementsëŠ” í¬ê¸° ì œí•œìœ¼ë¡œ ì €ì¥í•˜ì§€ ì•ŠìŒ
        }

        # ìƒˆ ë©”íƒ€ë°ì´í„° êµ¬ì„± (UnifiedChunk í˜•ì‹)
        new_metadata = {
            # ê³µí†µ í•„ë“œ
            "chunk_id": old_metadata.get("chunk_id", ""),
            "data_source": "file",  # í˜„ì¬ëŠ” í•­ìƒ "file"
            "created_at": old_metadata.get("created_at", now),
            "updated_at": now,

            # ì£¼ìš” í•„ë“œ (ê²€ìƒ‰ í¸ì˜ì„±)
            "file_name": file_metadata["file_name"],
            "file_type": file_metadata["file_type"],
            "file_hash": file_metadata["file_hash"],
            "page_number": file_metadata["page_number"],
            "architecture": file_metadata["architecture"],
            "processing_method": file_metadata["processing_method"],
            "vision_analysis": file_metadata["vision_analysis"],
            "section_title": file_metadata["section_title"],
            "element_count": file_metadata["element_count"],

            # ì „ì²´ file_metadata (JSON)
            "file_metadata_json": json.dumps(file_metadata, ensure_ascii=False),

            # jira_metadata (í˜„ì¬ëŠ” None)
            "jira_metadata_json": None
        }

        return new_metadata

    def rollback(self) -> bool:
        """
        ë°±ì—…ì—ì„œ ë³µì› (ë¡¤ë°±)

        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        print("\n" + "=" * 60)
        print("ğŸ”„ ë°±ì—…ì—ì„œ ë³µì› (ë¡¤ë°±)")
        print("=" * 60)

        # ê°€ì¥ ìµœê·¼ ë°±ì—… ì°¾ê¸°
        backup_dirs = [
            d for d in os.listdir(os.path.dirname(self.db_path) or ".")
            if d.startswith("vector_db_backup_")
        ]

        if not backup_dirs:
            print("âŒ ë°±ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False

        latest_backup = sorted(backup_dirs)[-1]
        backup_full_path = os.path.join(os.path.dirname(self.db_path) or ".", latest_backup)

        print(f"ğŸ“ ë³µì›í•  ë°±ì—…: {backup_full_path}")

        try:
            # í˜„ì¬ DB ì‚­ì œ
            if os.path.exists(self.db_path):
                print(f"ğŸ—‘ï¸  í˜„ì¬ DB ì‚­ì œ: {self.db_path}")
                shutil.rmtree(self.db_path)

            # ë°±ì—… ë³µì›
            print(f"ğŸ“¦ ë°±ì—… ë³µì› ì¤‘...")
            shutil.copytree(backup_full_path, self.db_path)
            print(f"âœ… ë³µì› ì™„ë£Œ!")
            return True

        except Exception as e:
            print(f"âŒ ë¡¤ë°± ì‹¤íŒ¨: {e}")
            return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="FileChunk â†’ UnifiedChunk ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤ì œ ì‹¤í–‰í•˜ì§€ ì•Šê³  ê³„íšë§Œ ì¶œë ¥"
    )
    parser.add_argument(
        "--backup",
        action="store_true",
        help="ë§ˆì´ê·¸ë ˆì´ì…˜ ì „ ë°±ì—… ìƒì„±"
    )
    parser.add_argument(
        "--execute",
        action="store_true",
        help="ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰"
    )
    parser.add_argument(
        "--rollback",
        action="store_true",
        help="ë°±ì—…ì—ì„œ ë³µì› (ë¡¤ë°±)"
    )
    parser.add_argument(
        "--db-path",
        default="./vector_db",
        help="ChromaDB ê²½ë¡œ (ê¸°ë³¸ê°’: ./vector_db)"
    )

    args = parser.parse_args()

    print("\n" + "=" * 60)
    print("ğŸ¤– FileChunk â†’ UnifiedChunk ë§ˆì´ê·¸ë ˆì´ì…˜ ë„êµ¬")
    print("=" * 60)

    migrator = FileChunkMigrator(db_path=args.db_path)

    # ë¡¤ë°± ìš”ì²­
    if args.rollback:
        success = migrator.rollback()
        sys.exit(0 if success else 1)

    # í˜„ì¬ ë°ì´í„° ë¶„ì„
    analysis = migrator.analyze_current_data()

    if not analysis.get("needs_migration", False):
        print("\nâœ… ë§ˆì´ê·¸ë ˆì´ì…˜ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.")
        sys.exit(0)

    # Dry-run
    if args.dry_run:
        migrator.migrate_data(dry_run=True)
        print("\nğŸ’¡ ì‹¤ì œ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì‹¤í–‰í•˜ë ¤ë©´ --backup --execute ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        sys.exit(0)

    # ì‹¤í–‰ (ë°±ì—… ì˜µì…˜ í™•ì¸)
    if args.execute:
        if args.backup:
            if not migrator.backup_database():
                print("\nâŒ ë°±ì—… ì‹¤íŒ¨ë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                sys.exit(1)

        result = migrator.migrate_data(dry_run=False)

        if result["success"]:
            print("\nâœ… ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
            sys.exit(0)
        else:
            print("\nâŒ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨!")
            if args.backup:
                print("ğŸ’¡ --rollback ì˜µì…˜ìœ¼ë¡œ ë°±ì—…ì—ì„œ ë³µì›í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            sys.exit(1)

    # ì˜µì…˜ì´ ì—†ìœ¼ë©´ ì‚¬ìš©ë²• ì¶œë ¥
    else:
        print("\nğŸ’¡ ì‚¬ìš©ë²•:")
        print("   1. Dry-run (ê³„íš í™•ì¸):    python scripts/migrate_filechunk_to_unified.py --dry-run")
        print("   2. ë°±ì—… í›„ ì‹¤í–‰:           python scripts/migrate_filechunk_to_unified.py --backup --execute")
        print("   3. ë¡¤ë°±:                   python scripts/migrate_filechunk_to_unified.py --rollback")
        sys.exit(0)


if __name__ == "__main__":
    main()
