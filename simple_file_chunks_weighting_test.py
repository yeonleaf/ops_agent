#!/usr/bin/env python3
"""
file_chunks ë°ì´í„°ë¥¼ í™œìš©í•œ ê°„ë‹¨í•œ ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸
pandas ì˜ì¡´ì„± ì—†ì´ ì‹¤í–‰ ê°€ëŠ¥
"""

import os
import sys
import chromadb
from chromadb.config import Settings
from typing import Dict, List, Any
import numpy as np

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from intelligent_chunk_weighting import IntelligentChunkWeighting

class FileChunksWeightingTest:
    """file_chunks ê¸°ë°˜ ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.client = chromadb.PersistentClient(
            path='./vector_db',
            settings=Settings(anonymized_telemetry=False, allow_reset=True)
        )
        self.collection = self.client.get_collection("file_chunks")
        self.weighting_system = IntelligentChunkWeighting()
        print(f"ğŸ“Š file_chunks ì»¬ë ‰ì…˜: {self.collection.count()}ê°œ ë¬¸ì„œ")

    def search_with_weighting(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """ê°€ì¤‘ì¹˜ ì ìš© ê²€ìƒ‰"""
        try:
            # 1. ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰
            basic_results = self.collection.query(
                query_texts=[query],
                n_results=n_results
            )

            if not basic_results['ids'][0]:
                return []

            # 2. ê²€ìƒ‰ ê²°ê³¼ ë³€í™˜
            search_results = []
            for i in range(len(basic_results['ids'][0])):
                metadata = basic_results['metadatas'][0][i] if basic_results['metadatas'][0] else {}
                content = basic_results['documents'][0][i] if basic_results['documents'][0] else ""
                distance = basic_results['distances'][0][i] if basic_results['distances'][0] else 1.0

                cosine_score = max(0.0, 1.0 - distance)
                chunk_type = self._estimate_chunk_type(content)

                search_result = {
                    'id': basic_results['ids'][0][i],
                    'content': content,
                    'chunk_type': chunk_type,
                    'cosine_score': cosine_score,
                    'embedding': [],
                    'metadata': metadata
                }
                search_results.append(search_result)

            # 3. ê°€ì¤‘ì¹˜ ì ìš©
            mock_query_embedding = np.random.normal(0, 1, 384).tolist()
            weighted_results = self.weighting_system.apply_weighted_scoring(
                search_results, mock_query_embedding
            )

            return weighted_results

        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def _estimate_chunk_type(self, content: str) -> str:
        """ë‚´ìš© ê¸°ë°˜ chunk_type ì¶”ì •"""
        content_lower = content.lower()

        if any(pattern in content_lower for pattern in ['ì œëª©:', 'title:', 'ì´ìŠˆ í‚¤:']):
            return 'title'
        elif any(pattern in content_lower for pattern in ['ìš”ì•½:', 'summary:']):
            return 'summary'
        elif any(pattern in content_lower for pattern in ['ì„¤ëª…:', 'description:']):
            return 'description'
        elif any(pattern in content_lower for pattern in ['ëŒ“ê¸€:', 'comment:']):
            return 'comment'
        elif len(content.strip()) < 30:
            return 'title'
        elif len(content.strip()) < 100:
            return 'summary'
        elif len(content.strip()) > 1000:
            return 'body'
        else:
            return 'description'

def run_weighting_test():
    """ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ¯ file_chunks ê°€ì¤‘ì¹˜ íš¨ê³¼ ê²€ì¦ í…ŒìŠ¤íŠ¸")
    print("="*80)

    test_questions = [
        "ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ê°œì„  ë°©ì•ˆ",
        "ì„œë²„ ì ‘ì† ë¬¸ì œ í•´ê²°",
        "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜",
        "ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™”",
        "í”„ë¡œì íŠ¸ ìš”ì•½ ì •ë³´"
    ]

    tester = FileChunksWeightingTest()

    for i, question in enumerate(test_questions, 1):
        print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}: {question}")
        print("-" * 60)

        # ê°€ì¤‘ì¹˜ ì ìš© ê²€ìƒ‰
        weighted_results = tester.search_with_weighting(question)

        if weighted_results:
            print(f"âœ… {len(weighted_results)}ê°œ ê²°ê³¼")

            # ìƒìœ„ 3ê°œ ê²°ê³¼ í‘œì‹œ
            for j, result in enumerate(weighted_results[:3], 1):
                chunk_type = result.chunk_type
                original_score = result.cosine_score
                weighted_score = result.weighted_score
                weight = result.weight
                improvement = weighted_score - original_score

                print(f"  {j}. [{chunk_type.upper()}] (ê°€ì¤‘ì¹˜: {weight:.1f})")
                print(f"     ì ìˆ˜ ë³€í™”: {original_score:.4f} â†’ {weighted_score:.4f} ({improvement:+.4f})")

                # ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
                content_preview = result.content[:80].replace('\n', ' ') + "..."
                print(f"     ë‚´ìš©: {content_preview}")
                print()

            # í†µê³„ ìš”ì•½
            chunk_type_stats = {}
            for result in weighted_results:
                chunk_type = result.chunk_type
                if chunk_type not in chunk_type_stats:
                    chunk_type_stats[chunk_type] = []
                improvement = result.weighted_score - result.cosine_score
                chunk_type_stats[chunk_type].append(improvement)

            print("ğŸ“Š ê°€ì¤‘ì¹˜ íš¨ê³¼ í†µê³„:")
            for chunk_type, improvements in chunk_type_stats.items():
                avg_improvement = np.mean(improvements)
                count = len(improvements)
                weight = tester.weighting_system.config.get_weight(chunk_type)
                print(f"  - {chunk_type}: {count}ê°œ, ê°€ì¤‘ì¹˜ {weight:.1f}, í‰ê·  ì ìˆ˜ ë³€í™” {avg_improvement:+.4f}")

        else:
            print("âŒ ê²°ê³¼ ì—†ìŒ")

if __name__ == "__main__":
    run_weighting_test()