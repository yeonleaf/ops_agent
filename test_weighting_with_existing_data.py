#!/usr/bin/env python3
"""
ê¸°ì¡´ file_chunks ë°ì´í„°ë¥¼ í™œìš©í•œ ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸
jira_multi_vector_chunksê°€ ë¹„ì–´ìˆì–´ì„œ file_chunksë¥¼ í™œìš©
"""

import chromadb
from chromadb.config import Settings
from intelligent_chunk_weighting import IntelligentChunkWeighting, ChunkData, SearchResult
from setup_korean_embedding import KoreanEmbeddingFunction
import numpy as np
from typing import List, Dict, Any

def test_weighting_with_file_chunks():
    """ê¸°ì¡´ file_chunks ë°ì´í„°ë¥¼ í™œìš©í•œ ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ ê¸°ì¡´ file_chunks ë°ì´í„°ë¡œ ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("="*80)

    try:
        # ChromaDB ì—°ê²°
        client = chromadb.PersistentClient(
            path='./vector_db',
            settings=Settings(anonymized_telemetry=False, allow_reset=True)
        )

        # file_chunks ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
        collection = client.get_collection("file_chunks")
        print(f"ğŸ“Š file_chunks ì»¬ë ‰ì…˜: {collection.count()}ê°œ ë¬¸ì„œ")

        # í•œêµ­ì–´ ì„ë² ë”© í•¨ìˆ˜ ì´ˆê¸°í™”
        korean_embedding = KoreanEmbeddingFunction()

        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
        test_queries = [
            "ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ê°œì„  ë°©ì•ˆ",
            "ì„œë²„ ì ‘ì† ë¬¸ì œ í•´ê²°",
            "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜",
            "ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ì„¤ê³„"
        ]

        for i, query in enumerate(test_queries, 1):
            print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}: {query}")
            print("-" * 60)

            # 1. ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰ (ê°€ì¤‘ì¹˜ ì—†ìŒ)
            query_embedding = korean_embedding([query])
            basic_results = collection.query(
                query_embeddings=query_embedding,
                n_results=10
            )

            if not basic_results['ids'][0]:
                print("âŒ ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                continue

            print(f"âœ… ê¸°ë³¸ ê²€ìƒ‰: {len(basic_results['ids'][0])}ê°œ ê²°ê³¼")

            # 2. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            search_results = []
            for j in range(len(basic_results['ids'][0])):
                # ë©”íƒ€ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ
                metadata = basic_results['metadatas'][0][j] if basic_results['metadatas'][0] else {}
                content = basic_results['documents'][0][j] if basic_results['documents'][0] else ""
                distance = basic_results['distances'][0][j] if basic_results['distances'][0] else 1.0

                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ê±°ë¦¬ -> ìœ ì‚¬ë„)
                cosine_score = max(0.0, 1.0 - distance)

                # chunk_type ì¶”ì • (íŒŒì¼ ê¸°ë°˜)
                chunk_type = estimate_chunk_type_from_metadata(metadata, content)

                search_result = {
                    'id': basic_results['ids'][0][j],
                    'content': content,
                    'chunk_type': chunk_type,
                    'cosine_score': cosine_score,
                    'embedding': [],  # ì„ë² ë”©ì€ ì¬ê³„ì‚°í•˜ì§€ ì•ŠìŒ
                    'metadata': {
                        **metadata,
                        'estimated_chunk_type': chunk_type,
                        'original_distance': distance
                    }
                }
                search_results.append(search_result)

            # 3. ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ ì ìš©
            weighting_system = IntelligentChunkWeighting()
            weighted_results = weighting_system.apply_weighted_scoring(
                search_results, query_embedding[0]
            )

            # 4. ê²°ê³¼ ë¹„êµ ì¶œë ¥
            print("\nğŸ“‹ Before weighting (ê¸°ë³¸ ìœ ì‚¬ë„ ìˆœ):")
            for j, result in enumerate(search_results[:5], 1):
                chunk_type = result['chunk_type']
                score = result['cosine_score']
                weight = weighting_system.config.get_weight(chunk_type)
                print(f"  {j}. {chunk_type}: {score:.4f} (ê°€ì¤‘ì¹˜: {weight:.1f})")

            print("\nğŸ“‹ After weighting (ê°€ì¤‘ì¹˜ ì ìš© ìˆœ):")
            for j, result in enumerate(weighted_results[:5], 1):
                print(f"  {j}. {result.chunk_type}: {result.cosine_score:.4f} â†’ {result.weighted_score:.4f} "
                      f"(ê°€ì¤‘ì¹˜: {result.weight:.1f})")

            # 5. ìˆœìœ„ ë³€í™” ë¶„ì„
            rank_changes = analyze_rank_changes(search_results, weighted_results)
            if rank_changes:
                print("\nğŸ“ˆ ì£¼ìš” ìˆœìœ„ ë³€í™”:")
                for change in rank_changes[:3]:
                    print(f"  {change}")

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

def estimate_chunk_type_from_metadata(metadata: Dict[str, Any], content: str) -> str:
    """ë©”íƒ€ë°ì´í„°ì™€ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ chunk_type ì¶”ì •"""

    # íŒŒì¼ëª…ì—ì„œ íƒ€ì… ì¶”ì •
    file_name = metadata.get('file_name', '').lower()
    section_title = metadata.get('section_title', '').lower()

    # ì„¹ì…˜ ì œëª©ì´ë‚˜ íŒŒì¼ëª…ì—ì„œ íƒ€ì… íŒë‹¨
    if any(keyword in section_title for keyword in ['title', 'ì œëª©', 'heading']):
        return 'title'
    elif any(keyword in section_title for keyword in ['summary', 'ìš”ì•½', 'ê°œìš”']):
        return 'summary'
    elif any(keyword in section_title for keyword in ['description', 'ì„¤ëª…', 'ìƒì„¸']):
        return 'description'
    elif any(keyword in section_title for keyword in ['comment', 'ëŒ“ê¸€', 'ì˜ê²¬']):
        return 'comment'
    elif any(keyword in section_title for keyword in ['header', 'í—¤ë”']):
        return 'header'

    # íŒŒì¼ í™•ì¥ìì—ì„œ íƒ€ì… ì¶”ì •
    elif any(ext in file_name for ext in ['.pdf', '.doc', '.txt']):
        # ë‚´ìš© ê¸¸ì´ë¡œ íŒë‹¨
        if len(content) < 50:
            return 'title'
        elif len(content) < 200:
            return 'summary'
        elif len(content) > 1000:
            return 'body'
        else:
            return 'description'

    # ê¸°íƒ€ ë©”íƒ€ë°ì´í„° í™œìš©
    elif metadata.get('element_count', 0) == 1:
        return 'title'  # ë‹¨ì¼ ìš”ì†ŒëŠ” ì œëª©ìœ¼ë¡œ ì¶”ì •

    # ê¸°ë³¸ê°’
    return 'body'

def analyze_rank_changes(basic_results: List[Dict], weighted_results: List[SearchResult]) -> List[str]:
    """ìˆœìœ„ ë³€í™” ë¶„ì„"""
    changes = []

    # ID ê¸°ë°˜ ìˆœìœ„ ë§¤í•‘
    basic_ranks = {result['id']: i+1 for i, result in enumerate(basic_results)}
    weighted_ranks = {result.id: i+1 for i, result in enumerate(weighted_results)}

    for doc_id in basic_ranks:
        if doc_id in weighted_ranks:
            basic_rank = basic_ranks[doc_id]
            weighted_rank = weighted_ranks[doc_id]
            rank_change = basic_rank - weighted_rank

            if abs(rank_change) >= 2:  # 2ìˆœìœ„ ì´ìƒ ë³€í™”ë§Œ í‘œì‹œ
                direction = "ìƒìŠ¹" if rank_change > 0 else "í•˜ë½"
                chunk_type = next((r.chunk_type for r in weighted_results if r.id == doc_id), "unknown")
                changes.append(f"{doc_id[:8]}... ({chunk_type}): {basic_rank} â†’ {weighted_rank} ({direction})")

    return changes

if __name__ == "__main__":
    test_weighting_with_file_chunks()