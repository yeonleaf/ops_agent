#!/usr/bin/env python3
"""
ê¸°ì¡´ file_chunks ë°ì´í„°ë¥¼ í™œìš©í•œ ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (í˜¸í™˜ì„± ë²„ì „)
ê¸°ì¡´ 384ì°¨ì› ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
"""

import chromadb
from chromadb.config import Settings
from intelligent_chunk_weighting import IntelligentChunkWeighting, SearchResult
import numpy as np
from typing import List, Dict, Any

def test_weighting_with_compatible_embeddings():
    """ê¸°ì¡´ file_chunksì™€ í˜¸í™˜ë˜ëŠ” ê°€ì¤‘ì¹˜ í…ŒìŠ¤íŠ¸"""
    print("ğŸš€ ê¸°ì¡´ file_chunks ë°ì´í„°ë¡œ ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ (í˜¸í™˜ì„± ëª¨ë“œ)")
    print("="*80)

    try:
        # ChromaDB ì—°ê²°
        client = chromadb.PersistentClient(
            path='./vector_db',
            settings=Settings(anonymized_telemetry=False, allow_reset=True)
        )

        # file_chunks ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° (ê¸°ì¡´ ì„ë² ë”© í•¨ìˆ˜ ì‚¬ìš©)
        collection = client.get_collection("file_chunks")
        print(f"ğŸ“Š file_chunks ì»¬ë ‰ì…˜: {collection.count()}ê°œ ë¬¸ì„œ")

        # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
        test_queries = [
            "ì‚¬ìš©ì ì¸í„°í˜ì´ìŠ¤ ê°œì„  ë°©ì•ˆ",
            "ì„œë²„ ì ‘ì† ë¬¸ì œ í•´ê²°",
            "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜",
            "ì‹œìŠ¤í…œ ì„±ëŠ¥ ìµœì í™”"
        ]

        for i, query in enumerate(test_queries, 1):
            print(f"\nğŸ“ í…ŒìŠ¤íŠ¸ {i}: {query}")
            print("-" * 60)

            # 1. ê¸°ë³¸ ë²¡í„° ê²€ìƒ‰ (ê¸°ì¡´ ì„ë² ë”© í•¨ìˆ˜ ì‚¬ìš©)
            basic_results = collection.query(
                query_texts=[query],
                n_results=10
            )

            if not basic_results['ids'][0]:
                print("âŒ ê¸°ë³¸ ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
                continue

            print(f"âœ… ê¸°ë³¸ ê²€ìƒ‰: {len(basic_results['ids'][0])}ê°œ ê²°ê³¼")

            # 2. ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            search_results = []
            for j in range(len(basic_results['ids'][0])):
                # ë©”íƒ€ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ
                metadata = basic_results['metadatas'][0][j] if basic_results['metadatas'][0] else {}
                content = basic_results['documents'][0][j] if basic_results['documents'][0] else ""
                distance = basic_results['distances'][0][j] if basic_results['distances'][0] else 1.0

                # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¡œ ë³€í™˜ (ê±°ë¦¬ -> ìœ ì‚¬ë„)
                cosine_score = max(0.0, 1.0 - distance)

                # chunk_type ì¶”ì • (íŒŒì¼ ê¸°ë°˜)
                chunk_type = estimate_chunk_type_from_content_and_metadata(metadata, content)

                search_result = {
                    'id': basic_results['ids'][0][j],
                    'content': content,
                    'chunk_type': chunk_type,
                    'cosine_score': cosine_score,
                    'embedding': [],  # ì„ë² ë”©ì€ ì¬ê³„ì‚°í•˜ì§€ ì•ŠìŒ
                    'metadata': {
                        **metadata,
                        'estimated_chunk_type': chunk_type,
                        'original_distance': distance
                    }
                }
                search_results.append(search_result)

            # 3. ê°€ì¤‘ì¹˜ ì‹œìŠ¤í…œ ì ìš© (Mock ì¿¼ë¦¬ ì„ë² ë”© ì‚¬ìš©)
            weighting_system = IntelligentChunkWeighting()

            # Mock ì¿¼ë¦¬ ì„ë² ë”© (384ì°¨ì›ìœ¼ë¡œ ë§ì¶¤)
            mock_query_embedding = np.random.normal(0, 1, 384).tolist()

            weighted_results = weighting_system.apply_weighted_scoring(
                search_results, mock_query_embedding
            )

            # 4. ê²°ê³¼ ë¹„êµ ì¶œë ¥
            print("\nğŸ“‹ Before weighting (ê¸°ë³¸ ìœ ì‚¬ë„ ìˆœ):")
            for j, result in enumerate(search_results[:5], 1):
                chunk_type = result['chunk_type']
                score = result['cosine_score']
                weight = weighting_system.config.get_weight(chunk_type)
                content_preview = result['content'][:50].replace('\n', ' ') + "..."
                print(f"  {j}. [{chunk_type}] {score:.4f} (ê°€ì¤‘ì¹˜: {weight:.1f})")
                print(f"      ë‚´ìš©: {content_preview}")

            print("\nğŸ“‹ After weighting (ê°€ì¤‘ì¹˜ ì ìš© ìˆœ):")
            for j, result in enumerate(weighted_results[:5], 1):
                content_preview = result.content[:50].replace('\n', ' ') + "..."
                print(f"  {j}. [{result.chunk_type}] {result.cosine_score:.4f} â†’ {result.weighted_score:.4f} "
                      f"(ê°€ì¤‘ì¹˜: {result.weight:.1f})")
                print(f"      ë‚´ìš©: {content_preview}")

            # 5. ìˆœìœ„ ë³€í™” ë¶„ì„
            rank_changes = analyze_rank_changes(search_results, weighted_results)
            if rank_changes:
                print("\nğŸ“ˆ ì£¼ìš” ìˆœìœ„ ë³€í™”:")
                for change in rank_changes[:3]:
                    print(f"  {change}")
            else:
                print("\nğŸ“ˆ ì£¼ìš” ìˆœìœ„ ë³€í™”: ì—†ìŒ")

            # 6. ê°€ì¤‘ì¹˜ íš¨ê³¼ í†µê³„
            print(f"\nğŸ“Š ê°€ì¤‘ì¹˜ íš¨ê³¼ í†µê³„:")
            chunk_type_stats = {}
            for result in weighted_results:
                chunk_type = result.chunk_type
                if chunk_type not in chunk_type_stats:
                    chunk_type_stats[chunk_type] = []
                improvement = result.weighted_score - result.cosine_score
                chunk_type_stats[chunk_type].append(improvement)

            for chunk_type, improvements in chunk_type_stats.items():
                avg_improvement = np.mean(improvements)
                count = len(improvements)
                weight = weighting_system.config.get_weight(chunk_type)
                print(f"  - {chunk_type}: {count}ê°œ, ê°€ì¤‘ì¹˜ {weight:.1f}, í‰ê·  ì ìˆ˜ ë³€í™” {avg_improvement:+.4f}")

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

def estimate_chunk_type_from_content_and_metadata(metadata: Dict[str, Any], content: str) -> str:
    """ë©”íƒ€ë°ì´í„°ì™€ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ chunk_type ì¶”ì • (ê°œì„ ëœ ë²„ì „)"""

    # íŒŒì¼ëª…ì—ì„œ íƒ€ì… ì¶”ì •
    file_name = metadata.get('file_name', '').lower()

    # ë‚´ìš© ê¸°ë°˜ ë¶„ì„
    content_lower = content.lower()
    content_lines = content.split('\n')

    # 1. êµ¬ì¡°ì  íŒ¨í„´ ë¶„ì„
    if any(pattern in content_lower for pattern in ['ì œëª©:', 'title:', 'ì´ìŠˆ í‚¤:', 'issue key:']):
        return 'title'
    elif any(pattern in content_lower for pattern in ['ìš”ì•½:', 'summary:', 'ê°œìš”:']):
        return 'summary'
    elif any(pattern in content_lower for pattern in ['ì„¤ëª…:', 'description:', 'ìƒì„¸:', 'ë‚´ìš©:']):
        return 'description'
    elif any(pattern in content_lower for pattern in ['ëŒ“ê¸€:', 'comment:', 'ì˜ê²¬:', 'í”¼ë“œë°±:']):
        return 'comment'
    elif any(pattern in content_lower for pattern in ['í—¤ë”:', 'header:', 'ì œëª©']):
        return 'header'

    # 2. ë‚´ìš© ê¸¸ì´ ê¸°ë°˜ ë¶„ì„
    elif len(content.strip()) < 30:
        return 'title'  # ë§¤ìš° ì§§ì€ í…ìŠ¤íŠ¸
    elif len(content.strip()) < 100:
        return 'summary'  # ì§§ì€ í…ìŠ¤íŠ¸
    elif len(content.strip()) > 1000:
        return 'body'  # ê¸´ í…ìŠ¤íŠ¸

    # 3. íŒŒì¼ í™•ì¥ì ê¸°ë°˜ ë¶„ì„
    elif any(ext in file_name for ext in ['.pdf', '.doc', '.docx']):
        # ë¬¸ì„œ íŒŒì¼ì¸ ê²½ìš° ë‚´ìš© íŒ¨í„´ìœ¼ë¡œ ì„¸ë¶„í™”
        if len(content_lines) == 1:
            return 'title'
        elif len(content_lines) <= 3:
            return 'summary'
        else:
            return 'description'

    # 4. ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì¶”ê°€ ë¶„ì„
    elif metadata.get('element_count', 0) == 1:
        return 'title'  # ë‹¨ì¼ ìš”ì†Œ
    elif 'vision_analysis' in metadata:
        return 'description'  # ì‹œê°ì  ë¶„ì„ì´ í¬í•¨ëœ ê²½ìš°

    # 5. ê¸°ë³¸ê°’
    else:
        return 'body'

def analyze_rank_changes(basic_results: List[Dict], weighted_results: List[SearchResult]) -> List[str]:
    """ìˆœìœ„ ë³€í™” ë¶„ì„"""
    changes = []

    # ID ê¸°ë°˜ ìˆœìœ„ ë§¤í•‘
    basic_ranks = {result['id']: i+1 for i, result in enumerate(basic_results)}
    weighted_ranks = {result.id: i+1 for i, result in enumerate(weighted_results)}

    for doc_id in basic_ranks:
        if doc_id in weighted_ranks:
            basic_rank = basic_ranks[doc_id]
            weighted_rank = weighted_ranks[doc_id]
            rank_change = basic_rank - weighted_rank

            if abs(rank_change) >= 2:  # 2ìˆœìœ„ ì´ìƒ ë³€í™”ë§Œ í‘œì‹œ
                direction = "â¬†ï¸ìƒìŠ¹" if rank_change > 0 else "â¬‡ï¸í•˜ë½"
                chunk_type = next((r.chunk_type for r in weighted_results if r.id == doc_id), "unknown")
                changes.append(f"{doc_id[:12]}... ({chunk_type}): {basic_rank} â†’ {weighted_rank} {direction}")

    return changes

if __name__ == "__main__":
    test_weighting_with_compatible_embeddings()