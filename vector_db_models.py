#!/usr/bin/env python3
"""
Vector DBìš© Mail ëª¨ë¸ ë° ê´€ë¦¬ì
"""

import os
from dotenv import load_dotenv
from dataclasses import dataclass, asdict
from typing import List, Optional, Dict, Any
import uuid
from datetime import datetime
import chromadb
from chromadb.config import Settings
import json

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

@dataclass
class Mail:
    """ë©”ì¼ ëª¨ë¸ - Vector DB Collectionìš©"""
    message_id: str  # PK - ì›ë³¸ ë©”ì¼ ID (ì—°ê²° í‚¤)
    original_content: str  # HTML/Text ì›ë³¸ ë‚´ìš©
    refined_content: str  # ì¶”ì¶œëœ í•µì‹¬ ë‚´ìš©
    sender: str  # ë³´ë‚¸ ì‚¬ëŒ
    status: str  # acceptable, unacceptable ë“±
    subject: str  # ë©”ì¼ ì œëª©
    received_datetime: str  # ìˆ˜ì‹  ì‹œê°„
    content_type: str  # html, text
    has_attachment: bool  # ì²¨ë¶€íŒŒì¼ ì—¬ë¶€
    extraction_method: str  # ì¶”ì¶œ ë°©ë²•
    content_summary: str  # ë‚´ìš© ìš”ì•½
    key_points: List[str]  # í•µì‹¬ í¬ì¸íŠ¸
    created_at: str  # ìƒì„± ì‹œê°

@dataclass
class FileChunk:
    """íŒŒì¼ ì²­í¬ ëª¨ë¸ - Vector DB Collectionìš©"""
    chunk_id: str  # PK - ê³ ìœ  ì²­í¬ ID
    file_name: str  # ì›ë³¸ íŒŒì¼ëª…
    file_hash: str  # íŒŒì¼ í•´ì‹œê°’ (ì¤‘ë³µ ë°©ì§€ìš©)
    text_chunk: str  # ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë‚´ìš©
    architecture: str  # dual_path_hybrid ë˜ëŠ” simple_conversion
    processing_method: str  # ì²˜ë¦¬ ë°©ë²•
    vision_analysis: bool  # Vision ë¶„ì„ ì ìš© ì—¬ë¶€
    section_title: str  # ì„¹ì…˜ ì œëª©
    page_number: int  # í˜ì´ì§€/ìŠ¬ë¼ì´ë“œ ë²ˆí˜¸
    element_count: int  # ìš”ì†Œ ê°œìˆ˜
    file_type: str  # pptx, docx, pdf, xlsx, txt, md, csv, scds
    elements: List[Dict[str, Any]]  # ìš”ì†Œë³„ ìƒì„¸ ì •ë³´
    created_at: str  # ìƒì„± ì‹œê°
    file_size: int  # íŒŒì¼ í¬ê¸° (bytes)
    processing_duration: float  # ì²˜ë¦¬ ì†Œìš” ì‹œê°„ (ì´ˆ)

class VectorDBManager:
    """Vector DB ê´€ë¦¬ì - ChromaDB ì‚¬ìš©"""
    
    def __init__(self, db_path: str = "./vector_db"):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        # Vector DB í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±í•˜ê³  ê¶Œí•œ ì„¤ì •
        import os
        if not os.path.exists(db_path):
            os.makedirs(db_path, mode=0o755, exist_ok=True)
            print(f"âœ… Vector DB í´ë” ìƒì„± ë° ê¶Œí•œ ì„¤ì •: {db_path}")
        
        self.client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        self.collection_name = "mail_collection"
        self.collection = self._get_or_create_collection()
        
        # ChromaDB íŒŒì¼ ê¶Œí•œ ìë™ ì„¤ì •
        try:
            chroma_file = os.path.join(db_path, "chroma.sqlite3")
            if os.path.exists(chroma_file):
                os.chmod(chroma_file, 0o666)
                print(f"âœ… ChromaDB íŒŒì¼ ê¶Œí•œ ìë™ ì„¤ì •: {chroma_file}")
        except Exception as e:
            print(f"âš ï¸ ChromaDB íŒŒì¼ ê¶Œí•œ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _ensure_vector_db_permissions(self):
        """Vector DB í´ë” ë° íŒŒì¼ ê¶Œí•œì„ í™•ì‹¤íˆ ì„¤ì •"""
        try:
            import os
            
            # Vector DB í´ë” ê¶Œí•œ ì„¤ì • (ì´ˆê¸°í™” ì‹œ ì„¤ì •í•œ ê²½ë¡œ ì‚¬ìš©)
            vector_db_path = "./vector_db"
            if os.path.exists(vector_db_path):
                os.chmod(vector_db_path, 0o755)
            
            # ChromaDB ê´€ë ¨ ëª¨ë“  íŒŒì¼ ê¶Œí•œ ì„¤ì •
            for root, dirs, files in os.walk(vector_db_path):
                # í´ë” ê¶Œí•œ ì„¤ì •
                for dir_name in dirs:
                    dir_path = os.path.join(root, dir_name)
                    try:
                        os.chmod(dir_path, 0o755)
                    except Exception:
                        pass
                
                # íŒŒì¼ ê¶Œí•œ ì„¤ì •
                for file_name in files:
                    file_path = os.path.join(root, file_name)
                    try:
                        os.chmod(file_path, 0o666)
                    except Exception:
                        pass
            
            print(f"âœ… Vector DB ê¶Œí•œ ì¬ì„¤ì • ì™„ë£Œ: {vector_db_path}")
            
        except Exception as e:
            print(f"âš ï¸ Vector DB ê¶Œí•œ ì¬ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _get_or_create_collection(self):
        """ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.client.get_collection(name=self.collection_name)
        except Exception:
            return self.client.create_collection(
                name=self.collection_name,
                metadata={
                    "description": "Email messages for ticket generation",
                    "created_at": datetime.now().isoformat()
                }
            )
    
    def save_mail(self, mail: Mail) -> bool:
        """ë©”ì¼ì„ Vector DBì— ì €ì¥"""
        try:
            # ì €ì¥ ì „ Vector DB í´ë” ë° íŒŒì¼ ê¶Œí•œ ì¬ì„¤ì •
            self._ensure_vector_db_permissions()
            
            # ChromaDB íŒŒì¼ ê¶Œí•œ íŠ¹ë³„ í™•ì¸
            import os
            import stat
            chroma_file = os.path.join("./vector_db", "chroma.sqlite3")
            if os.path.exists(chroma_file):
                # í˜„ì¬ ê¶Œí•œ í™•ì¸
                current_perms = os.stat(chroma_file).st_mode
                if not (current_perms & stat.S_IWUSR):
                    print(f"âš ï¸ ChromaDB íŒŒì¼ì´ ì½ê¸° ì „ìš©ì…ë‹ˆë‹¤. ê¶Œí•œì„ ê°•ì œë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
                    # ê°•ì œë¡œ ì“°ê¸° ê¶Œí•œ ë¶€ì—¬
                    os.chmod(chroma_file, 0o666)
                    # ì†Œìœ ì ê¶Œí•œë„ í™•ì¸
                    current_perms = os.stat(chroma_file).st_mode
                    if not (current_perms & stat.S_IWUSR):
                        os.chmod(chroma_file, current_perms | stat.S_IWUSR | stat.S_IWGRP | stat.S_IWOTH)
                    print(f"âœ… ChromaDB íŒŒì¼ ê¶Œí•œ ê°•ì œ ì„¤ì • ì™„ë£Œ")
            
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜)
            metadata = {
                "sender": mail.sender,
                "status": mail.status,
                "subject": mail.subject,
                "received_datetime": mail.received_datetime.isoformat() if hasattr(mail.received_datetime, 'isoformat') else str(mail.received_datetime),
                "content_type": mail.content_type,
                "has_attachment": mail.has_attachment,
                "extraction_method": mail.extraction_method,
                "content_summary": mail.content_summary,
                "key_points": json.dumps(mail.key_points),
                "labels": json.dumps(mail.key_points),  # labels í•„ë“œ ì¶”ê°€
                "created_at": mail.created_at.isoformat() if hasattr(mail.created_at, 'isoformat') else str(mail.created_at),
                "original_content": mail.original_content,  # ì›ë³¸ ë‚´ìš© ì¶”ê°€
                "refined_content": mail.refined_content    # ì •ì œëœ ë‚´ìš© ì¶”ê°€
            }
            
            # ë¬¸ì„œ ë‚´ìš© (ì„ë² ë”©í•  í…ìŠ¤íŠ¸)
            document_text = f"""
            Subject: {mail.subject}
            Sender: {mail.sender}
            Content: {mail.refined_content}
            Summary: {mail.content_summary}
            Key Points: {', '.join(mail.key_points)}
            Labels: {', '.join(mail.key_points)}
            """
            
            # ChromaDBì— ì €ì¥
            self.collection.add(
                documents=[document_text],
                metadatas=[metadata],
                ids=[mail.message_id]
            )
            
            # ì €ì¥ í›„ ê¶Œí•œ ì¬í™•ì¸
            self._ensure_vector_db_permissions()
            
            return True
            
        except Exception as e:
            print(f"Vector DB ì €ì¥ ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¶Œí•œ ì¬ì„¤ì • ì‹œë„
            try:
                self._ensure_vector_db_permissions()
            except:
                pass
            return False
    
    def get_mail_by_id(self, message_id: str) -> Optional[Mail]:
        """ë©”ì‹œì§€ IDë¡œ ë©”ì¼ ì¡°íšŒ"""
        try:
            result = self.collection.get(
                ids=[message_id],
                include=["metadatas", "documents"]
            )
            
            if not result['ids']:
                return None
            
            metadata = result['metadatas'][0]
            document = result['documents'][0]
            
            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì§ì ‘ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ë” ì•ˆì •ì )
            refined_content = metadata.get("refined_content", "")
            original_content = metadata.get("original_content", "")
            content_summary = metadata.get("content_summary", "")
            key_points_str = metadata.get("key_points", "[]")
            labels_str = metadata.get("labels", "[]")  # labels í•„ë“œ ì¶”ê°€
            
            # key_pointsì™€ labelsê°€ JSON ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
            try:
                key_points = json.loads(key_points_str) if key_points_str else []
            except (json.JSONDecodeError, TypeError):
                key_points = []
                
            try:
                labels = json.loads(labels_str) if labels_str else []
                # labelsê°€ ìˆìœ¼ë©´ key_pointsì— ë³‘í•© (ë ˆì´ë¸” ìš°ì„ )
                if labels:
                    key_points = labels
            except (json.JSONDecodeError, TypeError):
                labels = []
            
            # ë©”íƒ€ë°ì´í„°ì— ë‚´ìš©ì´ ì—†ìœ¼ë©´ documentì—ì„œ íŒŒì‹± ì‹œë„
            if not refined_content:
                lines = document.strip().split('\n')
                for line in lines:
                    if line.startswith("Content:"):
                        refined_content = line.replace("Content:", "").strip()
                    elif line.startswith("Summary:"):
                        content_summary = line.replace("Summary:", "").strip()
                    elif line.startswith("Key Points:"):
                        key_points_str = line.replace("Key Points:", "").strip()
                        key_points = [kp.strip() for kp in key_points_str.split(',') if kp.strip()]
            
            return Mail(
                message_id=message_id,
                original_content=original_content,
                refined_content=refined_content,
                sender=metadata.get("sender", ""),
                status=metadata.get("status", "acceptable"),
                subject=metadata.get("subject", ""),
                received_datetime=metadata.get("received_datetime", ""),
                content_type=metadata.get("content_type", "text"),
                has_attachment=metadata.get("has_attachment", False),
                extraction_method=metadata.get("extraction_method", ""),
                content_summary=content_summary,
                key_points=key_points,
                created_at=metadata.get("created_at", "")
            )
            
        except Exception as e:
            print(f"Vector DB ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def search_similar_mails(self, query: str, n_results: int = 5) -> List[Mail]:
        """ìœ ì‚¬í•œ ë©”ì¼ ê²€ìƒ‰"""
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                include=["metadatas", "documents", "distances"]
            )
            
            mails = []
            for i, message_id in enumerate(results['ids'][0]):
                metadata = results['metadatas'][0][i]
                document = results['documents'][0][i]
                
                # ë©”íƒ€ë°ì´í„°ì—ì„œ ì§ì ‘ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ë” ì•ˆì •ì )
                refined_content = metadata.get("refined_content", "")
                original_content = metadata.get("original_content", "")
                content_summary = metadata.get("content_summary", "")
                key_points_str = metadata.get("key_points", "[]")
                
                # key_pointsê°€ JSON ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
                try:
                    key_points = json.loads(key_points_str) if key_points_str else []
                except (json.JSONDecodeError, TypeError):
                    key_points = []
                
                # ë©”íƒ€ë°ì´í„°ì— ë‚´ìš©ì´ ì—†ìœ¼ë©´ documentì—ì„œ íŒŒì‹± ì‹œë„
                if not refined_content:
                    lines = document.strip().split('\n')
                    for line in lines:
                        if line.startswith("Content:"):
                            refined_content = line.replace("Content:", "").strip()
                        elif line.startswith("Summary:"):
                            content_summary = line.replace("Summary:", "").strip()
                        elif line.startswith("Key Points:"):
                            key_points_str = line.replace("Key Points:", "").strip()
                            key_points = [kp.strip() for kp in key_points_str.split(',') if kp.strip()]
                
                mail = Mail(
                    message_id=message_id,
                    original_content=original_content,
                    refined_content=refined_content,
                    sender=metadata.get("sender", ""),
                    status=metadata.get("status", "acceptable"),
                    subject=metadata.get("subject", ""),
                    received_datetime=metadata.get("received_datetime", ""),
                    content_type=metadata.get("content_type", "text"),
                    has_attachment=metadata.get("has_attachment", False),
                    extraction_method=metadata.get("extraction_method", ""),
                    content_summary=content_summary,
                    key_points=key_points,
                    created_at=metadata.get("created_at", "")
                )
                mails.append(mail)
            
            return mails
            
        except Exception as e:
            print(f"Vector DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def update_mail_status(self, message_id: str, new_status: str) -> bool:
        """ë©”ì¼ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            # ChromaDBëŠ” ë©”íƒ€ë°ì´í„° ì§ì ‘ ì—…ë°ì´íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
            # ê¸°ì¡´ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ì‚­ì œ í›„ ë‹¤ì‹œ ì‚½ì…
            mail = self.get_mail_by_id(message_id)
            if not mail:
                return False
            
            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            self.collection.delete(ids=[message_id])
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸ í›„ ë‹¤ì‹œ ì €ì¥
            mail.status = new_status
            return self.save_mail(mail)
            
        except Exception as e:
            print(f"Vector DB ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
            return False

    def update_mail_labels(self, message_id: str, new_labels: List[str]) -> bool:
        """ë©”ì¼ ë ˆì´ë¸” ì—…ë°ì´íŠ¸"""
        try:
            # ChromaDBëŠ” ë©”íƒ€ë°ì´í„° ì§ì ‘ ì—…ë°ì´íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
            # ê¸°ì¡´ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ì‚­ì œ í›„ ë‹¤ì‹œ ì‚½ì…
            mail = self.get_mail_by_id(message_id)
            if not mail:
                print(f"âš ï¸ ë©”ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {message_id}")
                return False
            
            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            self.collection.delete(ids=[message_id])
            
            # ë ˆì´ë¸”ì„ key_pointsì— ì €ì¥ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
            mail.key_points = new_labels
            
            # ì €ì¥ ì„±ê³µ ì—¬ë¶€ í™•ì¸
            success = self.save_mail(mail)
            if success:
                print(f"âœ… VectorDB ë ˆì´ë¸” ì—…ë°ì´íŠ¸ ì„±ê³µ: {message_id} -> {new_labels}")
            else:
                print(f"âŒ VectorDB ë ˆì´ë¸” ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {message_id}")
            
            return success
            
        except Exception as e:
            print(f"Vector DB ë ˆì´ë¸” ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
            return False
    
    def get_all_mails(self, limit: int = 100) -> List[Mail]:
        """ëª¨ë“  ë©”ì¼ ì¡°íšŒ (ìµœê·¼ ìˆœ)"""
        try:
            # ChromaDBì˜ ëª¨ë“  ë°ì´í„° ì¡°íšŒ
            result = self.collection.get(
                include=["metadatas", "documents"]
            )
            
            mails = []
            for i, message_id in enumerate(result['ids']):
                metadata = result['metadatas'][i]
                document = result['documents'][i]
                
                # ë©”íƒ€ë°ì´í„°ì—ì„œ ì§ì ‘ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ë” ì•ˆì •ì )
                refined_content = metadata.get("refined_content", "")
                original_content = metadata.get("original_content", "")
                content_summary = metadata.get("content_summary", "")
                key_points_str = metadata.get("key_points", "[]")
                
                # key_pointsê°€ JSON ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
                try:
                    key_points = json.loads(key_points_str) if key_points_str else []
                except (json.JSONDecodeError, TypeError):
                    key_points = []
                
                # ë©”íƒ€ë°ì´í„°ì— ë‚´ìš©ì´ ì—†ìœ¼ë©´ documentì—ì„œ íŒŒì‹± ì‹œë„
                if not refined_content:
                    lines = document.strip().split('\n')
                    for line in lines:
                        if line.startswith("Content:"):
                            refined_content = line.replace("Content:", "").strip()
                        elif line.startswith("Summary:"):
                            content_summary = line.replace("Summary:", "").strip()
                        elif line.startswith("Key Points:"):
                            key_points_str = line.replace("Key Points:", "").strip()
                            key_points = [kp.strip() for kp in key_points_str.split(',') if kp.strip()]
                
                mail = Mail(
                    message_id=message_id,
                    original_content=original_content,
                    refined_content=refined_content,
                    sender=metadata.get("sender", ""),
                    status=metadata.get("status", "acceptable"),
                    subject=metadata.get("subject", ""),
                    received_datetime=metadata.get("received_datetime", ""),
                    content_type=metadata.get("content_type", "text"),
                    has_attachment=metadata.get("has_attachment", False),
                    extraction_method=metadata.get("extraction_method", ""),
                    content_summary=content_summary,
                    key_points=key_points,
                    created_at=metadata.get("created_at", "")
                )
                mails.append(mail)
            
            # ìƒì„± ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ìµœê·¼ ìˆœ)
            mails.sort(key=lambda x: x.created_at, reverse=True)
            
            return mails[:limit]
            
        except Exception as e:
            print(f"Vector DB ì „ì²´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def reset_collection(self):
        """ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (ê°œë°œìš©)"""
        try:
            self.client.delete_collection(name=self.collection_name)
            self.collection = self._get_or_create_collection()
            return True
        except Exception as e:
            print(f"ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            return False

class UserActionVectorDBManager:
    """ì‚¬ìš©ì ì•¡ì…˜ ì €ì¥ìš© Vector DB ê´€ë¦¬ì - ChromaDB ì‚¬ìš© (ì¥ê¸° ê¸°ì–µ)"""
    
    def __init__(self, db_path: str = "./vector_db"):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        self.collection_name = "user_action"
        self.collection = self._get_or_create_collection()
    
    def _get_or_create_collection(self):
        """user_action ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.client.get_collection(name=self.collection_name)
        except Exception:
            return self.client.create_collection(
                name=self.collection_name,
                metadata={
                    "description": "User actions and AI decisions for long-term memory",
                    "created_at": datetime.now().isoformat(),
                    "type": "memory_actions"
                }
            )
    
    def save_action_memory(self, action_id: str, memory_sentence: str, 
                          action_type: str, ticket_id: Optional[int] = None, 
                          message_id: Optional[str] = None, user_id: Optional[str] = None) -> bool:
        """ì•¡ì…˜ ê¸°ì–µì„ Vector DBì— ì €ì¥"""
        try:
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            metadata = {
                "action_type": action_type,
                "ticket_id": ticket_id,
                "message_id": message_id,
                "user_id": user_id or "ai_system",
                "created_at": datetime.now().isoformat()
            }
            
            # ChromaDBì— ì €ì¥ (memory_sentenceê°€ ì„ë² ë”©ë  í…ìŠ¤íŠ¸)
            self.collection.add(
                documents=[memory_sentence],
                metadatas=[metadata],
                ids=[action_id]
            )
            
            return True
            
        except Exception as e:
            print(f"ì‚¬ìš©ì ì•¡ì…˜ Vector DB ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    def search_similar_actions(self, query: str, n_results: int = 10) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ ê³¼ê±° ì•¡ì…˜ë“¤ì„ ê²€ìƒ‰ (AI ê²°ì • + ì‚¬ìš©ì í”¼ë“œë°± ëª¨ë‘ í¬í•¨)"""
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                include=["metadatas", "documents", "distances"]
            )
            
            actions = []
            for i, action_id in enumerate(results['ids'][0]):
                metadata = results['metadatas'][0][i]
                memory_sentence = results['documents'][0][i]
                distance = results['distances'][0][i] if 'distances' in results else None
                
                actions.append({
                    "action_id": action_id,
                    "memory_sentence": memory_sentence,
                    "action_type": metadata.get('action_type', ''),
                    "ticket_id": metadata.get('ticket_id'),
                    "message_id": metadata.get('message_id'),
                    "user_id": metadata.get('user_id', 'ai_system'),
                    "created_at": metadata.get('created_at', ''),
                    "similarity_score": 1 - distance if distance is not None else None
                })
            
            return actions
            
        except Exception as e:
            print(f"ì‚¬ìš©ì ì•¡ì…˜ Vector DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def get_all_actions(self, limit: int = 100) -> List[Dict[str, Any]]:
        """ëª¨ë“  ì•¡ì…˜ ê¸°ì–µ ì¡°íšŒ (ìµœê·¼ ìˆœ)"""
        try:
            result = self.collection.get(
                include=["metadatas", "documents"]
            )
            
            actions = []
            for i, action_id in enumerate(result['ids']):
                metadata = result['metadatas'][i]
                memory_sentence = result['documents'][i]
                
                actions.append({
                    "action_id": action_id,
                    "memory_sentence": memory_sentence,
                    "action_type": metadata.get('action_type', ''),
                    "ticket_id": metadata.get('ticket_id'),
                    "message_id": metadata.get('message_id'),
                    "user_id": metadata.get('user_id', 'ai_system'),
                    "created_at": metadata.get('created_at', '')
                })
            
            # ìƒì„± ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ìµœê·¼ ìˆœ)
            actions.sort(key=lambda x: x.get('created_at', ''), reverse=True)
            
            return actions[:limit]
            
        except Exception as e:
            print(f"ì‚¬ìš©ì ì•¡ì…˜ Vector DB ì „ì²´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def reset_collection(self):
        """ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        try:
            self.client.delete_collection(name=self.collection_name)
            self.collection = self._get_or_create_collection()
            print(f"âœ… {self.collection_name} ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

class SystemInfoVectorDBManager:
    """ì‹œìŠ¤í…œ ì •ë³´ íŒŒì¼ ì €ì¥ìš© Vector DB ê´€ë¦¬ì - ChromaDB ì‚¬ìš©"""
    
    def __init__(self, db_path: str = "./vector_db"):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        self.collection_name = "system_info"
        self.collection = self._get_or_create_collection()
    
    def _get_or_create_collection(self):
        """system_info ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.client.get_collection(name=self.collection_name)
        except Exception:
            return self.client.create_collection(
                name=self.collection_name,
                metadata={
                    "description": "System information and document chunks for knowledge base",
                    "created_at": datetime.now().isoformat(),
                    "type": "document_chunks"
                }
            )
    
    def _calculate_file_hash(self, file_content: bytes) -> str:
        """íŒŒì¼ ë‚´ìš©ì˜ SHA-256 í•´ì‹œ ê³„ì‚°"""
        import hashlib
        return hashlib.sha256(file_content).hexdigest()
    
    def _is_file_already_processed(self, file_hash: str) -> bool:
        """íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸ (í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ë°©ì§€)"""
        try:
            # ë©”íƒ€ë°ì´í„°ì—ì„œ file_hash ê²€ìƒ‰
            results = self.collection.get(
                where={"file_hash": file_hash},
                include=["metadatas"]
            )
            return len(results['ids']) > 0
        except Exception:
            return False
    
    def save_file_chunks(self, chunks: List[Dict[str, Any]], file_content: bytes, 
                        file_name: str, processing_duration: float) -> Dict[str, Any]:
        """íŒŒì¼ ì²­í¬ë“¤ì„ Vector DBì— ì €ì¥ (ì¤‘ë³µ ë°©ì§€ í¬í•¨)"""
        try:
            # íŒŒì¼ í•´ì‹œ ê³„ì‚°
            file_hash = self._calculate_file_hash(file_content)
            
            # ì¤‘ë³µ íŒŒì¼ í™•ì¸
            if self._is_file_already_processed(file_hash):
                return {
                    "success": True,
                    "message": f"âœ… {file_name}ì€ ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì…ë‹ˆë‹¤ (ì¤‘ë³µ ë°©ì§€)",
                    "file_hash": file_hash,
                    "duplicate": True,
                    "chunks_saved": 0
                }
            
            # ê° ì²­í¬ë¥¼ Vector DBì— ì €ì¥
            saved_chunks = []
            for chunk in chunks:
                chunk_id = str(uuid.uuid4())
                
                # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                metadata = {
                    "file_name": file_name,
                    "file_hash": file_hash,
                    "architecture": chunk.get('metadata', {}).get('architecture', 'unknown'),
                    "processing_method": chunk.get('metadata', {}).get('processing_method', 'unknown'),
                    "vision_analysis": chunk.get('metadata', {}).get('vision_analysis', False),
                    "section_title": chunk.get('metadata', {}).get('section_title', ''),
                    "page_number": chunk.get('metadata', {}).get('page_number', 1),
                    "element_count": chunk.get('metadata', {}).get('element_count', 0),
                    "file_type": chunk.get('metadata', {}).get('file_type', 'unknown'),
                    "file_size": len(file_content),
                    "processing_duration": processing_duration,
                    "created_at": datetime.now().isoformat()
                }
                
                # ìš”ì†Œ ì •ë³´ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜ (ChromaDB ë©”íƒ€ë°ì´í„° ì œí•œ)
                elements = chunk.get('metadata', {}).get('elements', [])
                if elements:
                    metadata["elements_summary"] = f"{len(elements)}ê°œ ìš”ì†Œ: {', '.join([e.get('element_type', 'unknown') for e in elements[:5]])}"
                    if len(elements) > 5:
                        metadata["elements_summary"] += f" ì™¸ {len(elements) - 5}ê°œ"
                
                # í…ìŠ¤íŠ¸ ë‚´ìš© (ì„ë² ë”©í•  í…ìŠ¤íŠ¸)
                text_content = chunk.get('text_chunk_to_embed', '')
                if not text_content:
                    text_content = f"íŒŒì¼: {file_name}, ì•„í‚¤í…ì²˜: {metadata['architecture']}, ìš”ì†Œ: {metadata['element_count']}ê°œ"
                
                # ChromaDBì— ì €ì¥
                self.collection.add(
                    documents=[text_content],
                    metadatas=[metadata],
                    ids=[chunk_id]
                )
                
                saved_chunks.append(chunk_id)
            
            return {
                "success": True,
                "message": f"âœ… {file_name} ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì–´ {len(saved_chunks)}ê°œì˜ ì²­í¬ê°€ system_info ì»¬ë ‰ì…˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "file_hash": file_hash,
                "duplicate": False,
                "chunks_saved": len(saved_chunks),
                "chunk_ids": saved_chunks
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": f"âŒ {file_name} ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }
    
    def search_similar_chunks(self, query: str, n_results: int = 5, 
                             file_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰"""
        try:
            # ê²€ìƒ‰ ì¡°ê±´ ì„¤ì •
            where_filter = {}
            if file_type:
                where_filter["file_type"] = file_type
            
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                where=where_filter if where_filter else None,
                include=["metadatas", "documents", "distances"]
            )
            
            chunks = []
            for i, chunk_id in enumerate(results['ids'][0]):
                metadata = results['metadatas'][0][i]
                document = results['documents'][0][i]
                distance = results['distances'][0][i] if 'distances' in results else None
                
                chunks.append({
                    "chunk_id": chunk_id,
                    "text_content": document,
                    "metadata": metadata,
                    "similarity_score": 1 - distance if distance is not None else None
                })
            
            return chunks
            
        except Exception as e:
            print(f"Vector DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def get_file_chunks(self, file_name: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • íŒŒì¼ì˜ ëª¨ë“  ì²­í¬ ì¡°íšŒ"""
        try:
            results = self.collection.get(
                where={"file_name": file_name},
                include=["metadatas", "documents"]
            )
            
            chunks = []
            for i, chunk_id in enumerate(results['ids']):
                metadata = results['metadatas'][i]
                document = results['documents'][i]
                
                chunks.append({
                    "chunk_id": chunk_id,
                    "text_content": document,
                    "metadata": metadata
                })
            
            return chunks
            
        except Exception as e:
            print(f"Vector DB ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ í†µê³„ ì •ë³´ ì¡°íšŒ"""
        try:
            # ì „ì²´ ë°ì´í„° ì¡°íšŒ
            result = self.collection.get(include=["metadatas"])
            
            if not result['ids']:
                return {
                    "total_chunks": 0, 
                    "file_types": {}, 
                    "total_files": 0,
                    "collection_name": self.collection_name
                }
            
            # íŒŒì¼ íƒ€ì…ë³„ í†µê³„
            file_types = {}
            unique_files = set()
            
            for metadata in result['metadatas']:
                file_type = metadata.get('file_type', 'unknown')
                file_types[file_type] = file_types.get(file_type, 0) + 1
                
                file_name = metadata.get('file_name', '')
                if file_name:
                    unique_files.add(file_name)
            
            return {
                "total_chunks": len(result['ids']),
                "file_types": file_types,
                "total_files": len(unique_files),
                "collection_name": self.collection_name
            }
            
        except Exception as e:
            print(f"ì»¬ë ‰ì…˜ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {
                "error": str(e),
                "total_chunks": 0,
                "file_types": {},
                "total_files": 0,
                "collection_name": self.collection_name
            }
    
    def reset_collection(self):
        """ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        try:
            self.client.delete_collection(name=self.collection_name)
            self.collection = self._get_or_create_collection()
            print(f"âœ… {self.collection_name} ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

class JiraInfoVectorDBManager:
    """JIRA ì •ë³´ ì €ì¥ìš© Vector DB ê´€ë¦¬ì - ChromaDB ì‚¬ìš©"""
    
    def __init__(self, db_path: str = "./vector_db"):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        self.collection_name = "jira_info"
        self.collection = self._get_or_create_collection()
    
    def _get_or_create_collection(self):
        """jira_info ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.client.get_collection(name=self.collection_name)
        except Exception:
            return self.client.create_collection(
                name=self.collection_name,
                metadata={
                    "description": "JIRA issues and knowledge base for ticket resolution",
                    "created_at": datetime.now().isoformat(),
                    "type": "jira_issues"
                }
            )
    
    def save_jira_issue(self, issue_data: Dict[str, Any]) -> bool:
        """JIRA ì´ìŠˆë¥¼ Vector DBì— ì €ì¥"""
        try:
            issue_key = issue_data.get('key', str(uuid.uuid4()))
            
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            metadata = {
                "issue_key": issue_key,
                "summary": issue_data.get('summary', ''),
                "issue_type": issue_data.get('issue_type', ''),
                "status": issue_data.get('status', ''),
                "priority": issue_data.get('priority', ''),
                "assignee": issue_data.get('assignee', ''),
                "reporter": issue_data.get('reporter', ''),
                "project_key": issue_data.get('project_key', ''),
                "created": issue_data.get('created', ''),
                "updated": issue_data.get('updated', ''),
                "created_at": datetime.now().isoformat()
            }
            
            # ë¬¸ì„œ ë‚´ìš© (ì„ë² ë”©í•  í…ìŠ¤íŠ¸)
            document_text = f"""
            Issue: {issue_data.get('summary', '')}
            Description: {issue_data.get('description', '')}
            Type: {issue_data.get('issue_type', '')}
            Status: {issue_data.get('status', '')}
            Priority: {issue_data.get('priority', '')}
            Assignee: {issue_data.get('assignee', '')}
            Reporter: {issue_data.get('reporter', '')}
            Project: {issue_data.get('project_key', '')}
            """
            
            # ChromaDBì— ì €ì¥
            self.collection.add(
                documents=[document_text],
                metadatas=[metadata],
                ids=[issue_key]
            )
            
            return True
            
        except Exception as e:
            print(f"JIRA ì´ìŠˆ Vector DB ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    def search_similar_issues(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ JIRA ì´ìŠˆ ê²€ìƒ‰"""
        try:
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                include=["metadatas", "documents", "distances"]
            )
            
            issues = []
            for i, issue_key in enumerate(results['ids'][0]):
                metadata = results['metadatas'][0][i]
                document = results['documents'][0][i]
                distance = results['distances'][0][i] if 'distances' in results else None
                
                issues.append({
                    "issue_key": issue_key,
                    "summary": metadata.get('summary', ''),
                    "description": document,
                    "issue_type": metadata.get('issue_type', ''),
                    "status": metadata.get('status', ''),
                    "priority": metadata.get('priority', ''),
                    "similarity_score": 1 - distance if distance is not None else None
                })
            
            return issues
            
        except Exception as e:
            print(f"JIRA ì´ìŠˆ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def get_all_issues(self, limit: int = 100) -> List[Dict[str, Any]]:
        """ëª¨ë“  JIRA ì´ìŠˆ ì¡°íšŒ"""
        try:
            result = self.collection.get(
                include=["metadatas", "documents"]
            )
            
            issues = []
            for i, issue_key in enumerate(result['ids']):
                metadata = result['metadatas'][i]
                document = result['documents'][i]
                
                issues.append({
                    "issue_key": issue_key,
                    "summary": metadata.get('summary', ''),
                    "description": document,
                    "issue_type": metadata.get('issue_type', ''),
                    "status": metadata.get('status', ''),
                    "priority": metadata.get('priority', ''),
                    "assignee": metadata.get('assignee', ''),
                    "reporter": metadata.get('reporter', ''),
                    "project_key": metadata.get('project_key', ''),
                    "created": metadata.get('created', ''),
                    "updated": metadata.get('updated', '')
                })
            
            # ì—…ë°ì´íŠ¸ ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ìµœê·¼ ìˆœ)
            issues.sort(key=lambda x: x.get('updated', ''), reverse=True)
            
            return issues[:limit]
            
        except Exception as e:
            print(f"JIRA ì´ìŠˆ ì „ì²´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []

class AIRecommendationEngine:
    """AI ì¶”ì²œ í•´ê²°ë°©ë²• ìƒì„± ì—”ì§„"""
    
    def __init__(self):
        self.system_info_db = SystemInfoVectorDBManager()
        self.jira_info_db = JiraInfoVectorDBManager()
    
    def generate_solution_recommendation(self, mail_content: str, ticket_history: str, output_placeholder=None) -> str:
        """ë©”ì¼ ì›ë¬¸ê³¼ í‹°ì¼“ ì´ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ AI ì¶”ì²œ í•´ê²°ë°©ë²• ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)"""
        try:
            # 1. ê´€ë ¨ ì‹œìŠ¤í…œ ì •ë³´ ê²€ìƒ‰
            system_context = self._search_system_info(mail_content, ticket_history)
            
            # 2. ê´€ë ¨ JIRA ì´ìŠˆ ê²€ìƒ‰
            jira_context = self._search_jira_issues(mail_content, ticket_history)
            
            # 3. AI ì¶”ì²œ í•´ê²°ë°©ë²• ìƒì„±
            recommendation = self._create_ai_recommendation(
                mail_content, ticket_history, system_context, jira_context, output_placeholder
            )
            
            return recommendation
            
        except Exception as e:
            print(f"AI ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return "AI ì¶”ì²œ í•´ê²°ë°©ë²•ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _search_system_info(self, mail_content: str, ticket_history: str) -> List[Dict[str, Any]]:
        """ì‹œìŠ¤í…œ ì •ë³´ì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰"""
        try:
            # ë©”ì¼ ë‚´ìš©ê³¼ í‹°ì¼“ ì´ë ¥ì„ ê²°í•©í•˜ì—¬ ê²€ìƒ‰
            search_query = f"{mail_content[:200]} {ticket_history[:200]}"
            
            # system_info ì»¬ë ‰ì…˜ì—ì„œ ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰
            similar_chunks = self.system_info_db.search_similar_chunks(
                search_query, n_results=3
            )
            
            return similar_chunks
            
        except Exception as e:
            print(f"ì‹œìŠ¤í…œ ì •ë³´ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _search_jira_issues(self, mail_content: str, ticket_history: str) -> List[Dict[str, Any]]:
        """JIRA ì´ìŠˆì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰"""
        try:
            # ë©”ì¼ ë‚´ìš©ê³¼ í‹°ì¼“ ì´ë ¥ì„ ê²°í•©í•˜ì—¬ ê²€ìƒ‰
            search_query = f"{mail_content[:200]} {ticket_history[:200]}"
            
            # jira_info ì»¬ë ‰ì…˜ì—ì„œ ìœ ì‚¬í•œ ì´ìŠˆ ê²€ìƒ‰
            similar_issues = self.jira_info_db.search_similar_issues(
                search_query, n_results=3
            )
            
            return similar_issues
            
        except Exception as e:
            print(f"JIRA ì´ìŠˆ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _create_ai_recommendation(self, mail_content: str, ticket_history: str, 
                                 system_context: List[Dict[str, Any]], 
                                 jira_context: List[Dict[str, Any]], 
                                 output_placeholder=None) -> str:
        """AI ì¶”ì²œ í•´ê²°ë°©ë²• ìƒì„± - LLM ì „ìš© (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)"""
        try:
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì •ë¦¬
            system_info_text = ""
            if system_context:
                system_info_text = "\n\nê´€ë ¨ ì‹œìŠ¤í…œ ì •ë³´:\n"
                for chunk in system_context:
                    system_info_text += f"- {chunk.get('text_content', '')[:300]}...\n"
            
            jira_info_text = ""
            if jira_context:
                jira_info_text = "\n\nê´€ë ¨ JIRA ì´ìŠˆ:\n"
                for issue in jira_context:
                    jira_info_text += f"- {issue.get('summary', '')}: {issue.get('description', '')[:300]}...\n"
            
            # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± (f-string í¬ë§·íŒ… ì˜¤ë¥˜ ë°©ì§€)
            prompt = """
ë‹¹ì‹ ì€ IT ìš´ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ë°©ë²•ì„ ì œì‹œí•´ì£¼ì„¸ìš”.

ğŸ“§ **ë©”ì¼ ë‚´ìš©:**
{mail_content}

ğŸ“‹ **í‹°ì¼“ íˆìŠ¤í† ë¦¬:**
{ticket_history}

ğŸ“š **ê´€ë ¨ ì‹œìŠ¤í…œ ì •ë³´:**
{system_info}

ğŸ« **ê´€ë ¨ JIRA ì´ìŠˆ:**
{jira_info}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ êµ¬ì²´ì ì¸ í•´ê²°ë°©ë²•ì„ ì œì‹œí•´ì£¼ì„¸ìš”:

ğŸ¤– **AI ì¶”ì²œ í•´ê²°ë°©ë²•**

ğŸ” **ë¬¸ì œ ë¶„ì„:**
[ë©”ì¼ ë‚´ìš©ê³¼ í‹°ì¼“ íˆìŠ¤í† ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ë¬¸ì œ ìƒí™© ë¶„ì„]

ğŸ¯ **ê¶Œì¥ ì¡°ì¹˜:**
1. [êµ¬ì²´ì ì¸ ì²« ë²ˆì§¸ ì¡°ì¹˜]
2. [êµ¬ì²´ì ì¸ ë‘ ë²ˆì§¸ ì¡°ì¹˜]
3. [êµ¬ì²´ì ì¸ ì„¸ ë²ˆì§¸ ì¡°ì¹˜]

ğŸ“š **ì°¸ê³  ìë£Œ í™œìš©:**
[ì‹œìŠ¤í…œ ì •ë³´ì™€ JIRA ì´ìŠˆë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ì²´ì ì¸ ì°¸ê³ ì‚¬í•­]

ğŸ’¡ **ì¶”ê°€ ê¶Œì¥ì‚¬í•­:**
- [ë§¥ë½ì— ë§ëŠ” êµ¬ì²´ì ì¸ ê¶Œì¥ì‚¬í•­]
- [ë§¥ë½ì— ë§ëŠ” êµ¬ì²´ì ì¸ ê¶Œì¥ì‚¬í•­]
- [ë§¥ë½ì— ë§ëŠ” êµ¬ì²´ì ì¸ ê¶Œì¥ì‚¬í•­]

âš ï¸ **ì£¼ì˜ì‚¬í•­:**
[í•´ë‹¹ ìƒí™©ì—ì„œ ì£¼ì˜í•´ì•¼ í•  ì ì´ë‚˜ ìœ„í—˜ìš”ì†Œ]

ìœ„ í˜•ì‹ì— ë§ì¶° êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ë°©ë²•ì„ ì œì‹œí•´ì£¼ì„¸ìš”. ì¼ë°˜ì ì¸ ë‚´ìš©ì´ ì•„ë‹Œ, ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ë§¥ë½ì— ë§ëŠ” êµ¬ì²´ì ì¸ ë‚´ìš©ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
"""
            
            # LLM í˜¸ì¶œ (Azure OpenAI ì‚¬ìš©)
            try:
                from langchain_openai import AzureChatOpenAI
                from langchain_core.prompts import ChatPromptTemplate
                from langchain_core.output_parsers import StrOutputParser
                
                # í™˜ê²½ ë³€ìˆ˜ì—ì„œ Azure OpenAI ì„¤ì • ê°€ì ¸ì˜¤ê¸°
                import os
                azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
                deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
                api_key = os.getenv("AZURE_OPENAI_API_KEY")
                api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-10-21")
                
                if azure_endpoint and deployment_name and api_key:
                    # Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                    llm = AzureChatOpenAI(
                        azure_endpoint=azure_endpoint,
                        deployment_name=deployment_name,
                        openai_api_key=api_key,
                        openai_api_version=api_version,
                        temperature=0.3  # ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±ì˜ ê· í˜•
                    )
                    
                    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
                    prompt_template = ChatPromptTemplate.from_template(prompt)
                    
                    # ì²´ì¸ ì‹¤í–‰
                    chain = prompt_template | llm | StrOutputParser()
                    
                    # ë³€ìˆ˜ ë§¤í•‘
                    variables = {
                        "mail_content": mail_content[:1000],
                        "ticket_history": ticket_history[:1000] if ticket_history else "ì—†ìŒ",
                        "system_info": system_info_text if system_info_text else "ê´€ë ¨ ì‹œìŠ¤í…œ ì •ë³´ ì—†ìŒ",
                        "jira_info": jira_info_text if jira_info_text else "ê´€ë ¨ JIRA ì´ìŠˆ ì—†ìŒ"
                    }
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
                    if output_placeholder:
                        current_output = ""
                        final_recommendation = ""
                        
                        for chunk in chain.stream(variables):
                            current_output += chunk
                            final_recommendation = current_output
                            
                            # ì‹¤ì‹œê°„ ì¶œë ¥ ì—…ë°ì´íŠ¸
                            with output_placeholder.container():
                                st.markdown("### ğŸ¤– AI ì¶”ì²œ ìƒì„± ì¤‘...")
                                st.markdown(current_output)
                                st.info("ğŸ”„ AI ì¶”ì²œì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                        
                        # ìµœì¢… ì™„ë£Œ í‘œì‹œ
                        with output_placeholder.container():
                            st.success("âœ… AI ì¶”ì²œ ìƒì„± ì™„ë£Œ!")
                        
                        return final_recommendation
                    else:
                        # ì¼ë°˜ ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¬ë° ì—†ìŒ)
                        recommendation = chain.invoke(variables)
                        return recommendation
                else:
                    # Azure OpenAI ì„¤ì •ì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜
                    return "âŒ Azure OpenAI ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
                    
            except Exception as e:
                print(f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                return f"âŒ AI ì¶”ì²œ ìƒì„± ì‹¤íŒ¨: {str(e)}"
            
        except Exception as e:
            print(f"AI ì¶”ì²œ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"âŒ AI ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"