#!/usr/bin/env python3
"""
Vector DBìš© Mail ëª¨ë¸ ë° ê´€ë¦¬ì
"""

import os
from dotenv import load_dotenv
from dataclasses import dataclass, asdict
from typing import List, Optional, Dict, Any
import uuid
from datetime import datetime
import chromadb
from chromadb.config import Settings
import json
from chromadb_singleton import get_chromadb_client, get_chromadb_collection, reset_chromadb_singleton

# í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ëª¨ë“ˆ import
from text_preprocessor import preprocess_for_embedding

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

@dataclass
class Mail:
    """ë©”ì¼ ëª¨ë¸ - Vector DB Collectionìš©"""
    message_id: str  # PK - ì›ë³¸ ë©”ì¼ ID (ì—°ê²° í‚¤)
    original_content: str  # HTML/Text ì›ë³¸ ë‚´ìš©
    refined_content: str  # ì¶”ì¶œëœ í•µì‹¬ ë‚´ìš©
    sender: str  # ë³´ë‚¸ ì‚¬ëŒ
    status: str  # acceptable, unacceptable ë“±
    subject: str  # ë©”ì¼ ì œëª©
    received_datetime: str  # ìˆ˜ì‹  ì‹œê°„
    content_type: str  # html, text
    has_attachment: bool  # ì²¨ë¶€íŒŒì¼ ì—¬ë¶€
    extraction_method: str  # ì¶”ì¶œ ë°©ë²•
    content_summary: str  # ë‚´ìš© ìš”ì•½
    key_points: List[str]  # í•µì‹¬ í¬ì¸íŠ¸
    created_at: str  # ìƒì„± ì‹œê°

@dataclass
class FileChunk:
    """íŒŒì¼ ì²­í¬ ëª¨ë¸ - Vector DB Collectionìš©"""
    chunk_id: str  # PK - ê³ ìœ  ì²­í¬ ID
    file_name: str  # ì›ë³¸ íŒŒì¼ëª…
    file_hash: str  # íŒŒì¼ í•´ì‹œê°’ (ì¤‘ë³µ ë°©ì§€ìš©)
    text_chunk: str  # ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë‚´ìš©
    architecture: str  # dual_path_hybrid ë˜ëŠ” simple_conversion
    processing_method: str  # ì²˜ë¦¬ ë°©ë²•
    vision_analysis: bool  # Vision ë¶„ì„ ì ìš© ì—¬ë¶€
    section_title: str  # ì„¹ì…˜ ì œëª©
    page_number: int  # í˜ì´ì§€/ìŠ¬ë¼ì´ë“œ ë²ˆí˜¸
    element_count: int  # ìš”ì†Œ ê°œìˆ˜
    file_type: str  # pptx, docx, pdf, xlsx, txt, md, csv, scds
    elements: List[Dict[str, Any]]  # ìš”ì†Œë³„ ìƒì„¸ ì •ë³´
    created_at: str  # ìƒì„± ì‹œê°
    file_size: int  # íŒŒì¼ í¬ê¸° (ë°”ì´íŠ¸)
    processing_duration: float  # ì²˜ë¦¬ ì‹œê°„ (ì´ˆ)

@dataclass
class StructuredChunk:
    """êµ¬ì¡°ì  ì²­í¬ ëª¨ë¸ - Vector DB Collectionìš©"""
    chunk_id: str  # PK - ê³ ìœ  ì²­í¬ ID
    content: str  # ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë‚´ìš©
    chunk_type: str  # 'header', 'comment'
    ticket_id: str  # í‹°ì¼“ ID
    field_name: str  # í•„ë“œëª…
    field_value: str  # í•„ë“œê°’
    priority: int  # ìš°ì„ ìˆœìœ„ (1: ë†’ìŒ, 2: ì¤‘ê°„, 3: ë‚®ìŒ)
    file_name: str  # ì›ë³¸ íŒŒì¼ëª…
    file_type: str  # íŒŒì¼ íƒ€ì…
    metadata: Dict[str, Any]  # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
    created_at: str  # ìƒì„± ì‹œê°
    commenter: Optional[str] = None  # ëŒ“ê¸€ ì‘ì„±ì (comment íƒ€ì…ì¼ ë•Œë§Œ)

class VectorDBManager:
    """Vector DB ê´€ë¦¬ì - ChromaDB ì‚¬ìš©"""
    
    def __init__(self, db_path: str = "./vector_db"):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” (ì‹±ê¸€í†¤ ì‚¬ìš©)"""
        self.db_path = db_path
        
        # ì‹±ê¸€í†¤ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©
        try:
            self.client = get_chromadb_client()
            print("âœ… ChromaDB ì‹±ê¸€í†¤ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©")
        except Exception as e:
            print(f"âš ï¸ ChromaDB ì‹±ê¸€í†¤ í´ë¼ì´ì–¸íŠ¸ ì‹¤íŒ¨, ì¬ì„¤ì • ì‹œë„: {e}")
            try:
                reset_chromadb_singleton()
                self.client = get_chromadb_client()
                print("âœ… ChromaDB ì‹±ê¸€í†¤ ì¬ì„¤ì • í›„ ì„±ê³µ")
            except Exception as e2:
                print(f"âŒ ChromaDB ì‹±ê¸€í†¤ ì¬ì„¤ì • ì‹¤íŒ¨: {e2}")
                raise e2
        
        self.collection_name = "mail_collection"
        self.collection = self._get_or_create_collection()
        
        # ChromaDB íŒŒì¼ ê¶Œí•œ ìë™ ì„¤ì •
        try:
            chroma_file = os.path.join(db_path, "chroma.sqlite3")
            if os.path.exists(chroma_file):
                os.chmod(chroma_file, 0o666)
                print(f"âœ… ChromaDB íŒŒì¼ ê¶Œí•œ ìë™ ì„¤ì •: {chroma_file}")
        except Exception as e:
            print(f"âš ï¸ ChromaDB íŒŒì¼ ê¶Œí•œ ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _ensure_vector_db_permissions(self):
        """Vector DB í´ë” ë° íŒŒì¼ ê¶Œí•œì„ í™•ì‹¤íˆ ì„¤ì •"""
        try:
            import os
            
            # Vector DB í´ë” ê¶Œí•œ ì„¤ì • (ì´ˆê¸°í™” ì‹œ ì„¤ì •í•œ ê²½ë¡œ ì‚¬ìš©)
            vector_db_path = "./vector_db"
            if os.path.exists(vector_db_path):
                os.chmod(vector_db_path, 0o755)
            
            # ChromaDB ê´€ë ¨ ëª¨ë“  íŒŒì¼ ê¶Œí•œ ì„¤ì •
            for root, dirs, files in os.walk(vector_db_path):
                # í´ë” ê¶Œí•œ ì„¤ì •
                for dir_name in dirs:
                    dir_path = os.path.join(root, dir_name)
                    try:
                        os.chmod(dir_path, 0o755)
                    except Exception:
                        pass
                
                # íŒŒì¼ ê¶Œí•œ ì„¤ì •
                for file_name in files:
                    file_path = os.path.join(root, file_name)
                    try:
                        os.chmod(file_path, 0o666)
                    except Exception:
                        pass
            
            print(f"âœ… Vector DB ê¶Œí•œ ì¬ì„¤ì • ì™„ë£Œ: {vector_db_path}")
            
        except Exception as e:
            print(f"âš ï¸ Vector DB ê¶Œí•œ ì¬ì„¤ì • ì‹¤íŒ¨: {e}")
    
    def _get_or_create_collection(self, collection_name: str = None):
        """ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°"""
        if collection_name is None:
            collection_name = self.collection_name
        
        try:
            return self.client.get_collection(name=collection_name)
        except Exception:
            return self.client.create_collection(
                name=collection_name,
                metadata={
                    "description": f"Collection for {collection_name}",
                    "created_at": datetime.now().isoformat()
                }
            )
    
    def save_mail(self, mail: Mail) -> bool:
        """ë©”ì¼ì„ Vector DBì— ì €ì¥ (ìƒì„¸ ë¡œê·¸ í¬í•¨)"""
        print(f"\nğŸ’¾ [VectorDB] ë©”ì¼ ì €ì¥ ì‹œì‘: {mail.message_id}")
        print(f"   ğŸ“Š [VectorDB] ì €ì¥í•  ë©”ì¼ ì •ë³´:")
        print(f"      - ì œëª©: {mail.subject}")
        print(f"      - ë°œì‹ ì: {mail.sender}")
        print(f"      - original_content ê¸¸ì´: {len(mail.original_content)} ë¬¸ì")
        print(f"      - refined_content ê¸¸ì´: {len(mail.refined_content)} ë¬¸ì")

        try:
            # ì €ì¥ ì „ Vector DB í´ë” ë° íŒŒì¼ ê¶Œí•œ ì¬ì„¤ì •
            self._ensure_vector_db_permissions()

            # ChromaDB íŒŒì¼ ê¶Œí•œ íŠ¹ë³„ í™•ì¸
            import os
            import stat
            chroma_file = os.path.join("./vector_db", "chroma.sqlite3")
            if os.path.exists(chroma_file):
                # í˜„ì¬ ê¶Œí•œ í™•ì¸
                current_perms = os.stat(chroma_file).st_mode
                if not (current_perms & stat.S_IWUSR):
                    print(f"âš ï¸ ChromaDB íŒŒì¼ì´ ì½ê¸° ì „ìš©ì…ë‹ˆë‹¤. ê¶Œí•œì„ ê°•ì œë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
                    # ê°•ì œë¡œ ì“°ê¸° ê¶Œí•œ ë¶€ì—¬
                    os.chmod(chroma_file, 0o666)
                    # ì†Œìœ ì ê¶Œí•œë„ í™•ì¸
                    current_perms = os.stat(chroma_file).st_mode
                    if not (current_perms & stat.S_IWUSR):
                        os.chmod(chroma_file, current_perms | stat.S_IWUSR | stat.S_IWGRP | stat.S_IWOTH)
                    print(f"âœ… ChromaDB íŒŒì¼ ê¶Œí•œ ê°•ì œ ì„¤ì • ì™„ë£Œ")

            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„ (datetime ê°ì²´ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜)
            print(f"   ğŸ—ï¸ [VectorDB] ë©”íƒ€ë°ì´í„° ì¤€ë¹„ ì¤‘...")
            metadata = {
                "sender": mail.sender,
                "status": mail.status,
                "subject": mail.subject,
                "received_datetime": mail.received_datetime.isoformat() if hasattr(mail.received_datetime, 'isoformat') else str(mail.received_datetime),
                "content_type": mail.content_type,
                "has_attachment": mail.has_attachment,
                "extraction_method": mail.extraction_method,
                "content_summary": mail.content_summary,
                "key_points": json.dumps(mail.key_points),
                "labels": json.dumps(mail.key_points),  # labels í•„ë“œ ì¶”ê°€
                "created_at": mail.created_at.isoformat() if hasattr(mail.created_at, 'isoformat') else str(mail.created_at),
                "original_content": mail.original_content,  # ì›ë³¸ ë‚´ìš© ì¶”ê°€
                "refined_content": mail.refined_content    # ì •ì œëœ ë‚´ìš© ì¶”ê°€
            }

            print(f"   âœ… [VectorDB] ë©”íƒ€ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
            print(f"      - original_content in metadata: {len(metadata['original_content'])} ë¬¸ì")
            print(f"      - refined_content in metadata: {len(metadata['refined_content'])} ë¬¸ì")
            
            # ë¬¸ì„œ ë‚´ìš© (ì„ë² ë”©í•  í…ìŠ¤íŠ¸)
            print(f"   ğŸ“ [VectorDB] ì„ë² ë”©ìš© ë¬¸ì„œ í…ìŠ¤íŠ¸ ì¤€ë¹„ ì¤‘...")
            document_text = f"""
            Subject: {mail.subject}
            Sender: {mail.sender}
            Content: {mail.refined_content}
            Summary: {mail.content_summary}
            Key Points: {', '.join(mail.key_points)}
            Labels: {', '.join(mail.key_points)}
            """

            print(f"   ğŸ”§ [VectorDB] í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì ìš© ì¤‘...")
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì ìš©
            preprocessed_document = preprocess_for_embedding(document_text)
            print(f"   âœ… [VectorDB] ì „ì²˜ë¦¬ ì™„ë£Œ: {len(preprocessed_document)} ë¬¸ì")

            print(f"   ğŸ’¿ [VectorDB] ChromaDBì— ì €ì¥ ì¤‘... (ID: {mail.message_id})")
            # ChromaDBì— ì €ì¥ (ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸)
            self.collection.add(
                documents=[preprocessed_document],
                metadatas=[metadata],
                ids=[mail.message_id]
            )

            print(f"   ğŸ”’ [VectorDB] ì €ì¥ í›„ ê¶Œí•œ ì¬í™•ì¸ ì¤‘...")
            # ì €ì¥ í›„ ê¶Œí•œ ì¬í™•ì¸
            self._ensure_vector_db_permissions()

            print(f"   âœ… [VectorDB] ë©”ì¼ ì €ì¥ ì„±ê³µ: {mail.message_id}")
            return True

        except Exception as e:
            print(f"   âŒ [VectorDB] ì €ì¥ ì˜¤ë¥˜: {e}")
            import traceback
            print(f"   ğŸ“‹ [VectorDB] ìƒì„¸ ì €ì¥ ì˜¤ë¥˜:")
            traceback.print_exc()
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¶Œí•œ ì¬ì„¤ì • ì‹œë„
            try:
                print(f"   ğŸ”§ [VectorDB] ì˜¤ë¥˜ í›„ ê¶Œí•œ ì¬ì„¤ì • ì‹œë„...")
                self._ensure_vector_db_permissions()
            except:
                pass
            return False
    
    def get_mail_by_id(self, message_id: str) -> Optional[Mail]:
        """ë©”ì‹œì§€ IDë¡œ ë©”ì¼ ì¡°íšŒ (ìƒì„¸ ë¡œê·¸ í¬í•¨)"""
        print(f"\nğŸ” [VectorDB] ë©”ì¼ ì¡°íšŒ ì‹œì‘: {message_id}")
        try:
            print(f"   ğŸ“Š [VectorDB] ChromaDB ì»¬ë ‰ì…˜ì—ì„œ ì¡°íšŒ ì¤‘...")
            result = self.collection.get(
                ids=[message_id],
                include=["metadatas", "documents"]
            )

            print(f"   ğŸ“‹ [VectorDB] ì¡°íšŒ ê²°ê³¼: {len(result.get('ids', []))}ê°œ ì•„ì´í…œ ë°œê²¬")
            if not result['ids']:
                print(f"   âŒ [VectorDB] ë©”ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {message_id}")
                return None

            metadata = result['metadatas'][0]
            document = result['documents'][0]

            print(f"   âœ… [VectorDB] ë©”ì¼ ë°œê²¬! ë©”íƒ€ë°ì´í„° í‚¤: {list(metadata.keys())}")

            # ë©”íƒ€ë°ì´í„°ì—ì„œ ì§ì ‘ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ë” ì•ˆì •ì )
            refined_content = metadata.get("refined_content", "")
            original_content = metadata.get("original_content", "")
            content_summary = metadata.get("content_summary", "")
            key_points_str = metadata.get("key_points", "[]")
            labels_str = metadata.get("labels", "[]")  # labels í•„ë“œ ì¶”ê°€

            print(f"   ğŸ“ [VectorDB] ë‚´ìš© ê¸¸ì´ í™•ì¸:")
            print(f"      - original_content: {len(original_content)} ë¬¸ì")
            print(f"      - refined_content: {len(refined_content)} ë¬¸ì")
            print(f"      - content_summary: {len(content_summary)} ë¬¸ì")
            print(f"      - document: {len(document)} ë¬¸ì")
            
            # key_pointsì™€ labelsê°€ JSON ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
            try:
                key_points = json.loads(key_points_str) if key_points_str else []
            except (json.JSONDecodeError, TypeError):
                key_points = []
                
            try:
                labels = json.loads(labels_str) if labels_str else []
                # labelsê°€ ìˆìœ¼ë©´ key_pointsì— ë³‘í•© (ë ˆì´ë¸” ìš°ì„ )
                if labels:
                    key_points = labels
            except (json.JSONDecodeError, TypeError):
                labels = []
            
            # ë©”íƒ€ë°ì´í„°ì— ë‚´ìš©ì´ ì—†ìœ¼ë©´ documentì—ì„œ íŒŒì‹± ì‹œë„
            if not refined_content:
                print(f"   ğŸ”„ [VectorDB] refined_contentê°€ ì—†ìŒ, documentì—ì„œ íŒŒì‹± ì‹œë„...")
                lines = document.strip().split('\n')
                for line in lines:
                    if line.startswith("Content:"):
                        refined_content = line.replace("Content:", "").strip()
                        print(f"      ğŸ“„ Documentì—ì„œ Content íŒŒì‹±: {len(refined_content)} ë¬¸ì")
                    elif line.startswith("Summary:"):
                        content_summary = line.replace("Summary:", "").strip()
                        print(f"      ğŸ“‹ Documentì—ì„œ Summary íŒŒì‹±: {len(content_summary)} ë¬¸ì")
                    elif line.startswith("Key Points:"):
                        key_points_str = line.replace("Key Points:", "").strip()
                        key_points = [kp.strip() for kp in key_points_str.split(',') if kp.strip()]
                        print(f"      ğŸ”‘ Documentì—ì„œ Key Points íŒŒì‹±: {len(key_points)}ê°œ í¬ì¸íŠ¸")

            # ìµœì¢… ê²°ê³¼ ë¡œê·¸
            print(f"   âœ¨ [VectorDB] Mail ê°ì²´ ìƒì„±:")
            print(f"      - ì œëª©: {metadata.get('subject', 'ì œëª© ì—†ìŒ')}")
            print(f"      - ë°œì‹ ì: {metadata.get('sender', 'ë°œì‹ ì ë¶ˆëª…')}")
            print(f"      - original_content ìµœì¢… ê¸¸ì´: {len(original_content)} ë¬¸ì")
            print(f"      - refined_content ìµœì¢… ê¸¸ì´: {len(refined_content)} ë¬¸ì")

            if len(original_content) == 0:
                print(f"   âš ï¸ [VectorDB] ê²½ê³ : original_contentê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
                # ë©”íƒ€ë°ì´í„° ì „ì²´ ë‚´ìš© ì¶œë ¥
                print(f"   ğŸ” [VectorDB] ì „ì²´ ë©”íƒ€ë°ì´í„° ë””ë²„ê·¸:")
                for key, value in metadata.items():
                    value_preview = str(value)[:100] + "..." if len(str(value)) > 100 else str(value)
                    print(f"      - {key}: {value_preview}")

            mail_obj = Mail(
                message_id=message_id,
                original_content=original_content,
                refined_content=refined_content,
                sender=metadata.get("sender", ""),
                status=metadata.get("status", "acceptable"),
                subject=metadata.get("subject", ""),
                received_datetime=metadata.get("received_datetime", ""),
                content_type=metadata.get("content_type", "text"),
                has_attachment=metadata.get("has_attachment", False),
                extraction_method=metadata.get("extraction_method", ""),
                content_summary=content_summary,
                key_points=key_points,
                created_at=metadata.get("created_at", "")
            )

            print(f"   âœ… [VectorDB] Mail ê°ì²´ ìƒì„± ì™„ë£Œ!")
            return mail_obj

        except Exception as e:
            print(f"   âŒ [VectorDB] ì¡°íšŒ ì˜¤ë¥˜: {e}")
            import traceback
            print(f"   ğŸ“‹ [VectorDB] ìƒì„¸ ì˜¤ë¥˜:")
            traceback.print_exc()
            return None
    
    def search_similar_mails(self, query: str, n_results: int = 5) -> List[Mail]:
        """ìœ ì‚¬í•œ ë©”ì¼ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì ìš©
            preprocessed_query = preprocess_for_embedding(query)
            
            results = self.collection.query(
                query_texts=[preprocessed_query],
                n_results=n_results,
                include=["metadatas", "documents", "distances"]
            )
            
            mails = []
            for i, message_id in enumerate(results['ids'][0]):
                metadata = results['metadatas'][0][i]
                document = results['documents'][0][i]
                
                # ë©”íƒ€ë°ì´í„°ì—ì„œ ì§ì ‘ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ë” ì•ˆì •ì )
                refined_content = metadata.get("refined_content", "")
                original_content = metadata.get("original_content", "")
                content_summary = metadata.get("content_summary", "")
                key_points_str = metadata.get("key_points", "[]")
                
                # key_pointsê°€ JSON ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
                try:
                    key_points = json.loads(key_points_str) if key_points_str else []
                except (json.JSONDecodeError, TypeError):
                    key_points = []
                
                # ë©”íƒ€ë°ì´í„°ì— ë‚´ìš©ì´ ì—†ìœ¼ë©´ documentì—ì„œ íŒŒì‹± ì‹œë„
                if not refined_content:
                    lines = document.strip().split('\n')
                    for line in lines:
                        if line.startswith("Content:"):
                            refined_content = line.replace("Content:", "").strip()
                        elif line.startswith("Summary:"):
                            content_summary = line.replace("Summary:", "").strip()
                        elif line.startswith("Key Points:"):
                            key_points_str = line.replace("Key Points:", "").strip()
                            key_points = [kp.strip() for kp in key_points_str.split(',') if kp.strip()]
                
                mail = Mail(
                    message_id=message_id,
                    original_content=original_content,
                    refined_content=refined_content,
                    sender=metadata.get("sender", ""),
                    status=metadata.get("status", "acceptable"),
                    subject=metadata.get("subject", ""),
                    received_datetime=metadata.get("received_datetime", ""),
                    content_type=metadata.get("content_type", "text"),
                    has_attachment=metadata.get("has_attachment", False),
                    extraction_method=metadata.get("extraction_method", ""),
                    content_summary=content_summary,
                    key_points=key_points,
                    created_at=metadata.get("created_at", "")
                )
                mails.append(mail)
            
            return mails
            
        except Exception as e:
            print(f"Vector DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def update_mail_status(self, message_id: str, new_status: str) -> bool:
        """ë©”ì¼ ìƒíƒœ ì—…ë°ì´íŠ¸"""
        try:
            # ChromaDBëŠ” ë©”íƒ€ë°ì´í„° ì§ì ‘ ì—…ë°ì´íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
            # ê¸°ì¡´ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ì‚­ì œ í›„ ë‹¤ì‹œ ì‚½ì…
            mail = self.get_mail_by_id(message_id)
            if not mail:
                return False
            
            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            self.collection.delete(ids=[message_id])
            
            # ìƒíƒœ ì—…ë°ì´íŠ¸ í›„ ë‹¤ì‹œ ì €ì¥
            mail.status = new_status
            return self.save_mail(mail)
            
        except Exception as e:
            print(f"Vector DB ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
            return False

    def update_mail_labels(self, message_id: str, new_labels: List[str]) -> bool:
        """ë©”ì¼ ë ˆì´ë¸” ì—…ë°ì´íŠ¸"""
        try:
            # ChromaDBëŠ” ë©”íƒ€ë°ì´í„° ì§ì ‘ ì—…ë°ì´íŠ¸ë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ
            # ê¸°ì¡´ ë°ì´í„°ë¥¼ ê°€ì ¸ì™€ì„œ ì‚­ì œ í›„ ë‹¤ì‹œ ì‚½ì…
            mail = self.get_mail_by_id(message_id)
            if not mail:
                print(f"âš ï¸ ë©”ì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {message_id}")
                return False
            
            # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ
            self.collection.delete(ids=[message_id])
            
            # ë ˆì´ë¸”ì„ key_pointsì— ì €ì¥ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
            mail.key_points = new_labels
            
            # ì €ì¥ ì„±ê³µ ì—¬ë¶€ í™•ì¸
            success = self.save_mail(mail)
            if success:
                print(f"âœ… VectorDB ë ˆì´ë¸” ì—…ë°ì´íŠ¸ ì„±ê³µ: {message_id} -> {new_labels}")
            else:
                print(f"âŒ VectorDB ë ˆì´ë¸” ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {message_id}")
            
            return success
            
        except Exception as e:
            print(f"Vector DB ë ˆì´ë¸” ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
            return False
    
    def search_similar_file_chunks(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ íŒŒì¼ ì²­í¬ ê²€ìƒ‰ (í—¤ë” í…Œì´ë¸” í•„í„°ë§ í¬í•¨)"""
        try:
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì ìš©
            preprocessed_query = preprocess_for_embedding(query)
            
            # file_chunks ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
            file_chunks_collection = self.client.get_collection("file_chunks")
            
            # ë” ë§ì€ ê²°ê³¼ë¥¼ ê°€ì ¸ì™€ì„œ í•„í„°ë§ í›„ ì›í•˜ëŠ” ê°œìˆ˜ë§Œ ë°˜í™˜
            results = file_chunks_collection.query(
                query_texts=[preprocessed_query],
                n_results=n_results * 3,  # í•„í„°ë§ì„ ìœ„í•´ 3ë°° ë” ê°€ì ¸ì˜¤ê¸°
                include=["metadatas", "documents", "distances"]
            )
            
            file_chunks = []
            for i, chunk_id in enumerate(results['ids'][0]):
                metadata = results['metadatas'][0][i]
                document = results['documents'][0][i]
                distance = results['distances'][0][i] if results['distances'] else 0.0
                
                # í—¤ë” í…Œì´ë¸” í•„í„°ë§ (ë©”íƒ€ë°ì´í„°ë‚˜ ì§§ì€ ë‚´ìš© ì œì™¸)
                if self._is_header_table_content(document):
                    print(f"ğŸš« í—¤ë” í…Œì´ë¸” ë‚´ìš© í•„í„°ë§: {document[:50]}...")
                    continue
                
                # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° (ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ìœ ì‚¬ë„ ë†’ìŒ)
                similarity_score = max(0.0, 1.0 - distance)
                
                file_chunk = {
                    "chunk_id": chunk_id,
                    "file_name": metadata.get("file_name", ""),
                    "file_type": metadata.get("file_type", ""),
                    "content": document,
                    "page_number": metadata.get("page_number", 1),
                    "element_type": metadata.get("element_type", "text"),
                    "similarity_score": similarity_score,
                    "created_at": metadata.get("created_at", "")
                }
                file_chunks.append(file_chunk)
                
                # ì›í•˜ëŠ” ê°œìˆ˜ë§Œí¼ ìˆ˜ì§‘ë˜ë©´ ì¤‘ë‹¨
                if len(file_chunks) >= n_results:
                    break
            
            print(f"âœ… ìœ ì‚¬ íŒŒì¼ ì²­í¬ ê²€ìƒ‰ ì™„ë£Œ: {len(file_chunks)}ê°œ ê²°ê³¼ (í—¤ë” í…Œì´ë¸” í•„í„°ë§ ì ìš©)")
            return file_chunks
            
        except Exception as e:
            print(f"âŒ ìœ ì‚¬ íŒŒì¼ ì²­í¬ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def _is_header_table_content(self, document: str) -> bool:
        """í—¤ë” í…Œì´ë¸” ë‚´ìš©ì¸ì§€ íŒë‹¨"""
        if not document or len(document.strip()) < 10:
            return True
        
        # í—¤ë” í…Œì´ë¸” ê´€ë ¨ í‚¤ì›Œë“œë“¤
        header_keywords = [
            "2025-09-07 20:31ì—ì„œ187ì´ìŠˆë¥¼ í‘œì‹œ",
            "2025-09-07 20:32ì—ì„œ845ì´ìŠˆë¥¼ í‘œì‹œ",
            "2025-09-07 20:26ì—ì„œ672ì´ìŠˆë¥¼ í‘œì‹œ",
            "Jira 2025-09-07 20:26",
            "Jira 9.12.19#9120019-sha1",
            "ì—ì„œ 672 ì´ìŠˆë¥¼ í‘œì‹œ",
            "ì—ì„œ 845 ì´ìŠˆë¥¼ í‘œì‹œ",
            "ì—ì„œ 187 ì´ìŠˆë¥¼ í‘œì‹œ",
            "Jira 9.12.19",
            "SK C&C] ì¡°ì£¼ì—°ì— ì˜í•´",
            "Sun Sep 07 20:26:15 KST 2025ì—ì„œ ìƒì„±ë¨",
            "ì—ì„œ672ì´ìŠˆë¥¼ í‘œì‹œ",
            "ì—ì„œ845ì´ìŠˆë¥¼ í‘œì‹œ",
            "ì—ì„œ187ì´ìŠˆë¥¼ í‘œì‹œ"
        ]
        
        # í‚¤ì›Œë“œ ì¤‘ í•˜ë‚˜ë¼ë„ í¬í•¨ë˜ì–´ ìˆìœ¼ë©´ í—¤ë” í…Œì´ë¸”ë¡œ íŒë‹¨
        for keyword in header_keywords:
            if keyword in document:
                return True
        
        # ë„ˆë¬´ ì§§ì€ ë‚´ìš©ë„ ì œì™¸ (ì‹¤ì œ í‹°ì¼“ ë°ì´í„°ëŠ” ë” ê¸¸ì–´ì•¼ í•¨)
        if len(document.strip()) < 50:
            return True
            
        return False
    
    def get_all_mails(self, limit: int = 100) -> List[Mail]:
        """ëª¨ë“  ë©”ì¼ ì¡°íšŒ (ìµœê·¼ ìˆœ)"""
        try:
            # ChromaDBì˜ ëª¨ë“  ë°ì´í„° ì¡°íšŒ
            result = self.collection.get(
                include=["metadatas", "documents"]
            )
            
            mails = []
            for i, message_id in enumerate(result['ids']):
                metadata = result['metadatas'][i]
                document = result['documents'][i]
                
                # ë©”íƒ€ë°ì´í„°ì—ì„œ ì§ì ‘ ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ë” ì•ˆì •ì )
                refined_content = metadata.get("refined_content", "")
                original_content = metadata.get("original_content", "")
                content_summary = metadata.get("content_summary", "")
                key_points_str = metadata.get("key_points", "[]")
                
                # key_pointsê°€ JSON ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹±
                try:
                    key_points = json.loads(key_points_str) if key_points_str else []
                except (json.JSONDecodeError, TypeError):
                    key_points = []
                
                # ë©”íƒ€ë°ì´í„°ì— ë‚´ìš©ì´ ì—†ìœ¼ë©´ documentì—ì„œ íŒŒì‹± ì‹œë„
                if not refined_content:
                    lines = document.strip().split('\n')
                    for line in lines:
                        if line.startswith("Content:"):
                            refined_content = line.replace("Content:", "").strip()
                        elif line.startswith("Summary:"):
                            content_summary = line.replace("Summary:", "").strip()
                        elif line.startswith("Key Points:"):
                            key_points_str = line.replace("Key Points:", "").strip()
                            key_points = [kp.strip() for kp in key_points_str.split(',') if kp.strip()]
                
                mail = Mail(
                    message_id=message_id,
                    original_content=original_content,
                    refined_content=refined_content,
                    sender=metadata.get("sender", ""),
                    status=metadata.get("status", "acceptable"),
                    subject=metadata.get("subject", ""),
                    received_datetime=metadata.get("received_datetime", ""),
                    content_type=metadata.get("content_type", "text"),
                    has_attachment=metadata.get("has_attachment", False),
                    extraction_method=metadata.get("extraction_method", ""),
                    content_summary=content_summary,
                    key_points=key_points,
                    created_at=metadata.get("created_at", "")
                )
                mails.append(mail)
            
            # ìƒì„± ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ìµœê·¼ ìˆœ)
            mails.sort(key=lambda x: x.created_at, reverse=True)
            
            return mails[:limit]
            
        except Exception as e:
            print(f"Vector DB ì „ì²´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def reset_collection(self):
        """ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (ê°œë°œìš©)"""
        try:
            self.client.delete_collection(name=self.collection_name)
            self.collection = self._get_or_create_collection()
            return True
        except Exception as e:
            print(f"ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
            return False
    
    def force_reset_chromadb(self):
        """ChromaDB ê°•ì œ ì¬ì„¤ì • (ì¶©ëŒ í•´ê²°ìš©)"""
        try:
            import shutil
            import os
            
            # í˜„ì¬ í´ë¼ì´ì–¸íŠ¸ ì •ë¦¬
            try:
                self.client = None
            except:
                pass
            
            # vector_db ë””ë ‰í† ë¦¬ ë°±ì—… í›„ ì‚­ì œ
            vector_db_path = "./vector_db"
            backup_path = "./vector_db_backup"
            
            if os.path.exists(vector_db_path):
                if os.path.exists(backup_path):
                    shutil.rmtree(backup_path)
                shutil.move(vector_db_path, backup_path)
                print(f"âœ… ê¸°ì¡´ vector_dbë¥¼ {backup_path}ë¡œ ë°±ì—…í–ˆìŠµë‹ˆë‹¤.")
            
            # ìƒˆë¡œìš´ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(vector_db_path, mode=0o755, exist_ok=True)
            
            # ìƒˆë¡œìš´ í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            self.client = chromadb.PersistentClient(
                path=vector_db_path,
                settings=Settings(
                    anonymized_telemetry=False,
                    allow_reset=True
                )
            )
            
            # ì»¬ë ‰ì…˜ ì¬ìƒì„±
            self.collection = self._get_or_create_collection()
            
            print("âœ… ChromaDB ê°•ì œ ì¬ì„¤ì • ì™„ë£Œ!")
            return True
            
        except Exception as e:
            print(f"âŒ ChromaDB ê°•ì œ ì¬ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def get_file_chunks_count(self) -> int:
        """íŒŒì¼ ì²­í¬ ê°œìˆ˜ ì¡°íšŒ"""
        try:
            collection = self.client.get_collection("file_chunks")
            count = collection.count()
            return count
        except Exception as e:
            # ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ëŠ” ì •ìƒì ì¸ ìƒí™©
            if "does not exists" in str(e) or "not found" in str(e).lower():
                return 0
            print(f"íŒŒì¼ ì²­í¬ ê°œìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 0
    
    def get_mails_count(self) -> int:
        """ë©”ì¼ ë°ì´í„° ê°œìˆ˜ ì¡°íšŒ"""
        try:
            collection = self.client.get_collection("mails")
            count = collection.count()
            return count
        except Exception as e:
            # ì»¬ë ‰ì…˜ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ëŠ” ì •ìƒì ì¸ ìƒí™©
            if "does not exists" in str(e) or "not found" in str(e).lower():
                return 0
            print(f"ë©”ì¼ ê°œìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return 0
    
    def add_file_chunk(self, file_chunk: FileChunk, embedding_client=None):
        """íŒŒì¼ ì²­í¬ë¥¼ ë²¡í„° DBì— ì¶”ê°€ (ChromaDB ê¸°ë³¸ ì„ë² ë”© ì‚¬ìš©)"""
        try:
            # íŒŒì¼ ì²­í¬ìš© ë³„ë„ Collection ìƒì„± (ë©”ì¼ ì»¬ë ‰ì…˜ê³¼ ë¶„ë¦¬)
            try:
                collection = self.client.get_collection(name="file_chunks")
            except Exception:
                # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± (ChromaDB ê¸°ë³¸ ì„ë² ë”© ì‚¬ìš©)
                collection = self.client.create_collection(
                    name="file_chunks",
                    metadata={
                        "hnsw:space": "cosine",
                        "description": "File chunks for RAG system",
                        "created_at": datetime.now().isoformat()
                    }
                )
            
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            metadata = {
                "chunk_id": file_chunk.chunk_id,
                "file_name": file_chunk.file_name,
                "file_hash": file_chunk.file_hash,
                "architecture": file_chunk.architecture,
                "processing_method": file_chunk.processing_method,
                "vision_analysis": file_chunk.vision_analysis,
                "section_title": file_chunk.section_title,
                "page_number": file_chunk.page_number,
                "element_count": file_chunk.element_count,
                "file_type": file_chunk.file_type,
                "created_at": file_chunk.created_at
            }
            
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì ìš©
            preprocessed_text = preprocess_for_embedding(file_chunk.text_chunk)
            
            # ChromaDB ê¸°ë³¸ ì„ë² ë”© ì‚¬ìš© (ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸)
            collection.add(
                documents=[preprocessed_text],
                metadatas=[metadata],
                ids=[file_chunk.chunk_id]
            )
            
            print(f"âœ… íŒŒì¼ ì²­í¬ ì €ì¥ ì™„ë£Œ: {file_chunk.file_name} (ID: {file_chunk.chunk_id})")
            
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì²­í¬ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise e
    
    def clear_all_data(self):
        """ëª¨ë“  ë°ì´í„° ì‚­ì œ"""
        try:
            # ëª¨ë“  ì»¬ë ‰ì…˜ ì‚­ì œ
            collections = self.client.list_collections()
            for collection in collections:
                self.client.delete_collection(collection.name)
                print(f"âœ… ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ: {collection.name}")
            
            print("âœ… ëª¨ë“  ë²¡í„° DB ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ ë°ì´í„° ì‚­ì œ ì‹¤íŒ¨: {e}")
            raise e
    
    def add_structured_chunk(self, structured_chunk: StructuredChunk) -> bool:
        """
        êµ¬ì¡°ì  ì²­í¬ë¥¼ Vector DBì— ì¶”ê°€
        
        Args:
            structured_chunk: êµ¬ì¡°ì  ì²­í¬ ê°ì²´
            
        Returns:
            ì„±ê³µ ì—¬ë¶€
        """
        try:
            # êµ¬ì¡°ì  ì²­í¬ ì „ìš© ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
            collection = self._get_or_create_structured_chunk_collection()
            
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            metadata = {
                "chunk_id": structured_chunk.chunk_id,
                "chunk_type": structured_chunk.chunk_type,
                "ticket_id": structured_chunk.ticket_id,
                "field_name": structured_chunk.field_name,
                "field_value": structured_chunk.field_value,
                "priority": structured_chunk.priority,
                "file_name": structured_chunk.file_name,
                "file_type": structured_chunk.file_type,
                "created_at": structured_chunk.created_at,
                "commenter": structured_chunk.commenter or "",
                **structured_chunk.metadata
            }
            
            # ChromaDBì— ì¶”ê°€
            collection.add(
                ids=[structured_chunk.chunk_id],
                documents=[structured_chunk.content],
                metadatas=[metadata]
            )
            
            print(f"âœ… êµ¬ì¡°ì  ì²­í¬ ì €ì¥ ì™„ë£Œ: {structured_chunk.ticket_id} - {structured_chunk.field_name}")
            return True
            
        except Exception as e:
            print(f"âŒ êµ¬ì¡°ì  ì²­í¬ ì €ì¥ ì‹¤íŒ¨: {str(e)}")
            return False
    
    def _get_or_create_structured_chunk_collection(self):
        """êµ¬ì¡°ì  ì²­í¬ ì „ìš© ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒì„±"""
        try:
            collection = self.client.get_collection("structured_chunks")
            return collection
        except:
            # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±
            collection = self.client.create_collection(
                name="structured_chunks",
                metadata={"description": "êµ¬ì¡°ì  ì²­í¬ ì»¬ë ‰ì…˜"}
            )
            print("âœ… êµ¬ì¡°ì  ì²­í¬ ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")
            return collection
    
    def search_structured_chunks(self, query: str, n_results: int = 5, 
                                chunk_types: List[str] = None, 
                                ticket_ids: List[str] = None,
                                priority_filter: int = None) -> List[Dict[str, Any]]:
        """
        êµ¬ì¡°ì  ì²­í¬ ê²€ìƒ‰
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            n_results: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            chunk_types: ê²€ìƒ‰í•  ì²­í¬ íƒ€ì… í•„í„°
            ticket_ids: ê²€ìƒ‰í•  í‹°ì¼“ ID í•„í„°
            priority_filter: ìš°ì„ ìˆœìœ„ í•„í„° (1: ë†’ìŒ, 2: ì¤‘ê°„, 3: ë‚®ìŒ)
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì ìš©
            preprocessed_query = preprocess_for_embedding(query)
            
            collection = self._get_or_create_structured_chunk_collection()
            
            # í•„í„° ì¡°ê±´ êµ¬ì„±
            where_conditions = {}
            if chunk_types:
                where_conditions["chunk_type"] = {"$in": chunk_types}
            if ticket_ids:
                where_conditions["ticket_id"] = {"$in": ticket_ids}
            if priority_filter:
                where_conditions["priority"] = {"$lte": priority_filter}
            
            # ChromaDBëŠ” ë‹¨ì¼ ì¡°ê±´ë§Œ ì§€ì›í•˜ë¯€ë¡œ ì²« ë²ˆì§¸ ì¡°ê±´ë§Œ ì‚¬ìš©
            if len(where_conditions) > 1:
                # ìš°ì„ ìˆœìœ„: chunk_types > ticket_ids > priority_filter
                if "chunk_type" in where_conditions:
                    where_conditions = {"chunk_type": where_conditions["chunk_type"]}
                elif "ticket_id" in where_conditions:
                    where_conditions = {"ticket_id": where_conditions["ticket_id"]}
                elif "priority" in where_conditions:
                    where_conditions = {"priority": where_conditions["priority"]}
            
            # ê²€ìƒ‰ ì‹¤í–‰ (ì „ì²˜ë¦¬ëœ ì¿¼ë¦¬ ì‚¬ìš©)
            results = collection.query(
                query_texts=[preprocessed_query],
                n_results=n_results,
                where=where_conditions if where_conditions else None,
                include=["metadatas", "documents", "distances"]
            )
            
            structured_chunks = []
            for i, chunk_id in enumerate(results['ids'][0]):
                metadata = results['metadatas'][0][i]
                document = results['documents'][0][i]
                distance = results['distances'][0][i] if results['distances'] else 0.0
                
                # ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
                similarity_score = max(0.0, 1.0 - distance)
                
                structured_chunk = {
                    "chunk_id": chunk_id,
                    "content": document,
                    "chunk_type": metadata.get("chunk_type", ""),
                    "ticket_id": metadata.get("ticket_id", ""),
                    "field_name": metadata.get("field_name", ""),
                    "field_value": metadata.get("field_value", ""),
                    "priority": metadata.get("priority", 3),
                    "file_name": metadata.get("file_name", ""),
                    "file_type": metadata.get("file_type", ""),
                    "similarity_score": similarity_score,
                    "created_at": metadata.get("created_at", ""),
                    "metadata": {k: v for k, v in metadata.items() 
                               if k not in ["chunk_id", "chunk_type", "ticket_id", 
                                          "field_name", "field_value", "priority", 
                                          "file_name", "file_type", "created_at"]}
                }
                structured_chunks.append(structured_chunk)
            
            print(f"âœ… êµ¬ì¡°ì  ì²­í¬ ê²€ìƒ‰ ì™„ë£Œ: {len(structured_chunks)}ê°œ ê²°ê³¼")
            return structured_chunks
            
        except Exception as e:
            print(f"âŒ êµ¬ì¡°ì  ì²­í¬ ê²€ìƒ‰ ì‹¤íŒ¨: {str(e)}")
            return []
    
    def get_structured_chunk_stats(self) -> Dict[str, Any]:
        """êµ¬ì¡°ì  ì²­í¬ í†µê³„ ì¡°íšŒ"""
        try:
            collection = self._get_or_create_structured_chunk_collection()
            count = collection.count()
            
            # ì²­í¬ íƒ€ì…ë³„ í†µê³„
            all_chunks = collection.get(include=["metadatas"])
            chunk_type_stats = {}
            ticket_stats = {}
            
            for metadata in all_chunks['metadatas']:
                chunk_type = metadata.get('chunk_type', 'unknown')
                ticket_id = metadata.get('ticket_id', 'unknown')
                
                chunk_type_stats[chunk_type] = chunk_type_stats.get(chunk_type, 0) + 1
                ticket_stats[ticket_id] = ticket_stats.get(ticket_id, 0) + 1
            
            return {
                "total_chunks": count,
                "chunk_types": chunk_type_stats,
                "unique_tickets": len(ticket_stats),
                "tickets": ticket_stats
            }
            
        except Exception as e:
            print(f"âŒ êµ¬ì¡°ì  ì²­í¬ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            return {"total_chunks": 0, "chunk_types": {}, "unique_tickets": 0, "tickets": {}}
    
    # ==================== í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìœ„í•œ ë¬¸ì„œ ìˆ˜ì§‘ ë©”ì„œë“œë“¤ ====================
    
    def get_all_file_chunks(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  íŒŒì¼ ì²­í¬ ë°ì´í„° ë°˜í™˜"""
        try:
            # file_chunks ì»¬ë ‰ì…˜ì—ì„œ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            file_chunks_collection = self._get_or_create_collection("file_chunks")
            results = file_chunks_collection.get(include=["metadatas", "documents"])
            
            file_chunks = []
            if results and results['documents']:
                for i, doc in enumerate(results['documents']):
                    metadata = results['metadatas'][i] if results['metadatas'] else {}
                    file_chunks.append({
                        'chunk_id': metadata.get('chunk_id', f'chunk_{i}'),
                        'file_name': metadata.get('file_name', ''),
                        'content': doc,
                        'metadata': metadata,
                        'similarity_score': 0.0  # ê¸°ë³¸ê°’
                    })
            
            return file_chunks
            
        except Exception as e:
            print(f"íŒŒì¼ ì²­í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def get_all_mails(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  ë©”ì¼ ë°ì´í„° ë°˜í™˜"""
        try:
            # mail_collectionì—ì„œ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            results = self.collection.get(include=["metadatas", "documents"])
            
            mails = []
            if results and results['documents']:
                for i, doc in enumerate(results['documents']):
                    metadata = results['metadatas'][i] if results['metadatas'] else {}
                    mails.append({
                        'message_id': metadata.get('message_id', f'mail_{i}'),
                        'subject': metadata.get('subject', ''),
                        'sender': metadata.get('sender', ''),
                        'content': doc,
                        'metadata': metadata,
                        'similarity_score': 0.0  # ê¸°ë³¸ê°’
                    })
            
            return mails
            
        except Exception as e:
            print(f"ë©”ì¼ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def get_all_structured_chunks(self) -> List[Dict[str, Any]]:
        """ëª¨ë“  êµ¬ì¡°ì  ì²­í¬ ë°ì´í„° ë°˜í™˜"""
        try:
            # structured_chunks ì»¬ë ‰ì…˜ì—ì„œ ëª¨ë“  ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            structured_collection = self._get_or_create_collection("structured_chunks")
            results = structured_collection.get(include=["metadatas", "documents"])
            
            structured_chunks = []
            if results and results['documents']:
                for i, doc in enumerate(results['documents']):
                    metadata = results['metadatas'][i] if results['metadatas'] else {}
                    structured_chunks.append({
                        'chunk_id': metadata.get('chunk_id', f'structured_{i}'),
                        'ticket_id': metadata.get('ticket_id', ''),
                        'chunk_type': metadata.get('chunk_type', ''),
                        'content': doc,
                        'metadata': metadata,
                        'similarity_score': 0.0  # ê¸°ë³¸ê°’
                    })
            
            return structured_chunks
            
        except Exception as e:
            print(f"êµ¬ì¡°ì  ì²­í¬ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def get_all_documents_for_hybrid_search(self) -> List[Dict[str, Any]]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ì„ ìœ„í•œ ëª¨ë“  ë¬¸ì„œ í†µí•© ë°˜í™˜"""
        try:
            all_documents = []
            
            # íŒŒì¼ ì²­í¬ ì¶”ê°€
            file_chunks = self.get_all_file_chunks()
            for chunk in file_chunks:
                chunk['source_type'] = 'file_chunk'
                all_documents.append(chunk)
            
            # ë©”ì¼ ì¶”ê°€
            mails = self.get_all_mails()
            for mail in mails:
                mail['source_type'] = 'mail'
                all_documents.append(mail)
            
            # êµ¬ì¡°ì  ì²­í¬ ì¶”ê°€
            structured_chunks = self.get_all_structured_chunks()
            for chunk in structured_chunks:
                chunk['source_type'] = 'structured_chunk'
                all_documents.append(chunk)
            
            print(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš© ë¬¸ì„œ ìˆ˜ì§‘ ì™„ë£Œ: {len(all_documents)}ê°œ")
            return all_documents
            
        except Exception as e:
            print(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ìš© ë¬¸ì„œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

class UserActionVectorDBManager:
    """ì‚¬ìš©ì ì•¡ì…˜ ì €ì¥ìš© Vector DB ê´€ë¦¬ì - ChromaDB ì‚¬ìš© (ì¥ê¸° ê¸°ì–µ)"""
    
    def __init__(self, db_path: str = "./vector_db"):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        self.collection_name = "user_action"
        self.collection = self._get_or_create_collection()
    
    def _get_or_create_collection(self):
        """user_action ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.client.get_collection(name=self.collection_name)
        except Exception:
            return self.client.create_collection(
                name=self.collection_name,
                metadata={
                    "description": "User actions and AI decisions for long-term memory",
                    "created_at": datetime.now().isoformat(),
                    "type": "memory_actions"
                }
            )
    
    def save_action_memory(self, action_id: str, memory_sentence: str, 
                          action_type: str, ticket_id: Optional[int] = None, 
                          message_id: Optional[str] = None, user_id: Optional[str] = None) -> bool:
        """ì•¡ì…˜ ê¸°ì–µì„ Vector DBì— ì €ì¥"""
        try:
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            metadata = {
                "action_type": action_type,
                "ticket_id": ticket_id,
                "message_id": message_id,
                "user_id": user_id or "ai_system",
                "created_at": datetime.now().isoformat()
            }
            
            # ChromaDBì— ì €ì¥ (memory_sentenceê°€ ì„ë² ë”©ë  í…ìŠ¤íŠ¸)
            self.collection.add(
                documents=[memory_sentence],
                metadatas=[metadata],
                ids=[action_id]
            )
            
            return True
            
        except Exception as e:
            print(f"ì‚¬ìš©ì ì•¡ì…˜ Vector DB ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    def search_similar_actions(self, query: str, n_results: int = 10) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ ê³¼ê±° ì•¡ì…˜ë“¤ì„ ê²€ìƒ‰ (AI ê²°ì • + ì‚¬ìš©ì í”¼ë“œë°± ëª¨ë‘ í¬í•¨)"""
        try:
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì ìš©
            preprocessed_query = preprocess_for_embedding(query)
            
            results = self.collection.query(
                query_texts=[preprocessed_query],
                n_results=n_results,
                include=["metadatas", "documents", "distances"]
            )
            
            actions = []
            for i, action_id in enumerate(results['ids'][0]):
                metadata = results['metadatas'][0][i]
                memory_sentence = results['documents'][0][i]
                distance = results['distances'][0][i] if 'distances' in results else None
                
                actions.append({
                    "action_id": action_id,
                    "memory_sentence": memory_sentence,
                    "action_type": metadata.get('action_type', ''),
                    "ticket_id": metadata.get('ticket_id'),
                    "message_id": metadata.get('message_id'),
                    "user_id": metadata.get('user_id', 'ai_system'),
                    "created_at": metadata.get('created_at', ''),
                    "similarity_score": 1 - distance if distance is not None else None
                })
            
            return actions
            
        except Exception as e:
            print(f"ì‚¬ìš©ì ì•¡ì…˜ Vector DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def get_all_actions(self, limit: int = 100) -> List[Dict[str, Any]]:
        """ëª¨ë“  ì•¡ì…˜ ê¸°ì–µ ì¡°íšŒ (ìµœê·¼ ìˆœ)"""
        try:
            result = self.collection.get(
                include=["metadatas", "documents"]
            )
            
            actions = []
            for i, action_id in enumerate(result['ids']):
                metadata = result['metadatas'][i]
                memory_sentence = result['documents'][i]
                
                actions.append({
                    "action_id": action_id,
                    "memory_sentence": memory_sentence,
                    "action_type": metadata.get('action_type', ''),
                    "ticket_id": metadata.get('ticket_id'),
                    "message_id": metadata.get('message_id'),
                    "user_id": metadata.get('user_id', 'ai_system'),
                    "created_at": metadata.get('created_at', '')
                })
            
            # ìƒì„± ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ìµœê·¼ ìˆœ)
            actions.sort(key=lambda x: x.get('created_at', ''), reverse=True)
            
            return actions[:limit]
            
        except Exception as e:
            print(f"ì‚¬ìš©ì ì•¡ì…˜ Vector DB ì „ì²´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def reset_collection(self):
        """ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        try:
            self.client.delete_collection(name=self.collection_name)
            self.collection = self._get_or_create_collection()
            print(f"âœ… {self.collection_name} ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

class SystemInfoVectorDBManager:
    """ì‹œìŠ¤í…œ ì •ë³´ íŒŒì¼ ì €ì¥ìš© Vector DB ê´€ë¦¬ì - ChromaDB ì‚¬ìš©"""
    
    def __init__(self, db_path: str = "./vector_db"):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        self.collection_name = "system_info"
        self.collection = self._get_or_create_collection()
    
    def _get_or_create_collection(self):
        """system_info ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.client.get_collection(name=self.collection_name)
        except Exception:
            return self.client.create_collection(
                name=self.collection_name,
                metadata={
                    "description": "System information and document chunks for knowledge base",
                    "created_at": datetime.now().isoformat(),
                    "type": "document_chunks"
                }
            )
    
    def _calculate_file_hash(self, file_content: bytes) -> str:
        """íŒŒì¼ ë‚´ìš©ì˜ SHA-256 í•´ì‹œ ê³„ì‚°"""
        import hashlib
        return hashlib.sha256(file_content).hexdigest()
    
    def _is_file_already_processed(self, file_hash: str) -> bool:
        """íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ í™•ì¸ (í•´ì‹œ ê¸°ë°˜ ì¤‘ë³µ ë°©ì§€)"""
        try:
            # ë©”íƒ€ë°ì´í„°ì—ì„œ file_hash ê²€ìƒ‰
            results = self.collection.get(
                where={"file_hash": file_hash},
                include=["metadatas"]
            )
            return len(results['ids']) > 0
        except Exception:
            return False
    
    def save_file_chunks(self, chunks: List[Dict[str, Any]], file_content: bytes, 
                        file_name: str, processing_duration: float) -> Dict[str, Any]:
        """íŒŒì¼ ì²­í¬ë“¤ì„ Vector DBì— ì €ì¥ (ì¤‘ë³µ ë°©ì§€ í¬í•¨)"""
        try:
            # íŒŒì¼ í•´ì‹œ ê³„ì‚°
            file_hash = self._calculate_file_hash(file_content)
            
            # ì¤‘ë³µ íŒŒì¼ í™•ì¸
            if self._is_file_already_processed(file_hash):
                return {
                    "success": True,
                    "message": f"âœ… {file_name}ì€ ì´ë¯¸ ì²˜ë¦¬ëœ íŒŒì¼ì…ë‹ˆë‹¤ (ì¤‘ë³µ ë°©ì§€)",
                    "file_hash": file_hash,
                    "duplicate": True,
                    "chunks_saved": 0
                }
            
            # ê° ì²­í¬ë¥¼ Vector DBì— ì €ì¥
            saved_chunks = []
            for chunk in chunks:
                chunk_id = str(uuid.uuid4())
                
                # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
                metadata = {
                    "file_name": file_name,
                    "file_hash": file_hash,
                    "architecture": chunk.get('metadata', {}).get('architecture', 'unknown'),
                    "processing_method": chunk.get('metadata', {}).get('processing_method', 'unknown'),
                    "vision_analysis": chunk.get('metadata', {}).get('vision_analysis', False),
                    "section_title": chunk.get('metadata', {}).get('section_title', ''),
                    "page_number": chunk.get('metadata', {}).get('page_number', 1),
                    "element_count": chunk.get('metadata', {}).get('element_count', 0),
                    "file_type": chunk.get('metadata', {}).get('file_type', 'unknown'),
                    "file_size": len(file_content),
                    "processing_duration": processing_duration,
                    "created_at": datetime.now().isoformat()
                }
                
                # ìš”ì†Œ ì •ë³´ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜ (ChromaDB ë©”íƒ€ë°ì´í„° ì œí•œ)
                elements = chunk.get('metadata', {}).get('elements', [])
                if elements:
                    metadata["elements_summary"] = f"{len(elements)}ê°œ ìš”ì†Œ: {', '.join([e.get('element_type', 'unknown') for e in elements[:5]])}"
                    if len(elements) > 5:
                        metadata["elements_summary"] += f" ì™¸ {len(elements) - 5}ê°œ"
                
                # í…ìŠ¤íŠ¸ ë‚´ìš© (ì„ë² ë”©í•  í…ìŠ¤íŠ¸)
                text_content = chunk.get('text_chunk_to_embed', '')
                if not text_content:
                    text_content = f"íŒŒì¼: {file_name}, ì•„í‚¤í…ì²˜: {metadata['architecture']}, ìš”ì†Œ: {metadata['element_count']}ê°œ"
                
                # ChromaDBì— ì €ì¥
                self.collection.add(
                    documents=[text_content],
                    metadatas=[metadata],
                    ids=[chunk_id]
                )
                
                saved_chunks.append(chunk_id)
            
            return {
                "success": True,
                "message": f"âœ… {file_name} ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì–´ {len(saved_chunks)}ê°œì˜ ì²­í¬ê°€ system_info ì»¬ë ‰ì…˜ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "file_hash": file_hash,
                "duplicate": False,
                "chunks_saved": len(saved_chunks),
                "chunk_ids": saved_chunks
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "message": f"âŒ {file_name} ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
            }
    
    def search_similar_chunks(self, query: str, n_results: int = 5, 
                             file_type: Optional[str] = None) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì ìš©
            preprocessed_query = preprocess_for_embedding(query)
            
            # ê²€ìƒ‰ ì¡°ê±´ ì„¤ì •
            where_filter = {}
            if file_type:
                where_filter["file_type"] = file_type
            
            results = self.collection.query(
                query_texts=[preprocessed_query],
                n_results=n_results,
                where=where_filter if where_filter else None,
                include=["metadatas", "documents", "distances"]
            )
            
            chunks = []
            for i, chunk_id in enumerate(results['ids'][0]):
                metadata = results['metadatas'][0][i]
                document = results['documents'][0][i]
                distance = results['distances'][0][i] if 'distances' in results else None
                
                chunks.append({
                    "chunk_id": chunk_id,
                    "text_content": document,
                    "metadata": metadata,
                    "similarity_score": 1 - distance if distance is not None else None
                })
            
            return chunks
            
        except Exception as e:
            print(f"Vector DB ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def get_file_chunks(self, file_name: str) -> List[Dict[str, Any]]:
        """íŠ¹ì • íŒŒì¼ì˜ ëª¨ë“  ì²­í¬ ì¡°íšŒ"""
        try:
            results = self.collection.get(
                where={"file_name": file_name},
                include=["metadatas", "documents"]
            )
            
            chunks = []
            for i, chunk_id in enumerate(results['ids']):
                metadata = results['metadatas'][i]
                document = results['documents'][i]
                
                chunks.append({
                    "chunk_id": chunk_id,
                    "text_content": document,
                    "metadata": metadata
                })
            
            return chunks
            
        except Exception as e:
            print(f"Vector DB ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ í†µê³„ ì •ë³´ ì¡°íšŒ"""
        try:
            # ì „ì²´ ë°ì´í„° ì¡°íšŒ
            result = self.collection.get(include=["metadatas"])
            
            if not result['ids']:
                return {
                    "total_chunks": 0, 
                    "file_types": {}, 
                    "total_files": 0,
                    "collection_name": self.collection_name
                }
            
            # íŒŒì¼ íƒ€ì…ë³„ í†µê³„
            file_types = {}
            unique_files = set()
            
            for metadata in result['metadatas']:
                file_type = metadata.get('file_type', 'unknown')
                file_types[file_type] = file_types.get(file_type, 0) + 1
                
                file_name = metadata.get('file_name', '')
                if file_name:
                    unique_files.add(file_name)
            
            return {
                "total_chunks": len(result['ids']),
                "file_types": file_types,
                "total_files": len(unique_files),
                "collection_name": self.collection_name
            }
            
        except Exception as e:
            print(f"ì»¬ë ‰ì…˜ í†µê³„ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return {
                "error": str(e),
                "total_chunks": 0,
                "file_types": {},
                "total_files": 0,
                "collection_name": self.collection_name
            }
    
    def reset_collection(self):
        """ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        try:
            self.client.delete_collection(name=self.collection_name)
            self.collection = self._get_or_create_collection()
            print(f"âœ… {self.collection_name} ì»¬ë ‰ì…˜ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as e:
            print(f"âŒ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

class JiraInfoVectorDBManager:
    """JIRA ì •ë³´ ì €ì¥ìš© Vector DB ê´€ë¦¬ì - ChromaDB ì‚¬ìš©"""
    
    def __init__(self, db_path: str = "./vector_db"):
        """ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        self.client = chromadb.PersistentClient(
            path=db_path,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        self.collection_name = "jira_info"
        self.collection = self._get_or_create_collection()
    
    def _get_or_create_collection(self):
        """jira_info ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°"""
        try:
            return self.client.get_collection(name=self.collection_name)
        except Exception:
            return self.client.create_collection(
                name=self.collection_name,
                metadata={
                    "description": "JIRA issues and knowledge base for ticket resolution",
                    "created_at": datetime.now().isoformat(),
                    "type": "jira_issues"
                }
            )
    
    def save_jira_issue(self, issue_data: Dict[str, Any]) -> bool:
        """JIRA ì´ìŠˆë¥¼ Vector DBì— ì €ì¥"""
        try:
            issue_key = issue_data.get('key', str(uuid.uuid4()))
            
            # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
            metadata = {
                "issue_key": issue_key,
                "summary": issue_data.get('summary', ''),
                "issue_type": issue_data.get('issue_type', ''),
                "status": issue_data.get('status', ''),
                "priority": issue_data.get('priority', ''),
                "assignee": issue_data.get('assignee', ''),
                "reporter": issue_data.get('reporter', ''),
                "project_key": issue_data.get('project_key', ''),
                "created": issue_data.get('created', ''),
                "updated": issue_data.get('updated', ''),
                "created_at": datetime.now().isoformat()
            }
            
            # ë¬¸ì„œ ë‚´ìš© (ì„ë² ë”©í•  í…ìŠ¤íŠ¸)
            document_text = f"""
            Issue: {issue_data.get('summary', '')}
            Description: {issue_data.get('description', '')}
            Type: {issue_data.get('issue_type', '')}
            Status: {issue_data.get('status', '')}
            Priority: {issue_data.get('priority', '')}
            Assignee: {issue_data.get('assignee', '')}
            Reporter: {issue_data.get('reporter', '')}
            Project: {issue_data.get('project_key', '')}
            """
            
            # ChromaDBì— ì €ì¥
            self.collection.add(
                documents=[document_text],
                metadatas=[metadata],
                ids=[issue_key]
            )
            
            return True
            
        except Exception as e:
            print(f"JIRA ì´ìŠˆ Vector DB ì €ì¥ ì˜¤ë¥˜: {e}")
            return False
    
    def search_similar_issues(self, query: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """ìœ ì‚¬í•œ JIRA ì´ìŠˆ ê²€ìƒ‰"""
        try:
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬ ì ìš©
            preprocessed_query = preprocess_for_embedding(query)
            
            results = self.collection.query(
                query_texts=[preprocessed_query],
                n_results=n_results,
                include=["metadatas", "documents", "distances"]
            )
            
            issues = []
            for i, issue_key in enumerate(results['ids'][0]):
                metadata = results['metadatas'][0][i]
                document = results['documents'][0][i]
                distance = results['distances'][0][i] if 'distances' in results else None
                
                issues.append({
                    "issue_key": issue_key,
                    "summary": metadata.get('summary', ''),
                    "description": document,
                    "issue_type": metadata.get('issue_type', ''),
                    "status": metadata.get('status', ''),
                    "priority": metadata.get('priority', ''),
                    "similarity_score": 1 - distance if distance is not None else None
                })
            
            return issues
            
        except Exception as e:
            print(f"JIRA ì´ìŠˆ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def get_all_issues(self, limit: int = 100) -> List[Dict[str, Any]]:
        """ëª¨ë“  JIRA ì´ìŠˆ ì¡°íšŒ"""
        try:
            result = self.collection.get(
                include=["metadatas", "documents"]
            )
            
            issues = []
            for i, issue_key in enumerate(result['ids']):
                metadata = result['metadatas'][i]
                document = result['documents'][i]
                
                issues.append({
                    "issue_key": issue_key,
                    "summary": metadata.get('summary', ''),
                    "description": document,
                    "issue_type": metadata.get('issue_type', ''),
                    "status": metadata.get('status', ''),
                    "priority": metadata.get('priority', ''),
                    "assignee": metadata.get('assignee', ''),
                    "reporter": metadata.get('reporter', ''),
                    "project_key": metadata.get('project_key', ''),
                    "created": metadata.get('created', ''),
                    "updated": metadata.get('updated', '')
                })
            
            # ì—…ë°ì´íŠ¸ ì‹œê°„ ê¸°ì¤€ ì •ë ¬ (ìµœê·¼ ìˆœ)
            issues.sort(key=lambda x: x.get('updated', ''), reverse=True)
            
            return issues[:limit]
            
        except Exception as e:
            print(f"JIRA ì´ìŠˆ ì „ì²´ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []

class AIRecommendationEngine:
    """AI ì¶”ì²œ í•´ê²°ë°©ë²• ìƒì„± ì—”ì§„"""
    
    def __init__(self):
        self.system_info_db = SystemInfoVectorDBManager()
        self.jira_info_db = JiraInfoVectorDBManager()
    
    def generate_solution_recommendation(self, mail_content: str, ticket_history: str, output_placeholder=None) -> str:
        """ë©”ì¼ ì›ë¬¸ê³¼ í‹°ì¼“ ì´ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ AI ì¶”ì²œ í•´ê²°ë°©ë²• ìƒì„± (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)"""
        try:
            # 1. ê´€ë ¨ ì‹œìŠ¤í…œ ì •ë³´ ê²€ìƒ‰
            system_context = self._search_system_info(mail_content, ticket_history)
            
            # 2. ê´€ë ¨ JIRA ì´ìŠˆ ê²€ìƒ‰
            jira_context = self._search_jira_issues(mail_content, ticket_history)
            
            # 3. AI ì¶”ì²œ í•´ê²°ë°©ë²• ìƒì„±
            recommendation = self._create_ai_recommendation(
                mail_content, ticket_history, system_context, jira_context, output_placeholder
            )
            
            return recommendation
            
        except Exception as e:
            print(f"AI ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return "AI ì¶”ì²œ í•´ê²°ë°©ë²•ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _search_system_info(self, mail_content: str, ticket_history: str) -> List[Dict[str, Any]]:
        """ì‹œìŠ¤í…œ ì •ë³´ì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰"""
        try:
            # ë©”ì¼ ë‚´ìš©ê³¼ í‹°ì¼“ ì´ë ¥ì„ ê²°í•©í•˜ì—¬ ê²€ìƒ‰
            search_query = f"{mail_content[:200]} {ticket_history[:200]}"
            
            # system_info ì»¬ë ‰ì…˜ì—ì„œ ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰
            similar_chunks = self.system_info_db.search_similar_chunks(
                search_query, n_results=3
            )
            
            return similar_chunks
            
        except Exception as e:
            print(f"ì‹œìŠ¤í…œ ì •ë³´ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _search_jira_issues(self, mail_content: str, ticket_history: str) -> List[Dict[str, Any]]:
        """JIRA ì´ìŠˆì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰"""
        try:
            # ë©”ì¼ ë‚´ìš©ê³¼ í‹°ì¼“ ì´ë ¥ì„ ê²°í•©í•˜ì—¬ ê²€ìƒ‰
            search_query = f"{mail_content[:200]} {ticket_history[:200]}"
            
            # jira_info ì»¬ë ‰ì…˜ì—ì„œ ìœ ì‚¬í•œ ì´ìŠˆ ê²€ìƒ‰
            similar_issues = self.jira_info_db.search_similar_issues(
                search_query, n_results=3
            )
            
            return similar_issues
            
        except Exception as e:
            print(f"JIRA ì´ìŠˆ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return []
    
    def _create_ai_recommendation(self, mail_content: str, ticket_history: str, 
                                 system_context: List[Dict[str, Any]], 
                                 jira_context: List[Dict[str, Any]], 
                                 output_placeholder=None) -> str:
        """AI ì¶”ì²œ í•´ê²°ë°©ë²• ìƒì„± - LLM ì „ìš© (ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)"""
        try:
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì •ë¦¬
            system_info_text = ""
            if system_context:
                system_info_text = "\n\nê´€ë ¨ ì‹œìŠ¤í…œ ì •ë³´:\n"
                for chunk in system_context:
                    system_info_text += f"- {chunk.get('text_content', '')[:300]}...\n"
            
            jira_info_text = ""
            if jira_context:
                jira_info_text = "\n\nê´€ë ¨ JIRA ì´ìŠˆ:\n"
                for issue in jira_context:
                    jira_info_text += f"- {issue.get('summary', '')}: {issue.get('description', '')[:300]}...\n"
            
            # LLM í”„ë¡¬í”„íŠ¸ êµ¬ì„± (f-string í¬ë§·íŒ… ì˜¤ë¥˜ ë°©ì§€)
            prompt = """
ë‹¹ì‹ ì€ IT ìš´ì˜ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ë°©ë²•ì„ ì œì‹œí•´ì£¼ì„¸ìš”.

ğŸ“§ **ë©”ì¼ ë‚´ìš©:**
{mail_content}

ğŸ“‹ **í‹°ì¼“ íˆìŠ¤í† ë¦¬:**
{ticket_history}

ğŸ“š **ê´€ë ¨ ì‹œìŠ¤í…œ ì •ë³´:**
{system_info}

ğŸ« **ê´€ë ¨ JIRA ì´ìŠˆ:**
{jira_info}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ êµ¬ì²´ì ì¸ í•´ê²°ë°©ë²•ì„ ì œì‹œí•´ì£¼ì„¸ìš”:

ğŸ¤– **AI ì¶”ì²œ í•´ê²°ë°©ë²•**

ğŸ” **ë¬¸ì œ ë¶„ì„:**
[ë©”ì¼ ë‚´ìš©ê³¼ í‹°ì¼“ íˆìŠ¤í† ë¦¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ êµ¬ì²´ì ì¸ ë¬¸ì œ ìƒí™© ë¶„ì„]

ğŸ¯ **ê¶Œì¥ ì¡°ì¹˜:**
1. [êµ¬ì²´ì ì¸ ì²« ë²ˆì§¸ ì¡°ì¹˜]
2. [êµ¬ì²´ì ì¸ ë‘ ë²ˆì§¸ ì¡°ì¹˜]
3. [êµ¬ì²´ì ì¸ ì„¸ ë²ˆì§¸ ì¡°ì¹˜]

ğŸ“š **ì°¸ê³  ìë£Œ í™œìš©:**
[ì‹œìŠ¤í…œ ì •ë³´ì™€ JIRA ì´ìŠˆë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ êµ¬ì²´ì ì¸ ì°¸ê³ ì‚¬í•­]

ğŸ’¡ **ì¶”ê°€ ê¶Œì¥ì‚¬í•­:**
- [ë§¥ë½ì— ë§ëŠ” êµ¬ì²´ì ì¸ ê¶Œì¥ì‚¬í•­]
- [ë§¥ë½ì— ë§ëŠ” êµ¬ì²´ì ì¸ ê¶Œì¥ì‚¬í•­]
- [ë§¥ë½ì— ë§ëŠ” êµ¬ì²´ì ì¸ ê¶Œì¥ì‚¬í•­]

âš ï¸ **ì£¼ì˜ì‚¬í•­:**
[í•´ë‹¹ ìƒí™©ì—ì„œ ì£¼ì˜í•´ì•¼ í•  ì ì´ë‚˜ ìœ„í—˜ìš”ì†Œ]

ìœ„ í˜•ì‹ì— ë§ì¶° êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í•´ê²°ë°©ë²•ì„ ì œì‹œí•´ì£¼ì„¸ìš”. ì¼ë°˜ì ì¸ ë‚´ìš©ì´ ì•„ë‹Œ, ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•œ ë§¥ë½ì— ë§ëŠ” êµ¬ì²´ì ì¸ ë‚´ìš©ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
"""
            
            # LLM í˜¸ì¶œ (Azure OpenAI ì‚¬ìš©)
            try:
                from langchain_openai import AzureChatOpenAI
                from langchain_core.prompts import ChatPromptTemplate
                from langchain_core.output_parsers import StrOutputParser
                
                # í™˜ê²½ ë³€ìˆ˜ì—ì„œ Azure OpenAI ì„¤ì • ê°€ì ¸ì˜¤ê¸°
                import os
                azure_endpoint = os.getenv("AZURE_OPENAI_ENDPOINT")
                deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
                api_key = os.getenv("AZURE_OPENAI_API_KEY")
                api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-10-21")
                
                if azure_endpoint and deployment_name and api_key:
                    # Azure OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
                    llm = AzureChatOpenAI(
                        azure_endpoint=azure_endpoint,
                        deployment_name=deployment_name,
                        openai_api_key=api_key,
                        openai_api_version=api_version,
                        temperature=0.3  # ì°½ì˜ì„±ê³¼ ì¼ê´€ì„±ì˜ ê· í˜•
                    )
                    
                    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
                    prompt_template = ChatPromptTemplate.from_template(prompt)
                    
                    # ì²´ì¸ ì‹¤í–‰
                    chain = prompt_template | llm | StrOutputParser()
                    
                    # ë³€ìˆ˜ ë§¤í•‘
                    variables = {
                        "mail_content": mail_content[:1000],
                        "ticket_history": ticket_history[:1000] if ticket_history else "ì—†ìŒ",
                        "system_info": system_info_text if system_info_text else "ê´€ë ¨ ì‹œìŠ¤í…œ ì •ë³´ ì—†ìŒ",
                        "jira_info": jira_info_text if jira_info_text else "ê´€ë ¨ JIRA ì´ìŠˆ ì—†ìŒ"
                    }
                    
                    # ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
                    if output_placeholder:
                        current_output = ""
                        final_recommendation = ""
                        
                        for chunk in chain.stream(variables):
                            current_output += chunk
                            final_recommendation = current_output
                            
                            # ì‹¤ì‹œê°„ ì¶œë ¥ ì—…ë°ì´íŠ¸
                            with output_placeholder.container():
                                st.markdown("### ğŸ¤– AI ì¶”ì²œ ìƒì„± ì¤‘...")
                                st.markdown(current_output)
                                st.info("ğŸ”„ AI ì¶”ì²œì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤...")
                        
                        # ìµœì¢… ì™„ë£Œ í‘œì‹œ
                        with output_placeholder.container():
                            st.success("âœ… AI ì¶”ì²œ ìƒì„± ì™„ë£Œ!")
                        
                        return final_recommendation
                    else:
                        # ì¼ë°˜ ì²˜ë¦¬ (ìŠ¤íŠ¸ë¦¬ë° ì—†ìŒ)
                        recommendation = chain.invoke(variables)
                        return recommendation
                else:
                    # Azure OpenAI ì„¤ì •ì´ ì—†ìœ¼ë©´ ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜
                    return "âŒ Azure OpenAI ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤. í™˜ê²½ ë³€ìˆ˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
                    
            except Exception as e:
                print(f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                return f"âŒ AI ì¶”ì²œ ìƒì„± ì‹¤íŒ¨: {str(e)}"
            
        except Exception as e:
            print(f"AI ì¶”ì²œ ìƒì„± ì˜¤ë¥˜: {e}")
            return f"âŒ AI ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
    
