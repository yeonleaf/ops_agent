#!/usr/bin/env python3
"""
Whoosh ê¸°ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰ ê´€ë¦¬ì
ë””ìŠ¤í¬ ê¸°ë°˜ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ í‚¤ì›Œë“œ ê²€ìƒ‰ ì œê³µ
"""

import os
import logging
from typing import List, Dict, Any, Optional
from whoosh import index
from whoosh.qparser import QueryParser, MultifieldParser
from whoosh.query import *
from whoosh.analysis import StandardAnalyzer
from text_preprocessor import preprocess_for_embedding
from keyword_extractor import KeywordExtractor

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WhooshSearchManager:
    """Whoosh ê¸°ë°˜ í‚¤ì›Œë“œ ê²€ìƒ‰ ê´€ë¦¬ì"""
    
    def __init__(self, index_dir: str = "whoosh_index"):
        self.index_dir = index_dir
        self.keyword_extractor = KeywordExtractor()
        self._ensure_index_exists()
    
    def _ensure_index_exists(self):
        """ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        if not index.exists_in(self.index_dir):
            raise FileNotFoundError(f"Whoosh ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.index_dir}. ë¨¼ì € build_whoosh_index.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    
    def search_with_whoosh(self, query: str, k: int = 10) -> List[Dict[str, Any]]:
        """
        Whooshë¥¼ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ê²€ìƒ‰ ìˆ˜í–‰ (ë‹¤ì¤‘ ì „ëµ)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            logger.info(f"ğŸ” Whoosh í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹œì‘: '{query}'")
            
            # ì¸ë±ìŠ¤ ì—´ê¸°
            ix = index.open_dir(self.index_dir)
            
            with ix.searcher() as searcher:
                search_results = []
                
                # ì „ëµ 1: í‚¤ì›Œë“œ ì¶”ì¶œ í›„ ê²€ìƒ‰
                try:
                    keywords = self.keyword_extractor.extract_keywords(query)
                    keyword_query = " ".join(keywords)
                    logger.info(f"ğŸ”‘ ì¶”ì¶œëœ í‚¤ì›Œë“œ: {keywords}")
                    
                    # ì¿¼ë¦¬ ì „ì²˜ë¦¬
                    preprocessed_query = preprocess_for_embedding(keyword_query)
                    
                    # ì¿¼ë¦¬ íŒŒì„œ ì„¤ì •
                    parser = QueryParser("content", ix.schema)
                    search_query = parser.parse(preprocessed_query)
                    results = searcher.search(search_query, limit=k)
                    
                    for i, hit in enumerate(results):
                        result = {
                            'id': hit['id'],
                            'content': hit['content'],
                            'metadata': {
                                'source_type': hit.get('source_type', 'unknown'),
                                'search_type': 'whoosh_keywords',
                                'search_rank': i + 1,
                                'score': hit.score,
                                'extracted_keywords': keywords
                            },
                            'similarity_score': hit.score,
                            'source': 'whoosh_search'
                        }
                        search_results.append(result)
                    
                    logger.info(f"âœ… í‚¤ì›Œë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ í‚¤ì›Œë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                
                # ì „ëµ 2: ì›ë³¸ ì¿¼ë¦¬ë¡œ ì§ì ‘ ê²€ìƒ‰ (í‚¤ì›Œë“œ ê²€ìƒ‰ì´ ì‹¤íŒ¨í•œ ê²½ìš°)
                if not search_results:
                    try:
                        logger.info("ğŸ”„ ì›ë³¸ ì¿¼ë¦¬ë¡œ ì§ì ‘ ê²€ìƒ‰ ì‹œë„...")
                        parser = QueryParser("content", ix.schema)
                        search_query = parser.parse(query)
                        results = searcher.search(search_query, limit=k)
                        
                        for i, hit in enumerate(results):
                            result = {
                                'id': hit['id'],
                                'content': hit['content'],
                                'metadata': {
                                    'source_type': hit.get('source_type', 'unknown'),
                                    'search_type': 'whoosh_direct',
                                    'search_rank': i + 1,
                                    'score': hit.score
                                },
                                'similarity_score': hit.score,
                                'source': 'whoosh_search'
                            }
                            search_results.append(result)
                        
                        logger.info(f"âœ… ì§ì ‘ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ ì§ì ‘ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                
                # ì „ëµ 3: ì™€ì¼ë“œì¹´ë“œ ê²€ìƒ‰ (ì—¬ì „íˆ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°)
                if not search_results:
                    try:
                        logger.info("ğŸ”„ ì™€ì¼ë“œì¹´ë“œ ê²€ìƒ‰ ì‹œë„...")
                        # ì¿¼ë¦¬ì˜ ê° ë‹¨ì–´ì— ì™€ì¼ë“œì¹´ë“œ ì¶”ê°€
                        words = query.split()
                        wildcard_query = " OR ".join([f"{word}*" for word in words if len(word) > 2])
                        
                        if wildcard_query:
                            parser = QueryParser("content", ix.schema)
                            search_query = parser.parse(wildcard_query)
                            results = searcher.search(search_query, limit=k)
                            
                            for i, hit in enumerate(results):
                                result = {
                                    'id': hit['id'],
                                    'content': hit['content'],
                                    'metadata': {
                                        'source_type': hit.get('source_type', 'unknown'),
                                        'search_type': 'whoosh_wildcard',
                                        'search_rank': i + 1,
                                        'score': hit.score
                                    },
                                    'similarity_score': hit.score,
                                    'source': 'whoosh_search'
                                }
                                search_results.append(result)
                            
                            logger.info(f"âœ… ì™€ì¼ë“œì¹´ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
                        
                    except Exception as e:
                        logger.warning(f"âš ï¸ ì™€ì¼ë“œì¹´ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                
                logger.info(f"âœ… Whoosh ê²€ìƒ‰ ìµœì¢… ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
                return search_results
                
        except Exception as e:
            logger.error(f"âŒ Whoosh ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_with_multifield(self, query: str, k: int = 10) -> List[Dict[str, Any]]:
        """
        ë‹¤ì¤‘ í•„ë“œ ê²€ìƒ‰ (content, metadata)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            k: ë°˜í™˜í•  ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        try:
            logger.info(f"ğŸ” Whoosh ë‹¤ì¤‘ í•„ë“œ ê²€ìƒ‰ ì‹œì‘: '{query}'")
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ
            keywords = self.keyword_extractor.extract_keywords(query)
            keyword_query = " ".join(keywords)
            
            # ì¿¼ë¦¬ ì „ì²˜ë¦¬
            preprocessed_query = preprocess_for_embedding(keyword_query)
            
            # ì¸ë±ìŠ¤ ì—´ê¸°
            ix = index.open_dir(self.index_dir)
            
            with ix.searcher() as searcher:
                # ë‹¤ì¤‘ í•„ë“œ ì¿¼ë¦¬ íŒŒì„œ ì„¤ì •
                parser = MultifieldParser(["content", "metadata"], ix.schema)
                
                # ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±
                search_query = parser.parse(preprocessed_query)
                
                # ê²€ìƒ‰ ìˆ˜í–‰
                results = searcher.search(search_query, limit=k)
                
                # ê²°ê³¼ ë³€í™˜
                search_results = []
                for i, hit in enumerate(results):
                    result = {
                        'id': hit['id'],
                        'content': hit['content'],
                        'metadata': {
                            'source_type': hit.get('source_type', 'unknown'),
                            'search_type': 'whoosh_multifield',
                            'search_rank': i + 1,
                            'score': hit.score,
                            'extracted_keywords': keywords
                        },
                        'similarity_score': hit.score,
                        'source': 'whoosh_search'
                    }
                    search_results.append(result)
                
                logger.info(f"âœ… Whoosh ë‹¤ì¤‘ í•„ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
                return search_results
                
        except Exception as e:
            logger.error(f"âŒ Whoosh ë‹¤ì¤‘ í•„ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_index_stats(self) -> Dict[str, Any]:
        """ì¸ë±ìŠ¤ í†µê³„ ì •ë³´ ë°˜í™˜"""
        try:
            ix = index.open_dir(self.index_dir)
            with ix.searcher() as searcher:
                return {
                    'total_documents': searcher.doc_count(),
                    'index_dir': self.index_dir,
                    'schema_fields': list(ix.schema.names())
                }
        except Exception as e:
            logger.error(f"âŒ ì¸ë±ìŠ¤ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

def create_whoosh_search_manager(index_dir: str = "whoosh_index") -> WhooshSearchManager:
    """WhooshSearchManager ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    return WhooshSearchManager(index_dir)
